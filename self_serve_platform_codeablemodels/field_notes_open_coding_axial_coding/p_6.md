# p
6
## url
https://github.com/TomEijk/Data_as_a_Product_GLR/tree/master/python/self-serve-platform_codeablemodels/field_notes_open_coding_axial_coding
## tiny url
https://tinyurl.com/self-serve-platform-p6
## archive url
https://bit.ly/self-serve-platform-p6
## title
Interview Expert 6
## source code
no
## example
yes
## source type 
Interview
## author type
Practitioner
## references

**AXIAL CODING TRACE:**
``` python
p6 = CClass(source, "p6", values={
    "title": "Interview Expert 6",
    "url": "https://github.com/TomEijk/Data_as_a_Product_GLR/tree/master/python/self-serve-platform_codeablemodels/field_notes_open_coding_axial_coding",
    "archive url": "https://bit.ly/self-serve-platform-p6",
    "tiny url": "https://tinyurl.com/self-serve-platform-p6",
    "author type": "Practitioner",
    "type": "Interview",
    "example": True,
    "source code": False})
```

# coding

> Interviewer

> All right, let me quickly share my screen. I did everything again in Python. So I have a white screen by the way. Are you able to see the entire, okay, great. Okay, first of all, we have the inter-decision framework. So this actually displays all the possible decisions during your self-serve platform implementation. The first one is more high level. So how to align a self-serve platform and its components with data mesh. This is more in terms of, do we want to develop just one single self-serve platform that's taking care of all the resources and supplies in your data mesh, or do we wanna distinguish, for example, in two self-serve platforms, one for the source aligned data products and the other one specialized on the consumer aligned data products, or do we wanna develop different self-serve platforms dependent on region or functionality? So there are multiple ways in choosing the high level architecture of your self-serve platform. The second layer is more in terms of the three planes explained by Zhamak Dehghani in her book. So the infrastructure provisioning one, the data product developer experience plane and the data mesh supervision plan. And actually the last decision is about the self-serve UI. So how do we make our self-serve platform as accessible as possible for the generalists so that the person who has knowledge about everything, but it's not really an expert in, for example, data engineering practices. So if we look at the first one, this is actually based on the article of Piethein Strengholt where he was talking about a single self-serve platform. The source system and consumer aligned self-serve platforms. So then we have two self-serve platform, for example, and the hub generic and special data landing zone. And this actually means that we have a lot of virtualization and ingestion processes, which are the same for most of the data products so that we have a self-serve platform in the form of a hub. So everything is just coming in in one self-serve platform who's taking care of the ingestion processes. Another self-serve platform is taking care of all these generic data products. And yeah, the last one is actually the special data landing zones, if there are some special cases. So most of the data products would be placed in the generic, well, would be supplied by the generic self-serve platform. The hub self-serve platform would take care of all the ingestion processes and the special data landing zones. Yeah, these are just exceptions on the general practices, if you know what I mean. Then we can specify our self-serve platform on functionality or on region or both. For example, we place a self-serve platform in the U.S East One region in AWS and we place another one in the U.S West Two region, for example, so it can be dependent on region. And the last one is actually, if we have, well, if there is a really big enterprise with several business units and each business unit is so different compared to the other ones that we actually require different governance structures. And this also requires different self-serve platforms to supply these governance teams. So that's not a possibility. So yeah, this is perhaps something really in depth and it's not something you see a lot in the data mesh literature. So I'm really curious if you- Have you read some of these or that you- 

> Participant 6  

> No, I wasn't familiar on the paper, but I'm more familiar on the data mesh book concepts. I think that it's like starting with the end, but I think it's good to take into consideration these large scale enterprises. Yeah. Yeah, I think that's a good use case. And decision to take. And for the, yeah, we've done the concept of platform here, because you've, like one of the examples that you have provided just the storage part, right? So when we talk about buckets, it's just to store the data. And I don't know if that's the idea here or the concept of platform goes a bit beyond the storage part. Yeah. For your understanding and for the context of the discussion. 

**OPEN CODING TRACE:**

I think that's a good use case. And decision to take. And for the, yeah, we've done the concept of platform here, because you've, like one of the examples that you have provided just the storage part, right? So when we talk about buckets, it's just to store the data. And I don't know if that's the idea here or the concept of platform goes a bit beyond the storage part.

**AXIAL CODING TRACE:**
added:
``` python

    multiple_data_landing_zones
       
``` 

> Interviewer

> Yeah. Okay, so you don't think these concepts over here are related to the self-serve platform? 

> Participant 6  

> No, yes, yes, yes. But what I'm, my question is more about when you say self-serve platform, there are, well, actually you mentioned here, not like it has some components. And I don't know if you have thought about which ones should be part of the platform, like the storage component, the orchestration component, the, yeah, catalog perhaps, but because some of these, perhaps some of these options are mostly based on the storage part, like where do you store, store system aligned and consumer aligned datasets. But for example, for orchestration, I know that the official literature is a bit, yeah, what I would say is like my experience in the organizations is companies tend to align on which tools you can use when you build the platform. Right? So usually there is not the possibility to use several orchestration systems. There is just one and then it's used by everyone developing a data product. So, yeah, as I can understand that the storage and the data could be stored in different places, like in different landing zones, for example. But then for the catalog and for the orchestration, for example, I think that there is single instances of those, unless that you go into this large scale enterprise probably. 

**OPEN CODING TRACE:**

And I don't know if you have thought about which ones should be part of the platform, like the storage component, the orchestration component, the, yeah, catalog perhaps, but because some of these, perhaps some of these options are mostly based on the storage part, like where do you store, store system aligned and consumer aligned datasets. But for example, for orchestration, I know that the official literature is a bit, yeah, what I would say is like my experience in the organizations is companies tend to align on which tools you can use when you build the platform. Right? So usually there is not the possibility to use several orchestration systems. There is just one and then it's used by everyone developing a data product. So, yeah, as I can understand that the storage and the data could be stored in different places, like in different landing zones, for example. But then for the catalog and for the orchestration, for example, I think that there is single instances of those, unless that you go into this large scale enterprise probably. 

**AXIAL CODING TRACE:**
added:
``` python

    multiple_data_landing_zones
       
``` 

> Interviewer

> Yeah, exactly. Yeah, I agree with that. And if you perhaps look more in site infrastructure layer, it is a more extensive framework, to be honest. I always have some issues with distinguishing concepts between the infrastructure provisioning, the data product developer and the mesh plane. I think there is a lot of overlap between them. So sometimes one of these concepts can be placed in the infrastructure provisioning as well in the data product developer plane, for example. So, yeah, but from my perspective, the infrastructure provisioning plane is actually the outside layer. So everything that's happening around the data product and not inside the data product. So for example, workflow auation engine, this can be Apache Airflow, for example, that should be an component provisioned by the infrastructure provisioning plane. A processing service, of course, networking. So how does a data product communicate with another data product, VMs? So where do we put our data products in? Data source ingestion. 

> Participant 6  

> Well, one step, maybe on the networking, it's because I’m very, sorry to interrupt, it's very simple. If networking is the interaction between data products, I don't know if this would make sense to have a node that it's often mentioned as interoperability. Yeah, when I first read networking was more like on, I don't know, wires and cables and yeah. 

**OPEN CODING TRACE:**

If networking is the interaction between data products, I don't know if this would make sense to have a node that it's often mentioned as interoperability. Yeah, when I first read networking was more like on, I don't know, wires and cables and yeah. 

**AXIAL CODING TRACE:**
added:
``` python

    networking
       
``` 

> Interviewer

> You mean like a child node, like I did over here. So like a child node interoperability replacing the word as you prefer. Oh yeah, that's a good one, thanks. Yeah, and we also have data transformation orchestration. So we need to have an orchestrator that, yeah, that actually takes care of all the traffic inside your data mesh. A managed compute infrastructure. So we need a computing engine. For example, we can separate compute from compute and storage and compute like is done in Snowflake. A polyglot storage option. So we are able to store CSV files, AVRO schemas, JSON files. Yeah, and the last thing is actually a machine learning platform so that we are able to provide a feature store or other machine learning components. So this is actually why I encountered during my research for the infrastructure provisioning plane. I don't know if you think some of these options are redundant or should be placed somewhere else in another plane, for example. 

> Participant 6 

> I think it's very good. Can you go scroll back to the left? Because I'm thinking on the, yeah, probably, I don't know if it's included later, but the catalog should be somewhere around these planes. 

> Interviewer

> I actually placed that one in the mesh supervision plane. Because I thought it was more a part of the governance team. 

> Participant 6 

> Yeah, that makes sense. Maybe here, one part you can have is actually like the ability to register the data product in the catalog. Like if you are developing a data product and you need to go like, okay, what for, I should place a data product in the workflow. And I don't know, data source ingestion and then what data transformation. I think that it's good to remember to, I don't know, have a way to register it into the catalog. And maybe another piece that could be here would be on observability. So what about monitoring and alerting? So if, yeah, right. So if, yeah, I think that probably this belongs here and yeah, have some kind of data observability service or product. 

**OPEN CODING TRACE:**

schema_registry

one part you can have is actually like the ability to register the data product in the catalog

And maybe another piece that could be here would be on observability. So what about monitoring and alerting?

**AXIAL CODING TRACE:**
added:
``` python

    data_catalog
    
    schema_registry
    
    build_deploy_monitor_dp
       
``` 

> Interviewer

> Yeah. Yeah, that's interesting because I think I placed data quality management, for example, in the mesh supervision plane. Okay. But it's sometimes really hard to distinguish between. 

> Participant 6 

> Yes. I agree with. We can see that it's difficult. 

> Interviewer

> Yeah. And for example, registering functionality, is that something you place in a data product developer plane? Because it's something the data product developer uses during their practices, or would you rather place it here in the infrastructure provisioning plane? I think it can be perhaps placed in both planes that there is a functionality in the product developer plane and something as a, yeah, some pattern as a receiver in the infrastructure provisioning plane, for example, a receiving pattern. 

> Participant 6 

> Yeah. I mean, it's good that you have taken them into account if it's here or in the other plane. I don't think that's that relevant, I would say, but yeah, I don't know. Maybe they can be referenced in both with the specific action you do on those. For the purpose that you use them. 

> Interviewer

> Yeah. Because if we check the data product developer plane, the one we were discussing, that was registering. Oh, I see. I didn't place them that one over here. So yeah, I can add that one as well to the infrastructure provisioning or to this one. That's already really good feedback. Thank you. Yeah. So I discovered several functionalities for the data product developer experience plane. Of course, there needs to be a build, deploy, monitoring data product functionality. Zhamak Dehghani defined this one as just one single functionality. So building, deploying and monitoring is just one functionality. I don't know if you can distinguish between these three because I'm not really sure if this can, if this is, yeah, not too much for one functionality. Of course, versioning is something we need, should current our data products, making it accessible for everyone. So a read functionality, adding metadata to our data products. So documenting our data, oh, documenting data product that can be perhaps seen as registering your data product. I can make some catalog. In the catalog, yeah. I can make some child note or write some context around it to make it more clear. Declaratively create your data products so that it should be really easy to make it. Everyone was able to do this. Testing your data products. And the last one is the cross operations. So the create, read, update and delete. So that we have an API interface. I think there can be some more, but I don't know. 

> Participant 6 

> Yeah, I'm getting some ideas. So on the first point on the build deploy monitor, I think that they could be separated. I get the purpose of making, like getting them together because really you want to monitor things and you could build them, but then the deploy could be like more manual or more au> Intervieweratic depending on the technology. But yeah, I mean, there is a justification for keeping this unit united, but you would have them separated also. And perhaps one thing like as a feedback, if you want to add more.

**OPEN CODING TRACE:**

build deploy monitor, I think that they could be separated. I get the purpose of making, like getting them together because really you want to monitor things and you could build them, but then the deploy could be like more manual or more au> Intervieweratic depending on the technology. But yeah, I mean, there is a justification for keeping this unit united, but you would have them separated also. And perhaps one thing like as a feedback, if you want to add more.

version_dp

**AXIAL CODING TRACE:**
added:
``` python

    build_deploy_monitor_dp
    
    version_dp
    
``` 

> Interviewer

> You mean to make it not too complex? 

> Participant 6 

> Yeah, I mean, for us in my previous company, we worked on a template for creating data products and discussed, this was kind of a, like it was a template that was used as a wizard. So, and it provided all the boilerplate code and just needed to input your code in the places where we highlighted, right? So one of the things that this wizard was doing was saying, okay, do you have personal information in your data product? Do you have, is this an aggregated data product or we will have IDs here, for example, right? So it's like, it's a fit for purpose. It's a source aligned, it's aggregated. So yeah, I don't know. I mean, it's optional, but I think that as you mature in the developer experience for the data products and I think that templates become necessary or like they appear on this and then also monitoring and metrics, for example, on usage or how long it takes to build this data product, et cetera, et cetera. So maybe something about that could be. 

**OPEN CODING TRACE:**

Yeah, I mean, for us in my previous company, we worked on a template for creating data products and discussed, this was kind of a, like it was a template that was used as a wizard. So, and it provided all the boilerplate code and just needed to input your code in the places where we highlighted, right? So one of the things that this wizard was doing was saying, okay, do you have personal information in your data product? Do you have, is this an aggregated data product or we will have IDs here, for example, right? So it's like, it's a fit for purpose. It's a source aligned, it's aggregated. So yeah, I don't know. I mean, it's optional, but I think that as you mature in the developer experience for the data products and I think that templates become necessary or like they appear on this and then also monitoring and metrics, for example, on usage or how long it takes to build this data product, et cetera, et cetera. So maybe something about that could be. 

**AXIAL CODING TRACE:**
added:
``` python

    infrastructure_as_code
       
``` 

> Interviewer

> And what do you think of the other options over here? So for example, testing data product interface. 

> Participant 6 

> Yeah, I think, yeah, testing is important, crude operations. Maybe there is also something related on choosing how this is built. Like it's incremental. This is like full refresh every time that this is launched and provide different experiences for that. But I think that the rest are great. 

**OPEN CODING TRACE:**

Testing is important, crude operations. Maybe there is also something related on choosing how this is built. Like it's incremental. This is like full refresh every time that this is launched and provide different experiences for that.

crud_operations

**AXIAL CODING TRACE:**
added:
``` python

    testing_dp
    
    crud_operations
``` 

> Interviewer

> Thank you. And for example, yeah, I'm at the moment making some kind of MVP during my thesis for my company and we're using Snowflake and Snowflake is well known for its in-place consumption, which I noted over here. You think this can be really important to include in this framework or that we minimize the data movements. That's actually what I mean. So you make sure that data is not being moved. If yeah, we only want to read the data product and discover its functionalities and et cetera. 

> Participant 6 

> You can, I think that you can have two strategies. So I think that reading at the in-place consumption, it's great. You could also secure more of the data by not allowing the downloading that data into your local machine. For example, we had some of that in my previous organization, it was not possible to copy data from S3 buckets to your local machine. You had to do all the analysis on that, let's say. 

**OPEN CODING TRACE:**

So I think that reading at the in-place consumption, it's great. You could also secure more of the data by not allowing the downloading that data into your local machine. For example, we had some of that in my previous organization, it was not possible to copy data from S3 buckets to your local machine. You had to do all the analysis on that, let's say. 

**AXIAL CODING TRACE:**
added:
``` python

    read_dp
       
``` 

> Interviewer

> But can you submit a request for example, and that the data product developer approves this and if it is approved, you can download the data sets. So that is- I think it was not even, no, perhaps it was something like this on the aggregated models, for example, but not on the raw data. So we had lots of data scientists download like some samples of like big CSVs data to train their models locally, et cetera. And this was totally not encouraged to do anymore. And yeah, so you don't, you actually need to not allow that. 

> Interviewer

> Yeah, gotcha. All right, yeah, because this has also to do with the governance plane. So if we go to that one, we're actually good on time. We only have two frameworks left. So that's good, I guess. Yeah, this is the governance plane. So in my opinion, this is actually all the things you provide to the governance team, for example, oh, sorry. An alerting function, it's really important to be alerted if something goes wrong inside your mesh, things like that. A schema registry to enforce some interoperability measures, an API catalog so that we know what APIs are available in the mesh. Perhaps we can include this one in another plane. I'm just thinking about it out loud. 

> Participant 6 

> A lot of them can be included in the, because here you provide this, like the governance capabilities. Capabilities that then you will need to use in when or reference when you do the data product. Yeah, exactly. And for example, the manage emergent graphs of data products has actually, what I mean with that is that we have certain data products and we know the connections between them. So we know that the sales data product is connected to the customer data product and the customer data, well, the sales data product is also connected to the inventory data product. So we know we have some kind of graph that shows all the connections between the data products. Of course, it's important to manage your security policies. So who has access to data products? And we know all about the users and all the access controls and we can enforce policies. Essential data product catalog. We briefly talked about that one, but yeah, that's really important to be provisioned by our self-serve platform team to the governance team. And a lot of practitioners miss out on this one, actually the knowledge graph. This is something I have from the book of Piethein Strengholt as well. He wrote the book, Data Management at Scale. And his second edition is focused on data mesh and he's talking about a metalake. And with a metalake, he actually means a data and central data product catalog, as well as a knowledge graph. And the knowledge graph is like the semantic layer. So it actually links all the different concepts in your metadata. So for example, we have a customer, this one is linked to the customer address, the order ID. I think you get the idea. So it's all the other metadata values. And this graph is often overlooked by many practitioners, but I think it's a really nice addition. It's a really nice add on to have in your data mesh architecture. So that's why I included this one as well in the supervision plan. Yeah, I was thinking about metadata management, but we can use the data product catalog and the knowledge graph as child notes underneath the metadata management practice over here. So I can adjust that. And I was thinking about that just right now. 

> Participant 6 

> But it's good to provide it as important as the others. Well, I mean, I don't know, but I don't think it's bad to have it at the same level, to be honest. 

> Interviewer

> Yeah. Oh, so you mean, for example, to skip this one and just have these as tools? 

> Participant 6 

> No, no, like have it as you have it now. Like you have the catalog as an entity, the knowledge graph, and then master data management and metadata management. Like I think that they are different things that are standalone and necessary by or like providing value by themselves. Like nothing needs to be under. 

> Interviewer

> Yeah, got it. All right. Yeah, and then we only have the log management and master data management and data quality management left. So yeah, of course we need to have logs of all operations. Master data management is always important, I guess, because yeah, if we have a customer 360, we want to know that this central entity is the same for every data product in our mesh and yeah, data quality management. So you can think about the observability aspect. We just talked about, I think a few minutes ago. Yeah, so this is all we need to provide to the governance team, I guess, but I'm not sure if you think differently of some of these aspects, some of these items, options. 

> Participant 6 

> I think that, I think it's complete. I was trying to think on, okay, what's missing here. I was thinking on the word glossary, for example, like a business glossary or a data dictionary. I don't know if that would be equivalent or maybe that can be something that can be placed under master data management or yeah, the data catalog. That's a really good one. I was not thinking about that one. So maybe business glossary or yeah, under master data management could be. And then on block management and data quality. I know that nowadays the word data observability is quite trendy and there are some tools on that space. So if you want to summarize, I don't know, maybe you could have like data observability and then under data observability, you could have log management and the data quality management, for example, or replace data quality management with data observability. But then it's good also that data quality appears here. And I think that the first one on the left. Alerting. Alert, yes. Alerting could be included in this group, for example. Oh, that's good. Because that's to do with observability, of course. Yeah, you have log management, alerting and data quality management. I don't know, like if you want to simplify a bit and have more children here. 

**OPEN CODING TRACE:**

I was trying to think on, okay, what's missing here. I was thinking on the word glossary, for example, like a business glossary or a data dictionary. I don't know if that would be equivalent or maybe that can be something that can be placed under master data management or yeah, the data catalog. That's a really good one. I was not thinking about that one. So maybe business glossary or yeah, under master data management could be.

I don't know, maybe you could have like data observability and then under data observability, you could have log management and the data quality management, for example, or replace data quality management with data observability. But then it's good also that data quality appears here. And I think that the first one on the left. Alerting. Alert, yes. Alerting could be included in this group, for example.

**AXIAL CODING TRACE:**
added:
``` python

    business_glossary
    
    data_observability
       
``` 

> Interviewer

> And how do you feel about the knowledge graph over here? Do you think it's important to have a sort of semantic layer? 

> Participant 6 

> Yeah, I think that that's something organizations miss because like how do we share insights or how do we share analysis? How do we share? And here, the main point is to try to not repeat analysis that have been already done in the past, for example. So I think that it's a valuable component. It's not something that organizations will prioritize like as the first thing to do. 

> Interviewer

> Hard to monetize, of course. 

> Participant 6 

> Yeah. So, but I don't know, like, I don't know if this would be valuable to put in that under the data product catalog or it's a standalone entity. I don't have a strong opinion on this. 

> Interviewer

> Yeah, the API catalog is left because I was not really sure about it if we can place it over here. 

> Participant 6 

> But yeah, it is a catalog. So it can be connected to the supervision plan, but I don't know. You would have it also under the data product catalog. And then actually now I'm thinking that, I don't know if you are, I mean, the data mesh and data products are mostly targeted at analytics use cases. I know there is a huge, big debate on that. And if they know like how this interoperates with machine learning, for example, and the machine learning models life cycle. Yeah. You've put the machine learning platform and that just the machine learning platform would be a good example of that. Yeah, it would require like a full analysis as you are doing. That it's a different platform, right? Per se. But I am also thinking on the experimentation platform. I have quite a bit of an involvement on that in my current company. And we want data products, we have some data products that focuses on experimentation and like how and to analyze experiments, for example. But I don't know if the same way that you have the machine learning platform in one of the previous graphs, you could have some kind of the experimentation platform. Just to have it in mind, that's something that, yeah, probably if you want to do that it would require more exploration. 

**OPEN CODING TRACE:**

I mean, the data mesh and data products are mostly targeted at analytics use cases. I know there is a huge, big debate on that. And if they know like how this interoperates with machine learning, for example, and the machine learning models life cycle. Yeah. You've put the machine learning platform and that just the machine learning platform would be a good example of that. Yeah, it would require like a full analysis as you are doing. That it's a different platform, right? Per se. But I am also thinking on the experimentation platform. I have quite a bit of an involvement on that in my current company. And we want data products, we have some data products that focuses on experimentation and like how and to analyze experiments, for example.

**AXIAL CODING TRACE:**
added:
``` python

    experimental_platform
       
``` 

> Interviewer

> It's like a development environment. So that it's like a playground. You won’t  interfere with current processes, but you can do whatever you want. Oh, that's it. 

> Participant 6 

> Yeah, but then you will need some kind of analysis frameworks and stuff like that. So yeah. It's a platform per se, I would say. 

> Interviewer

> Yeah. Oh, wow. I think that one is also a bit connected to the last framework. So to the self-serve UI. So what kind of functionalities do we want to provide to our data product developer, for example, to experiment? Experimentation can be related to visualization function so that we can quickly see what our data set provides to us, what our data set includes, et cetera. So visualization function is just perhaps meant for some small visualizations. It's not meant for very detailed graphs already, but yeah, I think it can be important to have a visualization function, a cataloging function, of course, so that we know more about the data quality, data discoverability, data profiling, data lineage. And yeah, the visualization function, of course, connects to building reports and dashboards. A transformation function. This can be very interesting if our generalists are able to just click on a certain symbol and there is just like no code. There are just like no code transformation. So we can click on a certain symbol and all the missing values are removed from the data sets or that we can click on another symbol and all the duplicates are removed from the data sets, things like that. So for example, a node code development tool can be interesting to have in your self-serve UI. A data governance function. So we can provide this to the governance team as well, to the data product developer team, because we have local policies and global policies in our data mesh and the data product developer needs to be able to define their own local policies to maintain the autonomy principle in data mesh. Application build function. This is of course an option. It can be interesting if you are developing a SaaS product, the software as a service. A data integration function, so that we can just indicate which operational databases we wanna use or that we can import a data set or that we can export a data set, for example, in our UI. And the last one is a query recommendation function. So that we actually support our data product developer, our generalist in constructing their queries and to make the data set more accessible for them. But I think there can be perhaps many more self-serve UI functionalities. I'm not sure if these can be considered as complete. 

> Participant 6 

> I think that on the... No, I think it's very complete. Like maybe one thing now that... Maybe it's part of one of these already, but yeah. So in the data governance function, perhaps I would add something to visualize the schemas or the contracts, because this has been mentioned in other graphs. So you need to have a way to visualize if I'm expected to send data, for example, like when, if we have an app, when we need to trigger some clicks or page views, et cetera, like it's useful to have the schemas. And maybe that belongs to the data governance function or in the data integration. Yeah, for the rest, I think that's, yeah, very complete. 

**OPEN CODING TRACE:**

So in the data governance function, perhaps I would add something to visualize the schemas or the contracts, because this has been mentioned in other graphs. So you need to have a way to visualize if I'm expected to send data, for example, like when, if we have an app, when we need to trigger some clicks or page views, et cetera, like it's useful to have the schemas. And maybe that belongs to the data governance function or in the data integration. Yeah, for the rest, I think that's, yeah, very complete.

**AXIAL CODING TRACE:**
added:
``` python

    data_governance_function
       
``` 

> Interviewer

> Yes. So you think there's not another option, well, not a functionality to include in your UI interface. 

> Participant 6 

> Well, you would want to add alerts and monitoring here, right, but it's, I think it was on the left. No, I didn't actually. I know. That can be included as well. Quality, yeah. Or for example, also child node, we need the data governance function. It's built for the governance, I guess. Because, yeah, for example, if you have an schema, you would like to have an schema validator, right? So for example, that's monitoring and you can build a lot of rules and like alerting rules on top of that. I guess a good one. And maybe you would, that's interesting. You would want to add some kind of metrics on the data product, for example, like usage of a data product, like who has used this data product recently. And you could go even a step further on, okay, this has not been used in 90 days. Let's alert the owner of this and then they can delete, for example. Yeah, like retention policy. 

**OPEN CODING TRACE:**

you would want to add alerts and monitoring here, right, but it's, I think it was on the left. No, I didn't actually. I know. That can be included as well. Quality, yeah. Or for example, also child node, we need the data governance function. It's built for the governance, I guess. Because, yeah, for example, if you have an schema, you would like to have an schema validator, right? So for example, that's monitoring and you can build a lot of rules and like alerting rules on top of that. I guess a good one. And maybe you would, that's interesting. You would want to add some kind of metrics on the data product, for example, like usage of a data product, like who has used this data product recently. And you could go even a step further on, okay, this has not been used in 90 days. Let's alert the owner of this and then they can delete, for example. Yeah, like retention policy. 

**AXIAL CODING TRACE:**
added:
``` python

    data_governance_function
       
``` 

> Interviewer

> Exactly, yeah. Got it. Oh, that can be included in the data access, data access child note over here, perhaps, or it can be a different functionality. 

> Participant 6 

> I think, yeah, I think maybe it's enabling a different functionality. What I understand on data access is more like, we need to, for example, we need some finance tables that only the team, this team, or the people in this department must have access to. Yeah. So for me, data access would mean more this, yeah. Who is authorized to look at what, and here you could have like specific tables, restricting access to a specific tables, or you would have like row level data access policies or column level data access policies, et cetera, et cetera. 

**OPEN CODING TRACE:**

I think maybe it's enabling a different functionality. What I understand on data access is more like, we need to, for example, we need some finance tables that only the team, this team, or the people in this department must have access to. Yeah. So for me, data access would mean more this, yeah. Who is authorized to look at what, and here you could have like specific tables, restricting access to a specific tables, or you would have like row level data access policies or column level data access policies, et cetera, et cetera. 

**AXIAL CODING TRACE:**
added:
``` python

    data_governance_function
       
``` 

> Interviewer

> Got it. Oh, that can be interesting. Yeah, because in this self-serve UI, everything comes together again, if we look at the inter-decision framework over here. So first we defined on a high level what a self-serve platform actually means to us. So do we wanna have everything in just one self-serve platform, or do we wanna divide certain practices among different self-serve platforms? Then we talked about the three planes. So that's more in depth already, about each or just one self-serve platform would mean to us. And the last one is the self-serve UI, which is provided to the developer, as well as the infrastructure team, as well as the governance team. So yeah, this is actually the complete framework I considered during the self-serve platform implementation. Do you think there can be another decision perhaps, or did we cover most of it already? 

> Participant 6 

> I think since you already provided the framework, it's very difficult to think of if it's complete or not. I think that it covers the overall self-serve platform concept and it's good to use a model that's already in the literature, like the planes concept and then the UI. I think it's a good model. I don't know if maybe this is more of a sequential, like since you have to go over all boxes here, I don't know if it makes sense to just, yeah, just like how to align to a self-serve platform on its components with data mesh. And then you can just use the same framework and instead of having three arrows going into these three elements, like just having one arrow to the data infrastructure provisioning plane, then another arrow from this to the data product developer experience, then another arrow to the data mesh supervision and then ending in the UI. I mean, it's the same outcome and probably it's not as nicely visualized as this, but you could voice over that when presenting or writing it down, like, yes, here you have to go to all the boxes. 

> Interviewer

> Oh, that's a good one. I can definitely include that in the context around this framework. Yeah, all right. And I actually have only two questions left. And the first one is on how easy is it to use this framework for a practitioner during their implementation? And the other one is more about usability. So if the data, well, if the practitioner decides to implement a self-serve platform, can this framework be useful for them? So the first one is about easy, well, the ease of use and the other one is more about usability. Would they use this one as guidance for that? 

> Participant 6 

> Yeah, I think, yes. I like the approach of the graphs and the conversation more than the written, you know, like the super lengthy descriptions. So I think that for this to be used, yeah, I think that's some kind of, maybe this can be built into some kind of a spreadsheet or checklist. Because I think that once you have filled this and you can also assess where do you want to be in every aspect, then you can prioritize which efforts to work on first, right? So it's good, like if a practitioner comes and you deliver this to them, it's like, okay, knowledge graph, we don't have it and we don't want to have it in the next three years, right? So I think that this should be kind of a live document where you can assess your status against this framework or if you don't have anything, like at least help you start with that. So maybe as an idea, you could build some kind of prioritization around behind this, like as we discussed, knowledge graph, maybe it's more of a nice to have than a must have. And there are some different types of prioritization methods like like one, two, three, five, I don't know, like some of these are must have, some of these are should have, some of these are more like nice to have, I don't know. I think some kind of prioritization would help people that start from scratch and the ability to do an assessment and look at the results after one year for example. Or every six months is also something valuable for the practitioners. 

**OPEN CODING TRACE:**

So it's good, like if a practitioner comes and you deliver this to them, it's like, okay, knowledge graph, we don't have it and we don't want to have it in the next three years, right? So I think that this should be kind of a live document where you can assess your status against this framework or if you don't have anything, like at least help you start with that. So maybe as an idea, you could build some kind of prioritization around behind this, like as we discussed, knowledge graph, maybe it's more of a nice to have than a must have.

> Interviewer

> Yeah, that sounds good. And is it easy to interpret these decision and decision options? 

> Participant 6 

> Yeah, for me, yes. I don't know, like I have, I guess that there might be some people that would understand different things if you just put data quality, you know, like this topic, some of the topics are very broad. With a description would work better, but yeah. Yeah, because we briefly talked about the networking that's over here and that has different interpretations. So perhaps I can specify that one more. But overall- Maybe not in the charts, but like in the documentary if you build some kind of interactive version of this that people can use, like maybe a description it's needed. 

**OPEN CODING TRACE:**

Yeah, for me, yes. I don't know, like I have, I guess that there might be some people that would understand different things if you just put data quality, you know, like this topic, some of the topics are very broad. With a description would work better, but yeah. Yeah, because we briefly talked about the networking that's over here and that has different interpretations. So perhaps I can specify that one more. But overall- Maybe not in the charts, but like in the documentary if you build some kind of interactive version of this that people can use, like maybe a description it's needed. 

> Interviewer

> Yeah, gotcha. All right, yeah. That was actually all I wanted to ask to you. So thank you so much for this session. I always learn a lot when I talk to you and I will definitely implement the feedback and send the transcript to you. And also if I finish this chapter, I will share it with you as well. Hopefully we can keep sharing knowledge in this way. But yeah, once again. Works well. 

> Participant 6 

> Yeah, thanks for reaching out and taking into consideration my thoughts and yeah. Sure thing. Enjoy Chicago. Yeah, thank you so much. Hope to see you soon. See you. 

> Interviewer

> Bye. Bye bye.
