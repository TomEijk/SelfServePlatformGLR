# s 
29
## url
https://dzone.com/articles/top-challenges-in-data-mesh-and-how-a-data-product
## tiny url
https://tinyurl.com/self-serve-platform-s29
## archive url
https://bit.ly/self-serve-platform-s29
## title
The Definitive Guide To Building A Data Mesh With Event Streams
## source code
no
## example
yes
## source type 
Practitioner Audience Article
## author type
Practitioner
## references

**AXIAL CODING TRACE:**
``` python
s29 = CClass(source, "s29", values={
    "title": "The Definitive Guide To Building A Data Mesh With Event Streams",
    "url": "https://dzone.com/articles/top-challenges-in-data-mesh-and-how-a-data-product",
    "archive url": "https://bit.ly/self-serve-platform-s29",
    "tiny url": "https://tinyurl.com/self-serve-platform-s29",
    "author type": "Practitioner",
    "type": "Practitioner Audience Article",
    "example": True,
    "source code": False})
```

# coding

> The Definitive Guide To Building A Data Mesh With Event Streams

> Enabling Self-serve Data Products With the Cloud

> Minimizing the time from ideation to a working data-driven application is one of the key objectives of a good data mesh, and self-service is critical to accomplishing this goal. We have structured our prototype to showcase an end-to-end workflow of data product discovery, creation, and management — all features that make products as self-service as possible. 

![EndtoEnd](https://cdn.confluent.io/wp-content/uploads/data-discovery-creation-and-management.jpg)

> Data product discovery is just one aspect of self-serve. To effectively meet the needs of all consumers, data products must provide full access, on demand, to both current and historical data. By using event streams, consumers can autonomously select and read the historical data they need for their business use cases as many times as they require. This flexibility enables applications to build whatever data model and state they need to suit their business needs. 
Cloud-native computing makes self service much easier. In our prototype, scaling concerns have been largely offloaded to Confluent Cloud. Compaction and Infinite Storage provide the means of serving the entire suite of historical events, letting you retain any amount of data as long as you need. You can start up a new consumer, read in the historical event stream data, and then continue processing in real time. This pattern of access is exemplified in the Kappa architecture and avoids the data inconsistency problems of reconciling batch and event stream sources of the Lambda architecture. 

**OPEN CODING TRACE:**

access_control_management

data_catalog

distributed_file_storage

separating_storage_and_compute

kappa_architecture

lambda_architecture

scalability

application_build_function

costs

latency

**AXIAL CODING TRACE:**
added:
``` python
    
    access_control_management
    data_catalog
    scalability
    distributed_file_storage
    separating_storage_and_compute
    kappa_architecture
    lambda_architecture
    application_build_function
    costs
    latency
    
```

![ksqlDB](https://cdn.confluent.io/wp-content/uploads/application-information.jpg)

> Our prototype showcases several business use cases, complete with ksqlDB statements, to illustrate how easy it should be to access the data products. You could also achieve something similar by deploying an event-driven microservice, for example, that consumes from event streams and emits its own transformed events. In either case, the focus is to enable application developers to quickly build the applications they need for their domain while minimizing the overhead costs in accessing data. 
Connecting legacy applications to a data mesh can also pose a challenge, as it may be infeasible for older systems to consume event streams. Kafka Connect simplifies these issues by providing the means to sink data products to a destination data store. You can also create data products powered by change-data capture connectors. For example, you can use the database write through pattern to create rich events with low latency, while avoiding expensive polling queries of production systems. Once you publish a data product to the data mesh, you remain responsible for ensuring the minimum agreements of their product are met. A failure to meet these requirements should be treated like any other service outage. The emergency response may be limited in the case of low-tier data products and those with very accommodating SLAs, while a Tier 1 data product with a very tight SLA may require an on-call rotation to support. 

![row](https://cdn.confluent.io/wp-content/uploads/published-data-product.jpg)

> Removing a data product from the mesh can be done at the click of a button. However, the data product owner needs to be certain that all consumers of the product have been notified and have migrated off. Though not included in this prototype, a useful extension of this prototype would be to deprecate the data product prior to deletion and notify existing consumers of the need to migrate. Meanwhile, access to new consumers could be prevented by rejecting new consumer read access requests on the topic. 

![StreamVisualize](https://cdn.confluent.io/wp-content/uploads/stream-lineage-visualization-data-mesh.jpg)

> A good self-serve data mesh platform will track and account for all consumers and producers of the data products, and provide you with the lineage of the dependencies. Lineage helps identify data product consumers, so that they can be notified about upcoming deprecations, deletions, and breaking schema changes.

> Try This Open-Source Data Mesh Prototype and Learn How It Wor

> This prototype is written in Spring Boot Java with an Elm frontend. It is backed by Confluent Cloud, including Apache Kafka, ksqlDB, the schema registry, and the recently released data catalog. You can try out the hosted version, or you can run (and fork) your own copy of the prototype by following the steps outlined in the GitHub repo. The source code is freely available for you to use however you like. 
If you deploy this prototype locally, it will create a cluster, several Kafka topics, a ksqlDB application, and some data generators to populate the topics. Schemas are registered to the schema registry as events are produced, while the data catalog stores the data product’s metadata. 

![OpenSource](https://cdn.confluent.io/wp-content/uploads/data-mesh-prototype-diagram.jpg)

> Our prototype interacts with Confluent Cloud via REST APIs. A data product’s metadata is stored in the data catalog and includes a reference to the associated topic and schema. This information is pulled into the prototype when viewing data products for discovery and publishing. The prototype also interacts with hosted ksqlDB via its REST API to create the sample business applications, kicking off a ksqlDB application with the long-running query.

> Completing the Data Mesh

> The idea of this post is to be a definitive guide, yet our prototype only takes you part of the way. So what do you need to build the ultimate data mesh? There are a few more pieces that need attention:
Notifications: Integrate with an email or messaging system to notify people when a data product is created, updated, deprecated, or deleted.
Automated regulatory validation: Validating data products against various data protection acts (such as GDPR), security compliance checks, and data locality policies that prevent data from leaving certain regions.
Integrated sign-off process: Validate data products during code review, such that breaking changes cannot accidentally be deployed. Changes to the data product need to be validated by existing consumers and signed off before committing.
Automatically handle incompatible data product changes: In the case that breaking changes are required, automatically creating a new data product, deprecating the old one, and notifying the existing consumers to migrate can reduce errors. You may also need to automate support of both the old and new data products until all consumers migrate to the new one.
Additional event stream metadata: Event streams may contain only recent data (eg: last 30 days), a compacted snapshot of state, or all events ever published to the product. Exposing this information enables better self-service and tempers user expectations.

> Conclusion

> The data mesh is a federation of data products, sourced across the various domains of an organization, and used by other domains for their own business purposes. It is not centralized like a data warehouse or data lake. 
Event streams are the best implementation medium for a data mesh. They provide highly scalable and efficient access to all consumers, letting each one choose how to use, process, store, and act on the data. They’re also incredibly successful at bridging the analytical/operational divide, giving a consistent view of data for both batch and streaming users while reducing data quality issues. The formalization of data access–meaning all teams use the same interface to the mesh–makes it easier to track where data is going and who is accessing it, promoting strong governance and lineage tracking across your domains. 
Just as containers and container workload management systems like Kubernetes have transformed the way we build and deploy applications, event streams change the way we access data. By publishing event streams as well-defined and well-documented data products, you provide your business with the data building blocks needed to power event-driven microservices, content serving applications, analytical workflows, batch jobs, and many other services. A good data mesh will make it easy to access, use, and publish data products. Your time should be spent using the data and building applications, not struggling to find and interpret data, nor building your own access mechanisms. 
This prototype shows how a data mesh can work in practice. We encourage you to go try out the prototype, fork it for your own needs, and join us in our community if you have any questions.


**OPEN CODING TRACE:**

cdc

event_streaming_backbone

schema_registry

data_lineage

container_registry

**AXIAL CODING TRACE:**
added:
``` python
    
    cdc
    event_streaming_backbone
    schema_registry
    data_lineage
    container_registry
    
```































