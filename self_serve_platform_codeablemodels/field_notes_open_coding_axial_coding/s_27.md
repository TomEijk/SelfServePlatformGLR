# s 
27
## url
https://www.mo4tech.com/how-does-the-banking-industry-build-a-self-service-data-platform.html
## tiny url
https://tinyurl.com/self-serve-platform-s27
## archive url
https://bit.ly/self-serve-platform-s27
## title
How we scale our data platform efficiently and reliably
## source code
no
## example
yes
## source type 
Practitioner Audience Article
## author type
Practitioner
## references

**AXIAL CODING TRACE:**
``` python
s27 = CClass(source, "s27", values={
    "title": "How does the banking industry build a self-service data platform?",
    "url": "https://www.mo4tech.com/how-does-the-banking-industry-build-a-self-service-data-platform.html",
    "archive url": "https://bit.ly/self-serve-platform-s27",
    "tiny url": "https://tinyurl.com/self-serve-platform-s27",
    "author type": "Practitioner",
    "type": "Practitioner Audience Article",
    "example": True,
    "source code": False})
```

# coding

> Building a Data Mesh on Databricks — Fast

> How to build a data mesh platform inside Databricks

![Unsplash](https://miro.medium.com/v2/resize:fit:786/format:webp/1*W8Iwg-wftbeMhwRNc0noUQ.jpeg)

> Databricks enables teams to build data mesh platforms with extreme speed, and low maintenance needs.
But building a databricks-based data mesh platform comes with unexpected downsides as well.
In this article, I’ll outline a possible setup using an imaginary team structure and help you understand how a data mesh on databricks works.
This article is an extract from a draft of the book “Data Mesh in Action” in MEAP at Manning, which I am co-authoring.

> Why Databricks as Data Mesh Platform?

> Unlike AWS and GCP versions of the data meshes, a data mesh on Databricks looks a lot like a centralized data setup. At least with respect to technology. That brings obvious benefits, namely ease of setup and typical centralization benefits, and two big not-so-obvious drawbacks:
By centralizing large parts of the infrastructure, we are again subjecting ourselves to central bottlenecks. The effect of this is much smaller when using a hosted shared service, but it is still there. Additionally, we are physically centralizing the data in one place.
By centralizing parts of the process, we’re making the “data domain ownership” a lot harder to understand. In such a technical architecture it is of prime importance to put the focus on the processes, the people, and the organization.
On the flip side, this data mesh likely doesn’t need more than one data engineer to maintain the technical infrastructure inside a small to medium-sized company. Inside domains, you should be able to cover the needs with one data scientist/engineer per domain.

> 8.3.1 Self-Serve Data Platform architecture

> Our Data Mesh has a couple of participants.
Team Black & White are both development teams with one data scientist inside the team.
Team Gray is just a one-person team maintaining the platform on its own.
Consumers: we have a few business analysts and a recommendation system all of which mostly consume data.
Our business analysts are SQL-savvy and can do some work with Python inside notebooks.
The figure below depicts our architecture this time. Notice how the contexts are a bit more blurry. You could of course “separate out” the contexts of team Black & White but we chose not to in this case.
Let us again identify the different components of a Data Mesh, first the components of the platform, then the data product part.

![Databricks](https://miro.medium.com/v2/resize:fit:786/0*ELeT-8HKXn49dRw9)

> Identifying the components of the Platform

> In our architecture, the platform interface and the kernel are largely composed of Databricks and related services. The team of one could start to provide templates or some kind of service for provisioning of the Kinesis and Fargate components, but that would probably clash with the idea of having a very lean platform team.
What the platform team will have to do is to provide guidelines, a one-pager of how to use Databricks inside the data mesh. How to work with access rights, where to store all data products and the likes.

**OPEN CODING TRACE:**

infrastructure_as_code

**AXIAL CODING TRACE:**
added:
``` python
    
    infrastructure_as_code
    
    declaratively_create_dp
    
    maintenance_needs
    
    data_transformation_orchestration
    
    data_transformation_function
    
```

> Identifying the components of the Data Product

> All data products are built on top of Databricks. Let’s describe them in detail.
The data product owned by team Black is a bit more involved. The upstream data is sourced from an operational RDS instance and some other systems. To do so, the team uses a custom AWS Fargate task to load the data periodically into an S3 bucket. From there on, a Databricks spark batch job picks up the data and shoves them into Delta Lake. Then a Databricks notebook is used to transform the data into a data product and put it again into the Delta Lake form where it is exposed to others to use. The Delta Lake dataset in this case forms the data output port.
The data product owned by team White uses two upstream datasets. A user periodically inputs data into an AWS S3 bucket which is picked up by a batch job and delivered into the Delta Lake for temporary storage. This forms a push interface for the data product. On the other side, the team has provided a custom AWS Kinesis Data Stream. Operational components push data in real-time into the stream which then is picked up by a Databricks streaming job. Again a notebook is used for transformation and to produce the data output.

![White](https://miro.medium.com/v2/resize:fit:786/0*oQBcyvQXy7NdKVme)

> Remarks

> The workflows on Databricks are very Databricks specific. Teams can use the infrastructure to manage access rights for data products and analysts can use the so-called unity data catalog to explore data.
The data mesh principles work out a bit differently if we employ an architecture like this. The Databricks centric architecture limits the freedom of teams quite a bit, and there is no easy option to “opt-out” of the platform. Hence this setup only works well, if you either work around these limitations or make sure by governance measures, that the teams are aligned in the tool choice. With this, this architecture does bring the risk of slipping back into a non-data mesh-like mode of operation.
This makes this architecture much more suitable for younger companies. This architecture is also more aimed at data engineers and data scientists. This means it targets companies with a higher percentage of data people inside, whereas the GCP & AWS options are also accessible for software engineers.

**OPEN CODING TRACE:**

data_governance_function

access_control

data_catalog

accessibility

**AXIAL CODING TRACE:**
added:
``` python
    
    access_control
    data_catalog
    accessibility
    data_governance_function
    
```

> Variations

> Databricks in itself could be replaced by other bigger data platforms like the Snowflake ecosystem which is already offering almost all of the components needed to implement the architecture from above.
To scale up this kind of data mesh architecture you’ll need to enable the “opt-out” option for every participant. This can be achieved by adding an external data catalog and reducing Databricks mostly to the transformation tooling, which still is a powerful toolset. Your setup could look like this:
Teams store data in their own data stores
Ingestion tooling takes that data and puts it into Databricks accessible data stores.
Databricks is used to do transformations on these datasets, and produce data products.
A query engine like trinoDB is used to connect different data products together.
A central data catalog makes sure data products can be stored anywhere and still be accessible & findable.

**OPEN CODING TRACE:**

distributed_file_storage
data_source_ingestion
no_code_transformation
query_engine
data_catalog

**AXIAL CODING TRACE:**
added:
``` python
    
    distributed_file_storage
    data_source_ingestion
    no_code_transformation
    query_engine
    
```

> Databricks architecture summary

> The architecture described in this section is very much focused on using the data-transformation powers of Databricks to the maximum. It is thus very intertwined into the Databricks platform. We also saw that we need a bit of help in getting data into the central Delta Lake to unleash the Databricks powers.
The one big strength of this setup is the minimal maintenance effort with great support for all the analytical workloads companies usually want to take on.
On the flip side, we see limited support for development teams and the lock-in effect into the Databricks platform.
Take a look at the summary profile below.

![Summary(https://miro.medium.com/v2/resize:fit:786/format:webp/1*vuNTeZ1mqcsZoxAvsFfHoQ.png)]













































