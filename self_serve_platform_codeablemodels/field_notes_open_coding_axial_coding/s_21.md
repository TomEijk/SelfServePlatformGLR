# s 
21
## url
https://radiantadvisors.com/storage/uploads/2c152ae4b.pdf
## tiny url
https://tinyurl.com/self-serve-platform-s21
## archive url
https://bit.ly/self-serve-platform-s21
## title
ACHIEVING ENTERPRISE SELF-SERVICE DATA
## source code
no
## example
yes
## source type 
Practitioner Audience Article
## author type
Practitioner
## references

**AXIAL CODING TRACE:**
``` python
s21 = CClass(source, "s21", values={
    "title": "ACHIEVING ENTERPRISE SELF-SERVICE DATA: THE SIX PILLARS OF COMPREHENSIVE CAPABILITIES",
    "url": "https://radiantadvisors.com/storage/uploads/2c152ae4b.pdf",
    "archive url": "https://bit.ly/self-serve-platform-s21",
    "tiny url": "https://tinyurl.com/self-serve-platform-s21",
    "author type": "Practitioner",
    "type": "Practitioner Audience Article",
    "example": True,
    "source code": False})
```

# coding

> ACHIEVING ENTERPRISE SELF-SERVICE DATA

> The Six Pillars of Enterprise Self-Service Data

> An enterprise self-service data platform empowers business users to work
intuitively with data in an interactive way that quickly applies their unique
business domain knowledge to solve complex challenges independently of
IT developers and administrators. This concept goes far beyond self-service
BI’s custom-tailored reports and analyst tools that extract data into another BI
server environment for business users to access. Instead, enterprise self-service
data follows the end-to-end process that begins with business users searching,
identifying and properly accessing data to understand and prepare it to curate
valuable enterprise data insights that others can leverage.
These essential six pillars serve the needs of three different groups within
the organization: business users, IT development and the enterprise data
management culture. Working together transparently, users competently
participate in the self-service data process, thus achieving true data
democratization.

> #1. Data Discovery and Data Catalog

> For each business question to be answered and hypothesis to be verified, a
business user must apply his or her domain knowledge to uncover and explore
any data that may be relevant and valuable for the task at hand. The ability for a
business user to quickly find and understand available data sources is essential to
the self-service data process.
A data catalog helps business users search across all data sources for the data
that’s relevant to their specific needs – to see data characteristics and capture
related knowledge. A data catalog and index support the search and discovery
process for data from all available data sources (including databases, flat files,
unstructured data or SaaS APIs). The ability to use keywords to search and refine
search terms iteratively until relevant results are generated saves time and allows
users to explore data using familiar methods and syntax. Natural language query
further streamlines this interaction and returns better results with semantic
context.
The data catalog also helps business users evaluate potential data sources
regarding context and applicability. The first aspect of this is the information that
can be captured and represented in technical and business glossary definitions.
This data is relatively static, such as data type, format and relationships between
data sets. A second aspect is the highly valuable information about institutional
or tribal knowledge existing with people in the organization relative to the data.
Combined, these sets of information quickly allow a business user to determine
whether a data source will be useful. To capture this information in the data
catalog, business users must be able to easily enrich the data with comments,
ratings and reviews that can be preserved for reference and future use.

**OPEN CODING TRACE:**

data_catalog

data_governance_function

**AXIAL CODING TRACE:**
added:
``` python

    data_governance_function
    
    data_catalog
     
```

> #2. Data Discovery and Security

> Organizations must ensure that access to data and data sources is fully governed
and secured so that business users only access what they are allowed to and
understand how to use the data appropriately. Establishing and sustaining
governance and security procedures that are transparent to users and always
present – but do not unnecessarily restrain people while working with data –
provides safeguards and ensures that enterprise data compliance and standards
exist for all business users. Data authorization ensures that business users have access only to data that
has been granted to them. A structure of roles and groups for users along with
permissions for create, read, update and delete (CRUD) data operations must
be established in a centralized enterprise function to manage the relationship
between data and users. Within the data governance program, data owners are
responsible for establishing criteria for who has authorized access to their data
along these parameters.
Data compliance needs to be established broadly and deeply from the
beginning to prevent inadvertent data breaches, which can be extremely costly.
Companies are responsible for complying with their industry, federal and global
regulations, such as Health Insurance Portability and Accountability Act (HIPAA),
the Payment Card Industry Data Security Standard (PCI DSS), all personally
identifiable information (PII) rules, and the upcoming European General Data
Protection Regulation (GDPR), and must embed safeguards in data processing to
eliminate risks of non-compliance events. The most common starting point is to
implement data encryption, masking, tokenizing or other data assignments at
the data ingestion step before data is accessible by users.
Data governance is instrumental to fueling enterprise self-service data adoption.
When done correctly, governance will incorporate the collaboration and
community features of defining and validating self-service data rather than stifle
curation with restrictions regarding how to handle various data sets. Governance
is especially important to enterprise self-service data, where the goal is to create
new derived data sets that must also have definition, context and governance.
Data lineage intuitively shows any business user what data sources
were used and the series of data preparation steps in the workflow. This
information takes a business user from simply accepting the data output
definition to understanding how the data output was derived. The user
can then make their own determination of acceptance and validation for
their needs. Data lineage also improves the accuracy and speed of impact
analysis whenever a change in source data or a transformation is made. This
is not always possible when the data workflow has steps outside the data
preparation environment. This is especially important when regulated and
secure source data is being used to create derived data sets that must also
comply with privacy and regulations. 

**OPEN CODING TRACE:**

data_lineage
agility
crud_operations
agility

**AXIAL CODING TRACE:**
added:
``` python

    crud_operations
    agility
    data_lineage
    accuracy = CClass(force, 'Accuracy')
    speed = CClass(force, 'Speed')
     
```

> #3. Data Preparation with Workflow Automation

> At the heart of self-service data is the ability for business users to work with
data in ways that go beyond what desktop spreadsheets or small databases
could do – without requiring a developer to program any data integrations or
complex business rules. In the hands of a business user, business knowledge
can be seamlessly applied in data discovery, exploration and integration. Within
this construct, the business user creates a data workflow that is visually and
logically easy to read, design, change and see how data moves through their
transformations. The ability for business users to prepare data in an agile process
produces the exacting data sets needed by the business for decision-making and
complex analytics.
The bulk of data preparation workflows centers on three main steps. The first
step is to profile data sets to understand the quality of the data and determine
what needs parsing, cleansing, filtering or enriching. Next is to analyze multiple
data sets for integration while maintaining the correct business context for inner
and outer joins across several fields and choosing which fields should be in the
final output. Finally, the output data is prepared for publishing with various
aggregations and filters. These three steps are highly iterative and powerful in
the hands of business users who are able to seamlessly apply business domain
knowledge to their data sets during the workflow development.
To support this process, workflow automation operationalizes the data
preparations that business users have created. Most data preparations will not
be a one-time use for business users, and saving the preparation work for reuse
as new data arrives from operational systems or external sources eliminates
rework and saves time and resources. Data preparations fall into one of three
categories from which users will select to deploy their workflows for execution:
manual, scheduled or event-driven. In some cases, it’s not possible to remove
the dependency on the business user, and manual operations are needed for
processing. In most cases, refreshing published data sets can be scheduled as
appropriate for business needs. For event-driven situations, a form of change
data capture (CDC) can be utilized, such as the arrival of a file in a specified
location or database activity that initiates the data preparation job.
Overall, workflow automation requires a visual and highly intuitive interface for
business users to understand how to operationalize their data preparations. A
good workflow automation engine for enterprise self-service data will coordinate
all data preparation jobs at scale, manage dependencies and workloads, and
provide audit logs for reporting.

**OPEN CODING TRACE:**

workflow_automation_engine

loging_management 

**AXIAL CODING TRACE:**
added:
``` python

    workflow_automation_engine = CClass(pattern, 'Workflow Automation Engine')
     
    log_management
```









































