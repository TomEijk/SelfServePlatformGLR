# s 
42
## url
https://medium.com/@abeauvois/data-platforms-the-future-7175a354bea2
## tiny url
https://tinyurl.com/self-serve-platform-s42
## archive url
https://bit.ly/self-serve-platform-s42
## title
Data Platforms: The Future
## source code
no
## example
yes
## source type 
Practitioner Audience Article
## author type
Practitioner
## references

**AXIAL CODING TRACE:**
``` python
s42 = CClass(source, "s42", values={
    "title": "Data Platforms: The Present",
    "url": "https://medium.com/@abeauvois/data-platforms-the-future-7175a354bea2",
    "archive url": "https://bit.ly/self-serve-platform-s42",
    "tiny url": "https://tinyurl.com/self-serve-platform-s42",
    "author type": "Practitioner",
    "type": "Practitioner Audience Article",
    "example": True,
    "source code": False})
```

# coding

> Data Platforms: The Future
The Evolution of Data Platforms
This series of three articles (Data Platforms: Past, Present & Future) is a pedagogical experiment to help apprehend the data stack and its evolution.
In the previous articles (see them here and here), we’ve described the Data Platform from the past (since 2014) to the current one in 2022.

> Introduction
In this last article, we’ll focus on the Future of data platforms and how, with new data challenges, data-driven companies will make better decisions at a lower cost and at the best time (Time To Insight), even at scale.
Some data-driven companies will improve existing tools and processes without integrating new data types from new industries (i.e crypto-web3, solid, and metaverses). They’ll keep following mutation patterns from web2, while others will absorb new disruptions/inruptions and reshape their data strategy and tools accordingly.

![Gartner](https://miro.medium.com/v2/resize:fit:720/format:webp/1*-1Vn6EUafYADjDmRSa2eng.png)

> Adapting tools and processes in the web2 era
This adaptation will result from all responses required by all data mutations.
Mutation #1: From data consumption to data product
The same as PLG for the product, this vision pushes us to follow the same path for Data Product-Led Growth.
And companies that treat data like a product can reduce the time it takes to implement it in new use cases by as much as 90%, decrease their total ownership (technology, development, and maintenance) costs by up to 30%, and reduce their risk and data governance burden.
source: Harvard Business Review https://hbr.org/2022/07/a-better-way-to-put-your-data-to-work

![Traditional](https://miro.medium.com/v2/resize:fit:720/format:webp/1*I1hICgt2HTaxVB0CFfNtAQ.png)

> Mutation #2: From databases to data lakehouses
A lot of good articles exist about these topics. See references at the end of this article. For pedagogy, let’s summarize some definitions and fundamental differences.
Database: a collection of data organized for storage, accessibility, and retrieval.
Data warehouse: a database that integrates copies of transaction data from disparate source systems and provisions them for analytical use.
Data Lake: put everything (files) in storage in their original raw state.
Data lakehouse = Data warehouse (BI)+ Data lake (mainly used by AI)
The convergence of data warehouses and lakes is called “data lakehouse” by Databricks or Data Cloud by Snowflake, and it’s a system design that combines the changes in data captured from a data warehouse with the low-cost storage of a data lake.
Mutation #3: From data stack to data mesh/fabric
A data fabric and a data mesh provide a conceptual architecture to access data across multiple technologies and platforms. And tends to influence multi-tenant companies in their data platform architectures.
But
data fabric is technology-centric, with an architectural approach that tackles the complexity of data and metadata in an intelligent way
data mesh focuses on organizational change, on people and process rather than on architecture
According to Dehghani, distributed data mesh suggests a new architecture that is marked by four primary characteristics:
Domain-oriented decentralized data ownership and architecture;
Data as a product;
Self-serve data infrastructure as a platform;
Federated computational governance.
According to Forrester’s Yuhanna, the critical difference between the data mesh and the data fabric approach are in how APIs are accessed.
For example, here could be the layers and blocks of a data platform applying (partly) the data fabric + data mesh fundamentals.

![DataPlatform](https://miro.medium.com/v2/resize:fit:720/format:webp/1*OeWKGtL1sKRLUw8ibUPPcA.png)

**OPEN CODING TRACE:**

api_catalog

build_deploy_monitor_dp

data_catalog

costs

no_code_transformation

data_transformation_orchestration

access_control_management

accessibility

**AXIAL CODING TRACE:**
added:
``` python
    
    api_catalog
    build_deploy_monitor_dp
    data_catalog
    no_code_transformation
    data_transformation_orchestration
    accessibility
    costs
    access_control_management
    delegated_ownership
    
``` 

> Whatever the architecture, everything here is ending, directly or indirectly, in an IaaS or PaaS like AWS, Google Cloud Platform, Azure, OVH Cloud, Scaleway, and so on.

> Mutation #4: Cloud it all and share it all to focus on what matters
Whatever the organization context, it needs to improve data quality from A to Z, which involves collectively contributing to data via a data platform, not just “read-only” accessing it today, as it‘s still the case with “modern” data stacks.
Everybody should access it, not just Data Engineers, Data Analysts, Business Analysts, and other internal managers. External people like partners, shareholders, advisors, and other stakeholders should access the data platform with specific rules (governance).
In this context, we need to share data at any stage in the lineages, track changes, and avoid regressions by managing data as we manage code (aka Data as code).
So many great tools exist, but as stated below, we can’t build a full-stack platform by simply assembling the best parts:
When every party and system requires a DIY approach to security, access controls, auditing, compliance, and governance, those problems take on a life of their own, becoming heavy lifts that can dwarf the original issue of simply moving data between parties.
credits: From former AWS Lambda creator Tim Wagner https://thenewstack.io/the-real-time-data-mesh-and-its-place-in-modern-it-stacks/
The only way to solve this is to get a uniform solution from ELT to RELT, and it takes years for data-driven companies to do this independently.
Hopefully, today every open-source data software organization is creating its new “Managed/clouded/as a service” solution to monetize its “core OS technology” efforts. See Dbt Cloud, Airbyte Cloud, Prisma Data Platform, MongoDB Atlas Data Platform, and so on. We even see emerging “Full-stack Data Platform” services like those from Selfr.io, CanvasApp.com, 5x, and Keboola, to name a few.
It’s now possible for small companies (less than 500 people) to start fast by using the “as a service” version of these great technologies without paying too much in technical stack/team ownership and, at the same time, investing progressively in expertises and DIY tools in a Product-Led Growth way. Keeping in mind infrastructure is not the critical enabler for success:
“Uber, the world’s largest taxi company, owns no vehicles. Facebook, the world’s most popular media owner, creates no content. Alibaba, the most valuable retailer, has no inventory. And Airbnb, the world’s largest accommodation provider, owns no real estate,”
We could see in the future something like “ABC, the world’s most popular data services provider, owns no databases.”
Now that we have new “managed/turn-key solutions” on the infrastructure side, the question of ownership is not a goal anymore. The goal is to make better decisions at a lower cost, at the best time, and even at scale.

> Mutation #5: from code to low/no-code
Whether we “rent” or “own” a data platform, data teams must spend time on data strategies, use cases, data product delivery, and other high-level challenges. But so far, they are spending 80% of it fixing pipelines — e.g. after a developer changed the User schema from the CRM, which crashes x KPI calculations based on it. This is plumbing stuff that should not deserve the human brain of a Data Engineer (or at least 80% of it) but rather be done by AI once semantic standardization with be achieved — how many ways engineers have modeled a User entity (see Solid and W3C efforts later in this article).
This is not yet the case, but meanwhile, we can save scarce and highly qualified resources thanks to “low/no-code” tools. Without detailing them (so many articles/videos about it), it’s a way for less technical people to read/write information to a data platform visually. You can code a process, fix a schema and reload, configure governance policies or roles, update a pipeline orchestration, visually debug a pipeline crash from the lineage, etc.

**OPEN CODING TRACE:**

semantic_data_layer

schema_registry

**AXIAL CODING TRACE:**
added:
``` python
    
    semantic_data_layer
   
   schema_registry
   
``` 

> Mutation #6: from Data Platform to Community-driven Data Platform
Now that we can scale and safely read/write to the data platform with real-time no-code human-machine interactions, AI could be used to create new interfaces required by this platform, not only for science analytics. It would not allow only data and information inputs but also knowledge inputs.
What if knowledge management systems and internal community networks were integrated with the Data Platform? Kind of “Community-driven Data Platform.”
If we think about it, from one side, KMSs are adding more sophisticated tools to search and organize knowledge and analytics tools to extract insights or meta-knowledge from it. On the other side, Data tools are all adding collaborative features to their analytical dashboards. Corporation community networks are adding both KMS features and Analytics features for the same reasons. When new data regulations appear, these tools add different logic for solving the same problem in their siloes.
How do we reduce this kind of growing redundancy and, by the way, the growing complexity?
Would it make sense to merge them all?

> Mutation #7: from owning everything to renting as much as possible
The more value organizations want to offer, the more challenging it is to do without adding more features and complexity, making it feel simple to use.
“Simplicity is the ultimate sophistication.” — Leonardo da Vinci.
How did we manage it so far?
The answer before 1990 was integration + “clean” architecture. Put it all in the same monolith!
It doesn't scale said startups since 1995. Let’s break it into Infra + Apps with the help of the Web (invented in 1989 by Sir Timothy Berners-Lee).
It’s still too heavy. Let’s put it all into Paas and focus on SaaS Apps only.
Wait, Saas Apps are producing so much data. Let’s build Data Platforms.
It doesn’t scale said startups since 2015. Let’s break it into Infra (Data Cloud)+ Data Apps.
It’s still too heavy. Let’s put it all into Data as a Service and focus on BI, AI, and OPs (business apps for marketing, sales, and communication,… mainly Saas Apps).
Wait, BI tools are all Saas Apps today; same for most operational tools (sales, marketing, productivity & no-code tools, …). Saas Apps now provides even AI as a service.
The only things companies need to own with a specific added value are:
Specific data / AI models
Specific knowledge (teams / best practices)
Specific technologies (if any)
All the rest is a commodity that various massive operators can provide. This is what capitalism tends to do: Energie, Transports, Pharma, and all other industries are provided by massively centralized commodity providers, so it does with the digital industry, with (human?) data as the new oil.
Is there a path for a decentralized architecture that gives users control of their data, offers incentives to participate in decisions, and shares socially acceptable collective values?
Will this create new paths on how data-driven companies will include new disruptions/inruptions and create value?

> What could be the new paths?

> Path #1: web3
So far, the majority of the data industry has focused on data sources coming from:
Saas Apps (JSON)
Own operations incl. Sales, marketing, AI, BI, …(SQL, NoSQL, Graphs)
Files & Logs (txt, csv, xls, yaml, json, …)
IoT devices
Mobile devices
Shortly, significant flows of new data-streamed sources will join them:
Decentralized Apps (dApps)
Oracles (Verified data sources for dApps)
NFT meta-data and transactional data
IPFS (Inter Planetary File System = decentralized files)
Metaverses interactions and transactions
We could say the type of data will be the same (JSON, SQL, …), but from the point of view of the Data Engineer and his data stack, these are new beasts with their specific data connectors, sync. frequencies, real-time series, identification & permissioning systems, to new a few.

> Path #1: web3
One of the explicit promises of the crypto-web3 (or Dorsey‘s web5) is precisely to re-distribute power between peers (dedicated servers/nodes) of a blockchain network and users of this network (bots and humans) without compromising privacy.
Unfortunately, like for web1, Internet Providers and some big players got dominant; for web 2.0, social media got dominant. The semantic-web3 was never born, and then crypto-web3 went oligopolistic, with massive VCs owning most of the Coins issued by blockchains DAOs and nodes networks (mainly validators) by a small group of companies.

> Path #2: Web5
While crypto-web3 is still in its infant age and maturing, Dorsey promises that instead of creating new blockchains, we could use the mother of all: Bitcoin.
One of the key aims of the Web5 initiative is to fill a void in the web’s identification system by “building an extra decentralized web that puts you in control of your data and identity.”
Currently, identity and personal data are held by third parties. However, under the Web5 model, consumers will manage their own data and identities through decentralized web nodes.
source: https://www.euronews.com/next/2022/06/15/move-over-web3-former-twitter-ceo-jack-dorsey-wants-to-launch-web5-based-on-bitcoin

> Path #3: Solid
But another path exists and was born around 2018: Solid (derived from "social linked data").
A debate emerged from Sir Timothy Berners-Lee: When it comes to the interoperability, speed, scalability, and privacy dilemma and “…you try to build that stuff on the blockchain, it just doesn’t work,” he said. Solid is built with standard web tools and open specifications.
The dilemma he talks about says that we can’t optimize one of these attributes without sacrificing others. E.g., if you can keep the speed at scale, then you can’t be efficient on privacy. Similar to the blockchain trilemma is a framework that says that every public blockchain has to sacrifice one of three features: decentralization, security, or scalability.
And we have to admit that so far, the crypto-web3 industry has only addressed it partially
Sir Timothy Berners-Lee added:
people were using the web just for documents, not for the data of a big web-wide computer…Kind of read-only web, With Solid, it becomes a read-write web where users can interact and innovate, collaborate and share.
source: https://www.inrupt.com/one-small-step-for-the-web
… private information is stored in decentralized data stores called “pods,” which can be hosted wherever the user wants. They can then choose which (d)apps can access their data.
source: https://thenewstack.io/the-real-time-data-mesh-and-its-place-in-modern-it-stacks/
Berners-Lee says Solid serves two different purposes:
Preventing companies from misusing our data for unsolicited purposes, from manipulating voters to generating clickbait.
Providing opportunities to benefit from our information.
Healthcare data, for instance, could be shared across trusted services to improve our treatment and support medical research.
Our photos, meanwhile, could be supplied to Facebook friends, LinkedIn colleagues, and Flickr followers without having to upload the pictures to each platform.
This evokes Berners-Lee’s original aim to make the web a collaborative tool.
“I wanted to be able to solve problems when part of the solution is in my head, and part of the solution is in your head, and you’re on the other side of the planet — connected by the internet,” he said.

> Path #4: Real-time streaming
Real-time streaming pipelines and operational analytics will continue to push through.
Compared to batch pipelines which can end in zombie dashboards, Real-time streaming pipelines drive real-time decision-making via specific dashboards. Some use cases do make little sense without real-time, like in-app personalization, churn prediction, inventory forecasting, and fraud detection & dynamic pricing.
We’ve seen how web3 will provide new data streams that will push forward this trend.
At the same time, there are many voices out there who believe that real-time streaming pipelines are almost always overkilled.
Companies are investing in real-time infrastructure as they’re going from being data-driven (making decisions based on historical data) to becoming data-led (making decisions based on real-time and historical data)
source: https://www.validio.io/blog/5-data-trends-in-2022

**OPEN CODING TRACE:**

event_streaming_backbone

interoperability

speed

scalability

real_time_data

**AXIAL CODING TRACE:**
added:
``` python
    
    event_streaming_backbone
   
   interoperability
   
   speed
   
   scalability
   
   real_time_data
``` 

> Conclusion
We merely touched upon the changes data-driven companies will face in the coming years.
Current mutations in the web2 era will by themselves deeply impact the companies’ strategy, organization, and processes. They will amplify existing challenges and create new ones.
But new disruptions/inruptions like web3, Solid, and others, could require fundamental re-implementation of data platforms specifically but also information systems in general.
Whatever these disruptions/inruptions, the interoperability, speed, scalability, and privacy dilemma will be the new grail. Digital industries, like others before, will have to mature with better harmonization and consistency, with more standards and tools to accelerate their implementations. Regulations are progressively catching up with data privacy and some web3 concepts, companies will have to comply. New jobs and organizations will emerge. The companies’ ownership should be reduced in favor of a more flexible/reactive “as a service”/”all in the cloud” approach, only keeping strategic assets like specific data, specific AI models, core business technologies, specific knowledge, and skills. Data Platforms should allow more and more stakeholders to self-serve and self-contribute to data quality and analysis, without requiring engineering profiles.
This ends this series around Data Platforms. If it is of some interest, don’t hesitate to share and clap (more than once ;).


































































