# s 
30
## url
https://dzone.com/articles/top-challenges-in-data-mesh-and-how-a-data-product
## tiny url
https://tinyurl.com/self-serve-platform-s30
## archive url
https://bit.ly/self-serve-platform-s30
## title
Deploying Data Products at the speed of the business
## source code
no
## example
yes
## source type 
Practitioner Audience Article
## author type
Practitioner
## references

**AXIAL CODING TRACE:**
``` python
s30 = CClass(source, "s30", values={
    "title": "The Definitive Guide To Building A Data Mesh With Event Streams",
    "url": "https://dzone.com/articles/top-challenges-in-data-mesh-and-how-a-data-product",
    "archive url": "https://bit.ly/self-serve-platform-s30",
    "tiny url": "https://tinyurl.com/self-serve-platform-s30",
    "author type": "Practitioner",
    "type": "Practitioner Audience Article",
    "example": True,
    "source code": False})
```

# coding

> Challenge 4: Lack of re-use and duplication of pipelines and data
Solution: Allow analytics teams to easily find and understand existing data products across the whole platform
Addressable:Meaning it must participate in a global eco-system with a unique address that helps its users to programmatically find and access it.
The catalogue includes a global unique id and description of the product and artefacts and a way to execute
Each product each artefact includes a globally unique resource identifier so as to be able to be addressed, located and executed
Self-describing: One needs to understand it (business as well as technical structure) without going back to the domain teams.
Again, the product publishes all business and technical data about it in the catalogue. Typical artefacts that are sourced from the product include table/schemas/structures, metrics from BI source files etc…
Secure: Be able to be accessed security with global policies
All the artifacts are governed by global role-base-access, GDPR, Info security, data sovereignty and other policies. This includes not just the data but the execution of the processing engines and access to the visualisations.
Trustworthy: Providing data provenance and lineage and data quality from the owner
All data products publish their access to data and other products so one can follow the line of processing across the Mesh and see where it originated from.
Interoperable: Be able to reuse, correlate and stitch them together across domains for new use-cases
Products can be built on other products via well published end-points and share views, connections and other artefacts.

**OPEN CODING TRACE:**

container_registry

duplication

data_governance_function

data lineage

VMs

accessibility

api_catalog

infrastructure_as_code

**AXIAL CODING TRACE:**
added:
``` python
    
    data_lineage
    duplication
    container_registry
    VMs
    api_catalog
    infrastructure_as_code
    data_governance_function
    
```


![EnterpriseMesh](https://dataception.com/assets/img/DataProductsEcoSystemArch.png)

> To support the building of data products, one deploys an Enterprise data mesh infrastructure. Which is a self-service collection of services and allows business teams to create, share and build data products in a totally self-service way across the whole business.
These services are delivered as a PaaS (Platform as a service) and have a common execution and governance i.e. one they are all controlled using the same policy and security modes and the services can all be accessed using a common set of APIs
One major aim is to shrink the lead time to create and update new data products. Most (if not all) of the time should be spent by data and analytics creators on creating the actual logic, models, derived data and visualisations, not on ancillary tasks such as ingestion, understanding the data and platform engineering tasks common on lake and warehouse infrastructure.
One way this is done is, to avoid duplication of effort, provide the business analytics teams access to tools and services to bring on data and build truly end-to-end analytics without needing to constantly go to the central platform team for tasks. So, in effect this becomes data infrastructure as a platform rather than a data platform that requires central teams to build customer use-cases on. It enables teams building those data products to build new data products quickly and efficiently and with tools that they are familiar with, whilst providing a common set of architecture and skills framework.
It provides business user driven workspaces with a variety of user driven technologies (both back and front end) that can be spun up in safe sandboxes (cloud VMs or containers). The artefacts have central policies applied and easily searchable global meta-data about all the relevant other artefacts (datasets, dashboards, features etc.) that could be used to build the analytics.
Data products deployed on the data mesh share a lot in common with microservice architecture and are built on top of such. Data products run a composition of containerised components and microservices on either Function-As-A-Service or Container orchestration infrastructure. A data product only really qualifies as a data product if it can be found, executed and re-used. So, it needs to have a discoverable and executable endpoint that is found via the catalogue. Most importantly, it needs to be interoperable so that it can be orchestrated as part of other data products.
It’s important that a data infrastructure platform stays domain agnostic and as infrastructure. E.g., when you’re working on a component of the core infrastructure and you realize that you need to have some domain knowledge to solve a problem, there is a good chance you are careering towards building a monolithic data platform and this is something that needs to be avoided.

![Architecture](https://dataception.com/assets/img/DataMeshArchitecture.png)

> Lets do a deep dive into the technical architecture.
The mesh implements a distributed data and execution architecture but with centralized governance and standardization for interoperability. This is supported by shared and standardised self-serving data and cloud infrastructure.
The technical architecture is broken down into three layers.
Data Product layer – A distributed self-service design and run-time environment that allows users to run data products as self-contained units.
As an example, (the diagram below)– we have 4 data products.

![ProductLayer](https://dataception.com/assets/img/DataProducts.png)

**OPEN CODING TRACE:**

managed_compute_infrastructure
data_catalog
data_transformation_orchestration
version_dp
data_lineage

**AXIAL CODING TRACE:**
added:
``` python
    
    managed_compute_infrastructure
    data_catalog
    data_transformation_orchestration
    version_dp
    data_lineage
    data_source_ingestion
    
```

> All products need to self-publish into the catalogue along with meta-data which enables fast discovery by consumers browsing as well as analytics and data product creators. This avoids the “Swamp” effect, whereby artefacts are added to the infrastructure and either lost or the meaning has disappeared with a member of staff that has left the organisation long ago and took all the knowledge with them.
The Cloud Infrastructure layer - Allows users to spin up the data products as a collection of long running and/or ephemeral container-based components from templates. They implement a wide variety of data technologies in a cohesive way across a heterogeneous global hybrid cloud estate.
It provides single compute units and clusters, for distributed processing engines like Spark, Dask etc… It is self-service so people can use these clusters at a touch of a button without needing to care about actually operating the infrastructure i.e. without needing to get into container software APIs and complex configuration. It includes cost limits set by the business to avoid massive bills. It is delivered across all cloud providers and on-prem infrastructure provided by software like Docker and Kubernetes
Challenge 3: Centralized and monolithic
Solution:Allow the deployment, running and management of independent data products as containers or data microservices.
It also provides an abstraction into data infrastructure on the cloud. So, the user can spin up whatever service, database or NoSQL system they like without having to know any details of the configuration.
The layer also provides aspects like data product versioning, source control and life-cycle management.
The mesh allows a data product to be created from a system of data product templates that users use to build their data products in “project” workspaces.
Templates are made up of components that are configurations of technologies like databases, BI tools (Web and desktop based), processing engines, datasets, configuration, frameworks, web servers etc.
The components include the relevant type of data processing for the specific use-case. E.g. A SQL engine for BI queries, a graph engine for graph analytics, machine learning model frameworks (e.g. Python/ scikit-learn, R etc.).

![Template](https://dataception.com/assets/img/DataProductTemplateBrief.png)

> The template is stored in a computer readable configuration in the mesh that can be deployed by the infrastructure or built upon to create a new data product.
Templates could include instances of data pipeline tools like Alteryx, Apache Beam, Dagster, or databases like RDS on AWS or distributed processing frameworks like Spark or Dask, BI tools like Tableau or Apache Superset or even full solutions like Databricks or Sagemaker. It’s important to support whatever kind of technologies the business wants to use in the language they choose. If a user needs a new technology then it’s easy to deliver as a container or desktop app through a central governed platform on-boarding process. This means that we are still providing central services but they are also globally governed and interoperable.
The Data Access Layer – this surfaces all relevant data in the estate as close to raw as possible through a data virtualisation component. The data is indexed in a catalogue alongside business definitions and meta-data that describes the data
It includes connectors and a virtual view of all the data in the estate even if they reside in systems and databases, cloud storage, SAN and file systems and others infrastructure
The virtual views are surfaced through a metadata cataloguing component that surfaces information on all the different artefacts for people to be able to find easily. It is governed, not only by providing additional information about the artefact, but also by allowing standardized processes around, aspects like access, security and naming for attributes.
The way I think about it is a concept of “Curated Views”. This essentially allows users to have their own views of their data on top of their own cloud storage buckets. People have their own sandboxed cloud storage, that could be on-prem or on a public cloud provider or even scraped from a web-site. They create and land interact with the artefacts as part of a project that will ultimately become a data product. Any derived datasets are also catalogued and indexed in centrally so anyone (given the right permissions) can find them.
The data and metadata are accessible through a cloud abstraction layer, using the same governance layer that is used to apply access to services in the infrastructure and data product layers. It's important that the views are "synchronised" to the rest of the system i.e. the schema and meta-data is known and controlled by the management layer to stop the fragmentation that occurs using disparate components. This means when data or schemas change they are reflected in the views.
A key addition is data virtualisation which surfaces data sets that are external to the platform without creating time consuming data ingestion tasks.

![Virtualisation](https://dataception.com/assets/img/DataVirtulisation.png)

> The data mesh, in the original article, takes a different approach to the data connectivity between products. It talks out the concept of data ports, which are a means of accessing data by the data product exposing well defined interfaces e.g.SQL, REST etc... for each of the products datasets
For the Enterprise Mesh this is done via Data Virtulisation. Each dataset inside a product can be accessed through a common query layer that implements multiple query semantics e.g. SQL, Graph, Document, Key Value etc... This allows easy access to data as well as being able to expose data that lives outside the mesh (i.e. in he rest of the I.T. Estate) without the need of physically copying.
To dive into this further, as the diagram shows, data products have virtual views of data that are composites of the raw data in the source systems. They are implemented in the appropriate technology that is required for the product e.g. relational for random access SQL type use-cases, graph models for graph-use cases etc.
The Query layer stitches together queries across the domain “Views” and the catalogue provides the list of attributes (backed by business descriptions and other meta-data) that allows the analytics builders to pick and choose for each use-case. It also applies security and data policy at query time which avoids having to build separate data stores to support requirements contained in GDPR and data sovereignty regulations.
Data can be cached for large datasets or for systems that have sensitive or slow data access, but it’s important that they remain “synchronised'' i.e. one wants to avoid unnecessary copying of data that could result in it becoming out of sync with its source transactional system. It also shows aspects like usage (by all teams), lineage across the organisations, versioning and life-cycle events.
All this is needed in order to provide all the services necessary for the business teams to create data sets in a completely self-service but controlled way.
It’s important that this layer can surface addressable polyglot datasets from all types of data source without long ingestion task time-lines. The layer supports querying and returning result sets across a variety of data technologies. The obvious one is SQL based relational and data warehouse technologies, but it must include others like graph and document-based query semantics and result sets. Most data lake solutions struggle to do both in a consistent and self-service way.
The catalogue provides central discoverability, access control and governance of distributed domain datasets. This supports self-publishing of artefacts from a wide variety of cloud data storage options and allows domain data products to choose the relevant polyglot storage option for usage.

> Summary

> So, to summarise and bring together what we have discussed.
We started with the journey and some of the underlying aspects of current data platforms: centralized, monolithic, with highly coupled pipeline architecture, operated by silos of hyper-specialized data engineers and data scientists. We looked at the building blocks of data mesh as a self-service infrastructure; federated data products development, oriented around domains and owned by independent cross-functional teams who have their data engineers and data product owners embedded in their teams. All this using a common data infrastructure as a platform to host, prep and serve their insight and data products.
The way to think about this new approach is to treat domain orientated data products as a first-class concern and the tooling and pipeline as a second-class concern - an implementation detail. Steers away from the centralized data lake approach to a data ecosystem of data products that play nicely together. The key to success for self-serve data infrastructure is decreasing the time-to-market to create a new data product on the infrastructure. The infrastructure needs to provide a fully automated set of services that can be used wholly by the business teams to build “stand alone” data products. These must include, the data ingestion through configurations and scripts, data product creation scripts to put scaffolding in place and auto-registering a data product with the catalogue, etc.
Cloud infrastructure plays an important part as a backbone to reduce the operational costs and effort required to provide on-demand access to the infrastructure. Data virtualisation also needs to be implemented to further speed up data integration.
I hope this article has shown how the data mesh infrastructure goes beyond Data Lake and Hybrid Lake solutions that enable support of many business use-cases, in a cost effective and flexible way.
As always, comments and feedback are really welcome and even if you want to know more, drop me a line by using the social media links, below

**OPEN CODING TRACE:**

infrastructure_as_code

dependable_pipeline_management

distributed_file_storage

separating_storage_and_compute

data_lineage

metadata_management

data_catalog

polygot_storage_option

interoperability

secure_dp

container_registry

declaratively_create_dp

**AXIAL CODING TRACE:**
added:
``` python
    declaratively_create_dp
    infrastructure_as_code
    dependable_pipeline_management
    distributed_file_storage
    separating_storage_and_compute
    data_lineage
    metadata_management
    data_catalog
    polygot_storage_option = CClass(pattern, 'Polygot Storage Option')
    interoperability
    secure_dp
    container_registry
```




























