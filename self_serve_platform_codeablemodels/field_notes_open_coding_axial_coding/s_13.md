# s 
13
## url
https://www.advancinganalytics.co.uk/blog/2021/8/5/data-mesh-deep-dive
## tiny url
https://tinyurl.com/self-serve-platform-s13
## archive url
https://bit.ly/self-serve-platform-s13
## title
Data Mesh Deep Dive
## source code
no
## example
yes
## source type 
Practitioner Audience Article
## author type
Practitioner
## references

**AXIAL CODING TRACE:**
``` python
s13 = CClass(source, "s13", values={
    "title": "Self-Serve Data Platform",
    "url": "https://www.advancinganalytics.co.uk/blog/2021/8/5/data-mesh-deep-dive",
    "archive url": "https://bit.ly/self-serve-platform-s13",
    "tiny url": "https://tinyurl.com/self-serve-platform-s13",
    "author type": "Practitioner",
    "type": "Practitioner Audience Article",
    "example": True,
    "source code": False})
```

# coding

> Self-Serve Data Platform

> A Self-Serve Data Platform is effectively Platform-as-a-Service with Infrastructure as Code principles, with some additional abstraction to lower the barriers of entry needed to build data products.
Like with any architecture, there’s a large amount of infrastructure that’s needed to be deployed so that a data product can be built, deployed, executed, monitored and accessed. Infrastructure management is specialised skill-set that could be too prohibitive to have in every domain, particularly if there are hundreds of domains in an organisation. Therefore, the platform becomes a shared asset to enable domain autonomy.
There are some capabilities that the data platform should provide to domains:

**OPEN CODING TRACE:**

infrastructure as code

**AXIAL CODING TRACE:**
added:
``` python

    infrastructure_as_code
    
```

> Scalable, Distributed Storage

> Scalable, Distributed Query Engine

> Data Pipeline Implementation and Orchestration

> Identity and Control Management

> Data Product Code Deployment

> Data Product Discovery, Catalogue Registration, Lineage, etc.

> Data Product Monitoring / Alerting / Logs

**OPEN CODING TRACE:**

distributed_file_storage

query_engine

scalability

data_transformation_orchestration

access_control_management

build_deploy_monitor_dp

data_catalog

alerting

data_lineage

**AXIAL CODING TRACE:**
added:
``` python

    distributed_file_storage
    
    scalability
    
    query_engine
    
    data_transformation_orchestration
    
    access_control_management
    
    build_deploy_monitor_dp
    
    data_catalog
    
    alerting
    
    data_lineage
    
```

> A typical workload on a shared self-service data platform infrastructure could look like the following digram: incoming data (batch or streaming) gets ingested and processed and stored into the data structure that defines the data product, whether that is columnar or object. At the other end of the workload, an incoming request for data hits a web service, which then orchestrates a series of processes against the data product storage area, to then return that data to the customer. 1

![WorkloadSelfServe](https://images.squarespace-cdn.com/content/v1/5bce4071ab1a620db382773e/1628180630376-UUG2QM65LB97K6IY7CGM/data-product-self-serve-infra.png?format=1500w)

> This is highly abstracted to allow any technology to play the role of an ingestion service, processing engine, storage provider, web service, etc. While the platform needs to domain agnostic, it should also aim to be vendor agnostic and embrace Open Standards, Open Protocols and Open-Source Integrations. This allows the domain teams to be truly autonomous in choosing the tools to best meet their needs and skills - although, a word of caution here, as the selection of tooling should also be done in consultation with the data platform team, as they will be supporting the underlying infrastructure - including how the tooling interacts with the rest of the infrastructure.

**OPEN CODING TRACE:**

data_source_ingestion

autonomous

**AXIAL CODING TRACE:**
added:
``` python

    data_source_ingestion
    
    autonomous
    
```





