# p
4
## url
https://github.com/TomEijk/Data_as_a_Product_GLR/tree/master/python/self-serve-platform_codeablemodels/field_notes_open_coding_axial_coding
## tiny url
https://tinyurl.com/self-serve-platform-p4
## archive url
https://bit.ly/self-serve-platform-p4
## title
Interview Expert 4
## source code
no
## example
yes
## source type 
Interview
## author type
Practitioner
## references

**AXIAL CODING TRACE:**
``` python
p4 = CClass(source, "p4", values={
    "title": "Interview Expert 4",
    "url": "https://github.com/TomEijk/Data_as_a_Product_GLR/tree/master/python/self-serve-platform_codeablemodels/field_notes_open_coding_axial_coding",
    "archive url": "https://bit.ly/self-serve-platform-p4",
    "tiny url": "https://tinyurl.com/self-serve-platform-p4",
    "author type": "Practitioner",
    "type": "Interview",
    "example": True,
    "source code": False})
```

# coding

> Interviewer 

> I was researching the self-serve platform and I found quite a lot of articles about that. But I think this one is less controversial as the data product frameworks, because everyone seems to have its own opinion on how data products need to be derived and constructed, et cetera. Any self-serve platform that seems to be more coherent, but yeah, let's just find out. So here we have again, the inter-decision framework. So in total, I encountered five different decisions you can take into account while implementing a self-serve platform. The first one is more high level. So how to align a self-serve platform and its components with data mesh. And what I mean with this is we can choose to implement a single data lending zone at a single self-serve platform. And also this thing distinguish to self-serve platforms based on source aligned data products and consumer aligned data products so that we have actually two self-serve platforms, but there are multiple other ways to align your self-serve platform with your data mesh. For example, in a region that you'll have a self-serve platform per region and things like that. So it's more high level. On the second level, we encountered the three planes mentioned by Zhamak Dehghani in her book. So the infrastructure provisioning plane, the data product developer experience plane, and of course, a supervision plane, which is more related to governance. And the last decision is actually how to make your self-serve platform accessible for the generalist so that everyone is able to interact with it. And if we quickly go on to the first decision, are you able to see the framework or a little bit bigger? Okay, great. So this one is actually based on an article from Microsoft. And he was mentioning the possibility to implement your self-serve platform in a different way. So we can choose to just have one self-serve platform, which is supporting all activities in your data mesh, source system and consumer aligned data, self-serve platforms. We already mentioned these. A hub, generic and special data landing zone. And this actually means that a lot of the ingestion processes, a lot of the virtualization practices are the same for all the data products and they can be implemented in a central hub. And this central hub actually feeds all the different data domains, et cetera. So the ingestion part is centralized in a hub and all the other parts are decentralized in another data landing zone, in another self-serve platform. And some special cases are distinguished as well. So there can be a separate self-serve platform for that one. Then we have functional and regionally aligned data landing zones. I think that's the ones we can work with. And the last option is to choose for different data management zones so that we not only have different self-serve platforms, but also different governance platform. Well, not governance platforms, more like governance layers that each data domain, for example, has its own data governance plane and also its own self-serve platform. So, yeah, I'm really curious what you think about these distinctions, because not a lot of practitioners are mentioning these, but I found it triggered me. I really liked the broader way of thinking about each other. 

> Participant 4

> Well, let me, I guess the first question I would have, cause I think there might be one missing here, just before I, I just want to understand the context before I comment. So when I think of self-serve, the very first question you have to ask yourself is what component is actually self-serve? And in data mesh, data mesh is really just an ecosystem of data products. So if you believe that there's some kind of fabric, maybe some common platform to move stuff around or let them integrate, but ultimately it's all about an ecosystem of data products. So the real question is, what is self-serve in there? Well, I believe strongly it's the data product is self-serve. And if it's self-serve, then, and we want to apply other principles as Zhamak has highlighted, if I were very trivial, trivialize and simplify, you know, we want to move from centralized to just, you know, to distributed from federated. And so if that in fact is the case, I think strongly that the unit of self-serve is the data product. So now what is the implication of that? So I see here a bunch of examples of where I have data outside of a data product. Yeah. I think that fundamentally, not only to be a purist, but if you want to be a purist, it violates the data, you know, the whole idea of, you know, you don't want to be centralized. You want to be completely decentralized and federated. Yeah. The unit of federation and distribution is a data product. That means all data resides in a data product and nowhere else. Which means the data product is the unit of self-serve and all of the data, every artifact, every asset related to that data is inside the data product. That's the part that's not on here. So this is what I think classically, most people would say in analytics, which is, and I think is classically wrong, is we have these big data landing zones and big data warehouses, and we have to carve them up into little pieces. And that's borderline impractical. I've not seen anybody do it well, but you can do that. But you have to wrap your head around the fact that the data product is the unit of self-serve. And, you know, taking Zhamak principles into account, data is inside the data product. There aren't, you know, the pipelines are inside the data product. The consumption is controlled and owned by the data product owner. What that really means is there's no room for these landing zones. The landing zones are inside the data product. That's my philosophy. So now the question really is, what capabilities do you want to self-serve? Well, I think every data product, if you architect them well, they all look the same. They all use the same capabilities, but every one of them, to do self-serve, you want to have a discovery endpoint. You want to have an observe endpoint. You maybe want to have a usage endpoint, or logs or alerts or a lineage endpoint, or a query endpoint or an ingestion endpoint. But all of that is self-serve within the actual data product. Now, the thing that I just had a discussion with, you know, Norway's largest bank today about, how do I actually find data products in the data mesh? Well, self-serve, we call it, I call it a data product registry to distinguish it from, you know, your typical data catalog. So if I have, if I have the, again, the data product as the unit of self-serve, everything's inside a data pipelines inside the data product. Now the catalog, the data registry, data product registry becomes the window into the data mesh and it becomes the vehicle to actually allow self-serve. So what do I mean by that? Well, if I have hundreds of data products running around in my organization, how do I find them? Well, self-serve, I go to my catalog. I shouldn't have to phone somebody up. I shouldn't have to call a data engineering organization. If I want to consume it, you know, maybe I should look at the discovery endpoint, see what APIs are available or queries are available and use that. And if I want to actually analyze the data, and this is what we're doing with some of my clients, is I want to actually not just consume the data, but actually perform analytics on the data. What I, in most cases, most data scientists have to get data from three or more data products, a customer data product, an account data product, and the product data product, and bring them all together and create the next best offer. Well, what they end up doing is they find and discover each of those endpoints. They query them, they put the data into an analytics zone. We call that, in my terminology and my clients, a zero trust region. And we spin it up self-serve from the catalog, is the window portal again, into the data mesh. In five minutes or less, I get a zero trust region running in the cloud, the organization's cloud instance. I load the data that I asked for, could be gigabytes, could be a hundred gigabytes in some cases. I load it in there. And because it's a zero trust zone, spun up in five minutes or less, and it's a zero trust region, the data scientists can do all the stuff in the clear. Data's in the clear, which anybody who does data science knows that you can't have obfuscated data in AI machine learning training. It just doesn't work. It blows the model out the door. So when I think about the philosophy of self-serve, I think you have to think of what is the unit of self-serve? What is encapsulated in that unit? And how do I make that unit easy to find, consume, share, trust, and even facilitate analytics, again, across the mess, not just within one data product. So that covers a different dimension than what you have here. But I'm very much allergic to these so-called landing zones that people are, because I think they're counter, they propagate all the problems that we've had in the past and they propagate the same practices that we're trying to avoid with data mesh. Yeah. So I'll leave that with you. Let me suggest changes to this, but I have a very different philosophy in how you would facilitate self-serve. 

**OPEN CODING TRACE:**

The unit of federation and distribution is a data product. That means all data resides in a data product and nowhere else. Which means the data product is the unit of self-serve and all of the data, every artifact, every asset related to that data is inside the data product. That's the part that's not on here. So this is what I think classically, most people would say in analytics, which is, and I think is classically wrong, is we have these big data landing zones and big data warehouses, and we have to carve them up into little pieces. And that's borderline impractical. I've not seen anybody do it well, but you can do that. But you have to wrap your head around the fact that the data product is the unit of self-serve. And, you know, taking Zhamak principles into account, data is inside the data product. There aren't, you know, the pipelines are inside the data product. The consumption is controlled and owned by the data product owner. What that really means is there's no room for these landing zones. The landing zones are inside the data product. That's my philosophy.

So if I have, if I have the, again, the data product as the unit of self-serve, everything's inside a data pipelines inside the data product. Now the catalog, the data registry, data product registry becomes the window into the data mesh and it becomes the vehicle to actually allow self-serve. So what do I mean by that? Well, if I have hundreds of data products running around in my organization, how do I find them? Well, self-serve, I go to my catalog. I shouldn't have to phone somebody up. I shouldn't have to call a data engineering organization.

If I want to consume it, you know, maybe I should look at the discovery endpoint, see what APIs are available or queries are available and use that 

Well, what they end up doing is they find and discover each of those endpoints. They query them, they put the data into an analytics zone. We call that, in my terminology and my clients, a zero trust region. And we spin it up self-serve from the catalog, is the window portal again, into the data mesh. In five minutes or less, I get a zero trust region running in the cloud

**AXIAL CODING TRACE:**
added:
``` python

    data_product_as_self_serve
    
    data_catalog
    
    api_catalog
    
    VMs
       
``` 

> Interviewer

> Yeah. Okay. So I guess more from the data product point of view and that we need just one self-serve platform. 

> Participant 4

> No, not one self-serve platform. In fact, the data mesh principles say that the data product owner is the king of the castle. They reign supreme, if you will. They get to make the decision. In other words, there may not be an enterprise-wide platform. In fact, what I've seen is the dynamic is very different in real, when you're doing this for real life, is the data product is typically aligned to a business, business owner, typically. The business prizes speed and agility over anything. They could not care one iota about standardization, enterprise architecture standards, let alone if I had to prioritize speed and agility over cost. Cost is a very, very distant third or fourth priority. What that really means is your data product owner gets to make the calls. Here's the dynamic that you want to get to. Enterprise architecture, the CDO, the central organization does not say you must use my self-serve platform. Okay, that fails every single time. Rather, those groups now need to say my API platform, like Kafka streaming platform, my change data capture platform is better than anybody could build in a reasonable amount of time and actually services your needs. Then you may actually have a chance of having a standard platform. But at the end of the day, there is no such, in data mesh, there is no such thing as a standard platform, unless the data product owners coalesce around one. So it's a very different paradigm to what most organizations find. And again, the businesses gravitate to it because we enable speed and agility while forcing the quote standards police to actually make their case and explain that. By the way, it doesn't say there isn't a standard platform. It just means the standard platform has to be good enough and better than any other options so that the data product owner is actually adopted. Yeah, I'm kind of giving some radical thinking, but this is where real organizations are going and where you get the best transaction. Like my biggest customer in data products is usually the SVP of a business unit who has a very big problem that they have to solve. And we solve it because we, with data mesh data products, because we emphasize speed and agility over standardization's costs. Again, it doesn't say we don't value standardization, but in its context, the priority is speed and agility. Anyway, enough pontificating. I'll let you stew with that and see where that goes. 

> Interviewer

> No, it's a very interesting idea though. Yeah, to think more about the data products and what they need instead of, okay, how should I align my self-serve platform with the data products? Because then you enforce some way of thinking on the data product developer, perhaps already. Yeah, always dangerous to do. But yeah, we can perhaps go to the three main planes according to Zhamak Deghani. I will zoom in a little bit. This one is the infrastructure provisioning plane. And from my perspective, this is actually everything that's not considered, well, not entirely not considered, but it's the environment around the data product, which can be, for example, the workflow automation engine, like an Apache Airflow or an stream processing service or batch stream processing service. Networking, so our data products need to be able to communicate with each other. So we need networking, a networking aspect. So our data products need to be deployed in an environment, which can be a VM, for example. We need data source ingestion. So how do we connect the operational databases with the data products? You need an orchestrator to make sure that the data products, their analytical data is transported to the right, is transferred to the right consuming data product, for example, or to the right customer and customer, and usually. Then we need a computing engine. So a managed compute infrastructure. So for example, we can use, yeah, we can separate compute from compute, like they are doing in Snowflake, for example. Separating storage and compute. Of course, we need a query engine to make sure that the queries are able to be executed. An elastic performance engine, so that we do some auto scaling. A polyglot storage option, so that we can provide multiple types of storages. So not only CSV, but for example, for example, also query queries or Excel files, or just other data types. And the last one is to provide some machine learning capabilities, such as a feature store and other aspects. This is just one example. This is on a really high level what an infrastructure visioning plane should incorporate. So I'm curious if. 

> Participant 4

> No, this looks really good, actually. I like this. I call this the data mesh fabric. Yeah. Other people have different names for it, infrastructure platform, but this covers the basis pretty well. A lot of the capability obviously is available with some of the cloud vendors. But I think you cover all the bases. I can't think of anything that you've omitted. So this looks really good. 

> Interviewer

> Or do you think some of these aspects are redundant? Not necessarily. So almost all these capabilities are available with each cloud vendor. So if you're on AWS, you can use EC2 instances. They have data pipelines. They have obviously a CI CD capability. But cloud vendors have many, many more capabilities. Really doesn't matter how many they have. These are the ones that you need. So I think you have a relatively full list. I mean, there's a lot of things you need to know about how to use these things well. But I mean, you have the laundry list of capabilities that you actually need. I can't think of anything that I would add. 

> Interviewer

> Nice, because that's exactly the objective. The objective is to provide some guidance in what's possible. So what practices and what patterns are possible to implement when entering a solution. So yeah, that's a good goal. 

> Participant 4

> Yep, this one looks good. 

> Interviewer

> Great, then we can quickly go on to the next one. Quicker than I thought actually. All right. So this is more from the data product developer point of view. So what functionalities does a data, well, does a self-serve platform have to provide to the data product developer? For example, we need a functionality to build, deploy and monitor our data product. This can be just one functionality. Well, that includes all these three aspects. We can have a versioning data product functionality. Of course, our data product needs to be secured as well in terms of access controls, privacy controls, data retention policies, things like that. It needs to be accessible. So we were just talking about the discovery board. The discovery board is a functionality that can be provided to the data product developer. We need to be able to document our data product. So to register this one in the data product registry, it should be possible to create your data products. So everyone needs to understand how to, yeah, how to make your data product, et cetera. It should be, yeah, really easy. Yeah, of course testing is really important, especially in CICD practices. So we need to be sure that, yeah, the data product is actually functioning well. And the last one is about the CRUD operations. So the create, update, et cetera, create update. I don't know all four at the moment, but you know what I mean. Right? Yeah, so this is actually all I found from the data developer point of view, but I think you probably know some more practices and patterns because you're really knowledgeable about your data products. 

> Participant 4

> Yeah, so I think there's, first off, the keyword I think you have correct, which is this is all about the developer experience. Yeah. And in fact, the goal that we should have as data mesh practitioners is to ensure that there's an outstanding developer experience. And I say that very, very purposely. The current developer experience is horrible. You have nine different tools you got to work with. All of them are different. They all look different. They all require different security, et cetera, et cetera. It's a horrible developer experience. So the question really is, how do you create an outstanding developer experience? Yeah. Well, first off, number one is, again, you're not gonna have one data product. You're not gonna have 10. If you're successful, you'll have hundreds. So really anytime you have many instances of something, in order to develop to them, let alone operate and observe them, they ought to all look the same and act the same. So the very first thing in having a good developer experience is, these practices that you call here, I'll add to it in a bit, but these all need to be standardized and available and discoverable in every single data product. Now, what I typically do, I've written about, I think it's the anatomy of a data product article that I wrote a while back, but ultimately every one of the capabilities in the data product is exposed as an API. Okay. And if you're gonna expose it as an API, a developer can consume it. They can write command line interfaces, they can write applications that talk to it, and they can actually allow data products to talk to each other. Yeah. So the key to success is, every data product looks exactly the same. It has a common set of APIs. Question, what are those APIs? And you have a bunch of them here. So, but I would add, first off, and I'll talk about how you create data products in a second here, but once one is up and running, you need to have a standard slash discover endpoint, which I think you talked about. So, once you have your catalog, okay. So when you, I'm gonna take a step back. Actually, let's start with how you create a data product, cause I think that provides a little bit more context. It has to be drop dead simple for, with a UX or a very simple API, or a very simple command line interface for a developer to spin up a data product. Okay. That answer a few questions, provide some basic information, and lo and behold, they can point it to a Postgres database or S3 storage bucket or whatever, and your data product, albeit perhaps in a test capacity, experimental capacity is up and running in five minutes, five minutes after you've provided the simple configuration through a question and answer type thing. Okay. Once you've published the data product, it gets registered inside the data registry, which I think you've talked about here. That's a critical component. So once it's in the data registry, I should be able to find it, okay, as a developer. That's core, everybody seems to forget that. You have to have a catalog or some easy way to find these things. Now hesitate using searches, lots of different ways to find it, but you have to have a way to find the data products that are out there. So that's why I call it a registry, which has some search capability baked into it. Now that I've published it and I can find it, every single data product should look exactly the same. In other words, when I publish the data product, I automatically create a discovery, observability, operability capability, typically through a agent microservices that sits on top of the actual data and it enforces the boundary and makes it discoverable. So the agent and the set of APIs, a standard set of APIs that it exposes is part and parcel of what the data product is about. And because it's API oriented, it's very, very simple for a developer to understand. So the question then is, if you understand that and it's a standard thing, then the question is what are those things that every data product should actually expose? Well, you have a bunch of them here, a very good list, but you should be able to discover it. Yeah. Okay. You should be able to consume it. You should be able to share it. You should be able to indicate trust in it with a like or a GitHub star or something like that. And you should be able to observe it and you should be able to understand the operability characteristics, the logs, the alerts. Every data product should have that minimal and the security requirements to actually access the capability. The last but not least, which you kind of highlighted here, you call it testing. I call it quality. Yeah. You need to be able to have an API that gives you the quality attributes of the actual data product. For governance in particular or even regulatory compliance, it's much more than testing. Really, once it's in production, the data scientist wants to know what is the quality attributes of the data product? Are some of the trivial things like are some of the fields null and are more of them null than should be? And there's a policy, a data quality policy that says maybe 10\% can be null. Maybe the regulatory agency says 0\% can be null. There has to be that quality attribute also within the, as again, a published API for every single data product. Now, once you have that, you can build numerous sets of tools to make the developer job even easier. Like I said, command line interfaces, UXs, which is where my company builds these catalogs, data product, we call them data product registries. But once you have the standard interface, it's trivial to build a user interface on top of it to search and find data products. So those are all the capabilities. You have many of them, but not all of the ones that I would suggest you actually have to have, but you have a pretty good list. 

**OPEN CODING TRACE:**

crud_operations

It has to be drop dead simple for, with a UX or a very simple API, or a very simple command line interface for a developer to spin up a data product. Okay. That answer a few questions, provide some basic information, and lo and behold, they can point it to a Postgres database or S3 storage bucket or whatever, and your data product, albeit perhaps in a test capacity, experimental capacity is up and running in five minutes, five minutes after you've provided the simple configuration through a question and answer type thing.

Okay. Once you've published the data product, it gets registered inside the data registry, which I think you've talked about here. That's a critical component. So once it's in the data registry, I should be able to find it, okay, as a developer. That's core, everybody seems to forget that. You have to have a catalog or some easy way to find these things. Now hesitate using searches, lots of different ways to find it, but you have to have a way to find the data products that are out there. So that's why I call it a registry, which has some search capability baked into it.

In other words, when I publish the data product, I automatically create a discovery, observability, operability capability, typically through a agent microservices that sits on top of the actual data and it enforces the boundary and makes it discoverable. So the agent and the set of APIs, a standard set of APIs that it exposes is part and parcel of what the data product is about. And because it's API oriented, it's very, very simple for a developer to understand.

You should be able to indicate trust in it with a like or a Github star or something like that. 

And you should be able to understand the operability characteristics, the logs, the alerts

Every data product should have that minimal and the security requirements to actually access the capability. The last but not least, which you kind of highlighted here, you call it testing. I call it quality. Yeah. You need to be able to have an API that gives you the quality attributes of the actual data product.

For governance in particular or even regulatory compliance, it's much more than testing. Really, once it's in production, the data scientist wants to know what is the quality attributes of the data product?

version_dp

**AXIAL CODING TRACE:**
added:
``` python

    declaratively_create_dp
    
    data_catalog
    
    api_catalog
    
    feedback_loop
    
    secure_dp
    
    alerting
    
    log_management
    
    testing_dp
    
    crud_operations
    
    version_dp
    
``` 

> Interviewer

> Yeah, there can be more indeed. And you were just talking about a trust API and that we can use GitHub Star, but is this the same as using the feedback loops in Zhamak’s books? She was talking about feedback loops. 

> Participant 4

> Yeah, that's an example of the feedback loop. So what you wanna do is you wanna provide a mechanism to allow consumers to say to the data product owner, it's doing its job, it's good, it's bad, or to issue requests. What we find is we build most of these things on top of GitHub. So every data product has a GitHub repo, all the configuration of the data products in the GitHub repo. The beautiful thing about that is now that I'm using the GitHub repo, it's very simple to star, to indicate you think it's doing a great job. It's very easy to say, I wanna have a new feature by issuing a pull request. So there's a lot of capabilities that you get if you think through the design of how you enable a data, an outstanding developer experience. And this is just some of the lessons learned that I have. So the trust ranking is a combination of two things, the likes, the social things, the likes, the activity, the number of users. So it's that direct feedback coupled to usage. Maybe people like it, but if it's never used, those likes may be old or they're not relevant. But if you get a lot of likes and usage in GitHub, there's activity, which means the number of commits. In NPM, if you look at the node modules, node package manager shows you how many downloads per week. There's activity as an indicator of trust, as well as direct feedback. So we actually expose a trust metric, which is a combination of those things for every single data product. So again, the consumer, it not only enables a developer experience, a good developer, but an excellent consumer experience also. 

**OPEN CODING TRACE:**

Yeah, that's an example of the feedback loop. So what you wanna do is you wanna provide a mechanism to allow consumers to say to the data product owner, it's doing its job, it's good, it's bad, or to issue requests. What we find is we build most of these things on top of GitHub. So every data product has a GitHub repo, all the configuration of the data products in the GitHub repo. The beautiful thing about that is now that I'm using the GitHub repo, it's very simple to star, to indicate you think it's doing a great job. It's very easy to say, I wanna have a new feature by issuing a pull request. So there's a lot of capabilities that you get if you think through the design of how you enable a data, an outstanding developer experience. And this is just some of the lessons learned that I have. So the trust ranking is a combination of two things, the likes, the social things, the likes, the activity, the number of users. So it's that direct feedback coupled to usage. Maybe people like it, but if it's never used, those likes may be old or they're not relevant. But if you get a lot of likes and usage in GitHub, there's activity, which means the number of commits. In NPM, if you look at the node modules, node package manager shows you how many downloads per week. There's activity as an indicator of trust, as well as direct feedback. So we actually expose a trust metric, which is a combination of those things for every single data product. So again, the consumer, it not only enables a developer experience, a good developer, but an excellent consumer experience also. 

**AXIAL CODING TRACE:**
added:
``` python

    feedback_loop
       
``` 


> Interviewer

> Oh, wow. Yeah. I wasn't thinking about that too roughly, you know? So yeah, that's a good one.

> Participant 4

> There you go. You can add a little bit to your thesis 

> Interviewer

> Yeah. I think a lot of the aspects you just mentioned also overlap with the mesh plane over here. And then we actually have all three of them. So you were talking about the data quality port or the data quality aspect. Yeah. That's probably the practice I encountered for the governance plane. But yeah. Sometimes it's hard to distinguish between the data product developer experience plane and the governance plane and the infrastructure provisioning plane, because a lot of those features really overlap with each other. 

> Participant 4

> Yeah. They can. You know, maybe data quality, because I hadn't seen how you delineate it, data quality probably does, should stay here as opposed to the developer thing. But walk me through what you have here. Like, I think like, for example, log management is great for the data governance side, but it's also great for developers. So when they have to diagnose a problem. So there is some overlap, but you can put it in either one, but let's go with what you got. 

**OPEN CODING TRACE:**

log management is great for the data governance side, but it's also great for developers.

**AXIAL CODING TRACE:**
added:
``` python

    log_management
       
``` 

> Interviewer

> Yeah. All right. Let's quickly go through it. Yeah. Of course, alerting, a very important practice we need to implement in our governance plane. This needs to be provisioned by the self-serve platform. The schema registry to enforce some, yeah, schema policies, some schema standards, the API catalogs, the API catalog, we have briefly talked about this one already, but yeah, it should be visible which kind of APIs are out there. We need to manage emergent graphs of data products. So what I mean with this is that we know which data products are connected with each other. So for example, that we have a sales data product and a recommendation data product, and we know that there is a connection for the governance plane. So there needs to be a functionality that enables this. We need to be able to secure, yeah, well, to manage the security policies of data products. So for example, policy automation is a very, very good example of that one. Yeah, the schema product registry, you call it. So the central data product catalog actually, which has the, yeah, which has a discoverability function, exploreability. A knowledge graph. Many practitioners actually forget about the knowledge graph which is more some kind of semantic layer. This was mentioned in the book of Piethein Strengholt, I really liked his book on data management at scale. And he actually mentions the fact that, well, when we consider a metadata lake, he actually claims that we should implement a knowledge graph as well as a data product registry, because a data product registry is more static and it doesn't really change over time. And the knowledge graph actually keeps track of all the semantics within your data mesh, which is really important because if we have a customer in our sales product, as well as in our promotion data product, this one needs to be the same. There needs to be some central authority that keeps track of the semantics. And now I was thinking about metadata management. I actually think that metadata management can have two child nodes in this case, so that we move the data product registry and the knowledge graph actually under metadata and nodes and the last two are the log management. So what operations are being executed in our data mesh and there needs to be some form of master data management. Which- 

> Participant 4

> I could debate the master data management thing, but let me give you the initial perspective. I think like when we do the data product registry, we use security specifically roles to allow certain menu options to be surfaced or not. So for example, if you're, if you are a, so everybody gets the marketplace, so they can do the search and find to figure out what to do to the process. But if you're a governance person, you're going to see logs, maybe alerts in that data product registry as a menu option. And once you find your data product, you can look at the logs and the alerts. If you're a developer, maybe you're looking for the slash discover endpoint or the artifact list that's within a data product. With, if you're a governance professional, you know, the knowledge graph, absolutely. But here's the thing. We, every one of these things is, we talked about the data and developer experience. There's also the other ones, which you haven't mentioned, which I think fit into here is the operability experience, which is operability and observability. So there's several different constituent audiences, right? There's the developer, the governance professional and the operability folks, the folks who have to run it in production. And when it goes bump in the night and something breaks, they have to call somebody, but they have to be alerted. So that's a very big constituent audience. Okay. But again, then you have the governance professional and there's overlap between them, but the operability professional, you know, they would have permissions to explicitly look at alerts and logs in this case. Okay. And those menu options would be enabled in the data registry. So another where I'm going with it is the supervisor, supervision plane, it's implemented through it, through the data registry. It just has, you know, based on your particular role, you know, it makes certain capabilities available or not, but there's like, we have only one registry and it's just security that governs what menu options and capabilities you have. Now, here's the interesting thing is semantics, knowledge graph descriptions, all those things are literally one in the same. And what we typically do is we provide tools to the data product owner, mostly natural language processing tools in the old days that would allow you to process all this data and come up with keywords, which become your tags. There's some tools that you could actually use to summarize. We're now, some of my clients, we're now looking at using chat GPT to summarize all these things. And we just say, hey, ingest these 48 different documents, understand, you know, and summarize them. So that's the description for the data product and give me the keywords, those are the tags. So use some pretty sophisticated, we can use now some pretty sophisticated tools to describe and create tags for the data products. The interesting thing is chat, you know, creating a knowledge graph up until today has been a very manual exercise and it's very error prone. Most people don't know this, but if you work with chat GPT for a little while, it actually creates, you can ask it to create knowledge graphs of all those documents that we summarized. And it can actually create a knowledge graph using the schema is, they call it JSON-LD. I can't remember what the LD is, but ultimately it creates a knowledge graph of a particular format from documents. So we are contemplating using chat GPT to create the whole data product description from any documentation that they give us, create the keywords for the tagging and create the knowledge graph. And then, you know, use that to create the semantic taxonomy. So all this is part of the developer experience that we actually have expected and it's hugely powerful. So yes, the supervision, supervisory plane exposes some of these things, yes, but it's also part of the developer experience that we provide. Yes. Yeah, that's the fine line I was talking about. Well, the thin line between the supervision plan and the data product development apps. Yeah, so there's some overlap. I mean, there's nothing wrong with where you put them, you know, but it's all about the story you wanna tell, I suppose. 

**OPEN CODING TRACE:**

Now, here's the interesting thing is semantics, knowledge graph descriptions, all those things are literally one in the same. And what we typically do is we provide tools to the data product owner, mostly natural language processing tools in the old days that would allow you to process all this data and come up with keywords, which become your tags. There's some tools that you could actually use to summarize. We're now, some of my clients, we're now looking at using chat GPT to summarize all these things. And we just say, hey, ingest these 48 different documents, understand, you know, and summarize them. So that's the description for the data product and give me the keywords, those are the tags. 

schema_registry

**AXIAL CODING TRACE:**
added:
``` python
    schema_registry
    
    knowledge_graph
       
``` 

> Interviewer

> Yeah, and you were talking about the operability experience. So the way I interpret it now is that I can perhaps create a note for that one and change this one in a child note of the operability experience practice. And you were mentioning and alerting, yeah. So alerting the log management, then we can narrow down the framework a little bit. Yep, that would work. That would be a good way to visualize it. Yeah, and you were briefly talking about master data management, that you have a strong opinion on that one. 

> Participant 4

> Yeah, so here's the challenge you have. So again, every data product is, you know, the data product owner is the king of the castle. There's a clear boundary around it. All the data that's required for that data product is inside the data product. So that may necessitate duplication, okay? And it mostly because, especially in the analytic world, you're taking, you're literally duplicating copy from operational systems and you're putting it into a different repository. So literally by nature, the data products will foster, maybe not foster, but as a byproduct of using the data mesh approach and the decentralized federated and local decision-making approach that we're talking about, you will get duplication and redundancy. So the question really is, is that a bad thing? And I would say it's not as bad as everybody says, because like I said, every data, every business owner prioritizes agility and speed over standardization, data duplication and all the bad things that enterprise architecture says, because here's the deal. If I can get into a new market, that's worth a hundred times, a thousand times revenue and profit, then I could say by squeezing 10\% out of the cost budget of IT. So there's no comparison. So this really boils down to, you know, where do you want to actually focus your attention? Now, coming back to master data management, real question is, what is, what does master data management look like? In my, in the old world, everybody says, I need to have this master customer record and I'm going to gather it from all these different operational systems. I'm going to move those kinds of things and I'm going to reconcile it and I'm going to put it into this one master repository. I think that's fundamentally flawed for a bunch of reasons. First off, it's a central repository. So it goes against our data mesh practices. So the question is, can I do this in a way without creating yet another central repository? And I think the answer is yes. Okay. It's not a trivial practice. It becomes master data management for customer, for account, for product, whichever one becomes another data product. Okay. And what it is, the master data management is literally a view, not a materialization, there's no data. It's a view, okay, and references to all the data products. So the master data would say, I have a customer record. I have > Interviewer's customer information in Salesforce. I can go get that. I'm not going to copy it. I'm going to go get it when I need it. I have > Interviewer's list of products and that's in the product management system. And I have all of > Interviewer's bank account information and his loans in three different systems over here. The master data management says, I have a record for > Interviewer, okay. And here's how I can actually go link to the identifying information for > Interviewer. So it is a book of, it is a literally, it's a DNS, the main naming service, just as the DNS allows us to find stuff on, link IP addresses all over the internet, hosts all over the internet. Master data management, all it is is a index into all the other data. Data does not move. The majority of data stays in the data products where they should be. And the master data management is a way to link to the appropriate attributes that are required to be mastered. So now the interesting thing about that is if all you chain is references, and the data resides in the data products, it's never out of date. Whereas in master data, one of the biggest problems in master data today is I'm copying stuff from all these source systems and it's constantly out of date. If you think about how you wanna do this, data mesh gives you a brand new opportunity to tackle data, master data management properly. And I know because I've actually implemented before data mesh long, long time ago, implemented master data management at a big bank. And it was, because of the reasons I talked about it, it was very, very painful and difficult to do. There's better ways to do it is what I'm saying. 

**OPEN CODING TRACE:**

What does master data management look like? In my, in the old world, everybody says, I need to have this master customer record and I'm going to gather it from all these different operational systems. I'm going to move those kinds of things and I'm going to reconcile it and I'm going to put it into this one master repository. I think that's fundamentally flawed for a bunch of reasons. First off, it's a central repository. So it goes against our data mesh practices. So the question is, can I do this in a way without creating yet another central repository? And I think the answer is yes. Okay. It's not a trivial practice. It becomes master data management for customer, for account, for product, whichever one becomes another data product. Okay. And what it is, the master data management is literally a view, not a materialization, there's no data. It's a view, okay, and references to all the data products. So the master data would say, I have a customer record. I have > Interviewer's customer information in Salesforce. I can go get that. I'm not going to copy it. I'm going to go get it when I need it. I have > Interviewer's list of products and that's in the product management system. And I have all of > Interviewer's bank account information and his loans in three different systems over here. The master data management says, I have a record for > Interviewer, okay. And here's how I can actually go link to the identifying information for > Interviewer. So it is a book of, it is a literally, it's a DNS, the main naming service, just as the DNS allows us to find stuff on, link IP addresses all over the internet, hosts all over the internet. Master data management, all it is is a index into all the other data. Data does not move. The majority of data stays in the data products where they should be. And the master data management is a way to link to the appropriate attributes that are required to be mastered. So now the interesting thing about that is if all you chain is references, and the data resides in the data products, it's never out of date.

**AXIAL CODING TRACE:**
added:
``` python

    mdm
       
``` 

> Interviewer

> Yeah. Yeah, because with master data management, you often fall back in the centralized approach where we have a central storage and stuff like that. 

> Participant 4

> Yeah, and I'm saying forget the central storage, just have simple references to all the data wherever it may exist. 

> Interviewer

> Yeah. Yeah, we wanna- Like I said, data products make it easy. Yeah, we wanna minimize data movement as much as possible because that would result in stale and especially costly data. 

> Participant 4

> Yep, absolutely. 

> Interviewer

> All right, thank you very much. This is about the supervision plan and we can go on to the last one for just some time, I guess. This is about the self-serve UI. And you were briefly talking about the self-serve UI just a couple of minutes ago where your company is actually developing a UI accessible for the generalist. Yeah. And I briefly go on to the functions and self-serve UI should provide according to several practitioners is of course the cataloging function. So it needs to be connected to the data catalog, et cetera. And there needs to be an easy function to register your data products and stuff like that, a visualization function. So I wanna know what's in my data product. I want to make some easy, small visualizations, some preliminary visualizations, for example. A data transformation function. So for example, use no code transformation where we can just pick certain icons and each icon has data cleaning operation, for example, with a building pipeline function. Yeah, we were briefly talking about that. A data governance function. So of course there are local policies and global policies and the data product developer, he or she needs to be able to adjust the local policies and things like that. And data security, then needs to be a collaboration function, of course. So the GitHub star we were talking about, this is an option and is of course not mandatory, but if we think about software as a product, then software as a product, software as a service, then we can use the application bout function. Well, we can at least implement it in our UI. A data integration function so that we can select which operational databases we want to use for our data product. And the last one, query recommendation function. It helps generalist find the right queries for that data products. So yeah, this is about all the functionalities of itself, but I think you know a couple more. 

> Participant 4

> Yeah, like a lot of the discussion that we've had with all these various different planes, experience planes, supervisory planes. Yeah. In every one of them, there's a set of capability. And then there's to allow the end user to experience it, there's a typically user interface, sometimes an API, sometimes a command line set of capability. But ultimately every plane typically has a user experience. So who's that user? Like I said, you could develop individual user interfaces for each type of users, and that's definitely viable and doable. We choose to do it a little differently. It really helps us develop common capability and a consistent look and feel. Where we put all of these functions that we talk about here into the UX and based off of your role, you have the ability to access different capabilities and not access others. So literally my comment on the UX is it is the UI, it is the self-serve vehicle for everything we've talked about. And it serves different users. There's the developer user, there's the supervisory user, which we segmented into governance and operability, if you will. Every one of those things, every capability you have can be and should be surfaced as a UX. So as a developer, there should be a publish capability. Okay, just simplistically. As a governance person, there should be a data lineage capability, which we didn't talk about, but you should include data lineage in there somewhere. Yeah, I thought it was connected. Oh, there it is. Yeah, it's right there. Visualization function. It could be under there, but just as an example, how you show the lineage could be visual, all those kinds of things. But ultimately, all of those capabilities that you have can be segregated by a constituent audience or user. But everything's in the, all the capabilities surfaced in the registry. And it's the security and the role that governs what capabilities you have access to. That's how we design these things. That seems to be the best balance between, being able to do it relatively well and being able to have it evolve in a speedy and efficient way and drive some semblance of consistency in user experience. Yeah. So I wouldn't, like, I think you have all these things, but the UX is the fundamental vehicle for self-serve across all of your constituent audiences. Yeah, sure thing. Yeah, perhaps I should take this one a bit broader. Expectantly. Or perhaps I would, my advice is, all the stuff we've talked about before are capabilities, I think. Yeah, yeah. And the UX, this one here, fundamentally it says all those capabilities should be surfaced as a UX element. So the question I think in this one is how, well, what are the UX capabilities in general? What are the end points that are required for those? And then what are the capabilities that are rolled under there, all organized by your constituent audience? The three that we've had, developers, supervisor, governance and vulnerability folks. That would be my recommendation in terms of how you look at the UI. Cause that's how my customers look at it by audience and by capability, user interface component, API, that accesses the broad capability underneath the covers. 

**OPEN CODING TRACE:**

Where we put all of these functions that we talk about here into the UX and based off of your role, you have the ability to access different capabilities and not access others. So literally my comment on the UX is it is the UI, it is the self-serve vehicle for everything we've talked about. And it serves different users.

Visualization function. It could be under there, but just as an example, how you show the lineage could be visual, all those kinds of things. But ultimately, all of those capabilities that you have can be segregated by a constituent audience or user. But everything's in the, all the capabilities surfaced in the registry. And it's the security and the role that governs what capabilities you have access to. That's how we design these things. That seems to be the best balance between, being able to do it relatively well and being able to have it evolve in a speedy and efficient way and drive some semblance of consistency in user experience.

application_build_function

**AXIAL CODING TRACE:**
added:
``` python

    role_bases_access_control
    
    visualization_function
    
    application_build_function
       
``` 

> Interviewer

> And that's actually how I interpreted it as well in my inter-deceitment framework. So all comes back to the previous three planes over here. So after that, we have the self-serve UI where all these three planes are considered again. 

> Participant 4

> Yeah, but the thing is, I think there's elements in each of them that you haven't put into the final one. And I think that ability to organize, like the infrastructure provisioning is mostly a developer experience, I think. The developer experience is much more than just publishing, but all the other things we talked about. The governance plane is very different from operability, although at a very high level, I suppose there's some overlap, but I'll give you an example why there's usually some significant differences. When something goes bump in the night and it breaks, the operability folks need to look at the logs, which may be very, very sensitive information in the logs. The governance folks should not be able to see those logs. And if they see any of the logs, all that sensitive data has to be filtered because it's not part of their role and responsibility to actually see that. So we see the operability constituent audience very different from the governance, although there could be overlap in any of these, very different from the developer. So, and that may be some hints in terms of how you want to- 

> Interviewer

> Yeah, sounds about right. Yeah, and then actually I have two last questions and that's more on usability and interpretability. So in terms of how a practitioner can interpret all these decisions over here, do you think it's easy to understand when they are using this framework for their implementation practices? So the way I constructed these decisions separately and the way I formulated them, or do you think when you have perhaps seen my previous framework as well? 

> Participant 4

> Yeah, almost so the nature of what you're doing, you need to show all the paths. There's nothing wrong with that. When you're designing a user interface, you want to have as many intelligent defaults as you possibly can, because the goal that you have is to allow the user to fulfill their objective in as timely and fast a fashion as possible. So clearly those intelligent defaults should be changeable if necessary, but the way we design the user interfaces is, like we say everything should be done in five minutes. But if you provided like all the 48 different configuration parameters, it would take them a half an hour. They'd probably have to ask eight different people, what is a good answer? So for all those 48, 43 of them are defaults that can be overridden if you want, but answer five questions and you're done. That's the philosophy we have for everything in the UX. Five minutes is the goal and it shouldn't take longer than five minutes, which means you have to think about intelligent defaults. So the decisions are correct, I think, but you want to make intelligent defaults for everything. So for example, like we made some intelligent defaults. If you're a developer, you only get to see these three menu options. If you're a governance person, you get these four. And as a operability person, you only get these two. If I were to show them all, I'd make life very confusing. So that's just a very trivial example of how we simplify the user experience, knowing who the user is. 

**OPEN CODING TRACE:**

When you're designing a user interface, you want to have as many intelligent defaults as you possibly can, because the goal that you have is to allow the user to fulfill their objective in as timely and fast a fashion as possible. So clearly those intelligent defaults should be changeable if necessary, but the way we design the user interfaces is, like we say everything should be done in five minutes. But if you provided like all the 48 different configuration parameters, it would take them a half an hour. They'd probably have to ask eight different people, what is a good answer? So for all those 48, 43 of them are defaults that can be overridden if you want, but answer five questions and you're done. That's the philosophy we have for everything in the UX. Five minutes is the goal and it shouldn't take longer than five minutes, which means you have to think about intelligent defaults. So the decisions are correct, I think, but you want to make intelligent defaults for everything. So for example, like we made some intelligent defaults. If you're a developer, you only get to see these three menu options. If you're a governance person, you get these four. And as a operability person, you only get these two. If I were to show them all, I'd make life very confusing. So that's just a very trivial example of how we simplify the user experience, knowing who the user is. 

> Interviewer,

> Yeah. And do you think a practitioner can use this in practice, this framework? So for example, when we have the workflow automation engine in the infrastructure provisioning plan, that's something they can really implement. And- 

> Participant 4

> Well, I think, well, here's how I would suggest. If I was a consultant earlier in my career for Accenture and Deloitte at different times, so competitors to Bain, here's the thing is consultant makes their living by making something that's hard seem easy. Okay. And you do that because you have the expertise that they don't. So if you were to provide this to a client, their head would spin. Yeah. Because it has too many choices. So what you need to do is figure out, what is the sequence of decisions that you want to make, break it down by constituent audience and see if you can simplify and say, yeah, these are the three things you need to provide, but there's 48 other things that you can customize if you want. So literally, if you wanted to say, what is the decision framework? Put yourself in the shoes of the constituent audience and try and make it as simple as possible and add intelligent defaults for many of the questions that are in your decision tree. 

**OPEN CODING TRACE:**

Well, I think, well, here's how I would suggest. If I was a consultant earlier in my career for Accenture and Deloitte at different times, so competitors to Bain, here's the thing is consultant makes their living by making something that's hard seem easy. Okay. And you do that because you have the expertise that they don't. So if you were to provide this to a client, their head would spin. Yeah. Because it has too many choices. So what you need to do is figure out, what is the sequence of decisions that you want to make, break it down by constituent audience and see if you can simplify and say, yeah, these are the three things you need to provide, but there's 48 other things that you can customize if you want. So literally, if you wanted to say, what is the decision framework? Put yourself in the shoes of the constituent audience and try and make it as simple as possible and add intelligent defaults for many of the questions that are in your decision tree. 


> Interviewer

> Yeah. Oh, that's a good one. Yeah. Thank you so much. These are all the questions I actually had. So- Perfect. We're right on time, aren't we? Yeah. Thanks once again for this feedback, which I can definitely implement in my framework. 

> Participant 4

> Yeah. Well, like I said, the only thing I would ask is, I'd love to see the final product. 

> Interviewer

> Yeah, sure. The deadline- Obviously, I'm available for any follow-up questions if you have any. Yeah. Yeah. The deadline of the thesis is June the 1st and I only want to do the federal governance principle as well. So yeah, that would be the last framework. And then we're, yeah, then we hopefully have three really good frameworks that can help practitioners in their data mesh journey because the data, yeah, the domain ownership was more related to conceptual aspects and it's really closely related to the data product principle. So we actually merged both of them. 

> Participant 4

> Good. Okay. Well, I'll tell you what, > Interviewer, we're kind of at time. Yeah. Obviously feel free to ask any questions through data mesh Slack and if it makes sense, we could probably schedule if you want another session on your federated governance sometime. 

> Interviewer

> Yeah, that'd be lovely. And it makes sense. Okay. All right. Thanks > \textbf{Participant 4}. 

> Participant 4

> No problem. Thank you very much, > Interviewer, for allowing me the opportunity to help you out and I hope it helps and I look forward to our next discussion. 

> Interviewer

> Yes, have a good day. 

> Participant 4

> Take care. Bye now. Bye.
