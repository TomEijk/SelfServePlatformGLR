# p
5
## url
https://github.com/TomEijk/Data_as_a_Product_GLR/tree/master/python/self-serve-platform_codeablemodels/field_notes_open_coding_axial_coding
## tiny url
https://tinyurl.com/self-serve-platform-p5
## archive url
https://bit.ly/self-serve-platform-p5
## title
Interview Expert 5
## source code
no
## example
yes
## source type 
Interview
## author type
Practitioner
## references

**AXIAL CODING TRACE:**
``` python
p5 = CClass(source, "p5", values={
    "title": "Interview Expert 5",
    "url": "https://github.com/TomEijk/Data_as_a_Product_GLR/tree/master/python/self-serve-platform_codeablemodels/field_notes_open_coding_axial_coding",
    "archive url": "https://bit.ly/self-serve-platform-p5",
    "tiny url": "https://tinyurl.com/self-serve-platform-p5",
    "author type": "Practitioner",
    "type": "Interview",
    "example": True,
    "source code": False})
```

# coding

> Interviewer 

> Right, let me quickly share my screen. Oh, I should make everything bigger. The entire screen because I have a wide screen at the moment so sometimes it only takes a part of the screen. I'll put it on my big screen, give me a second. Or I can move this one a bit. Got it, got it, nice and big, yep. Okay. Okay, yeah, so during my self-serve platform research, I actually discovered well, five different decisions. The first one is how to align a self-serve platform and its components with data mesh. So it is more on a high level. Do we want to have only one single self-serve platform or do we wanna distinguish it based on source data products and consumer data products so that we have two self-serve platforms or do we wanna align it with certain regions so that we have a self-serve platform so specified on the US East one region in AWS and these two region, et cetera. So there are multiple possibilities over there but it's more on a high level. Then we can deep dive more into the three different planes, measure. I guess what would drive your decision making? 

> Participant 5

> What would be your decision points there? I mean, I guess location of the data. My view is that you have one platform that can be hosted wherever you want but the platform technology should be the same. I wouldn't build different platforms that I just host them differently. 

> Interviewer 

> Gotcha, yeah. Yeah, and the next three decisions are more related to Samak Deghani's book. So infrastructure, data product developer and more the governance layer. And yeah, of course the last thing you need to consider is the self-serve UI. So how do we make the self-serve platform as accessible as possible for the end user? Those five decisions I actually encountered during my research and I love to talk more about it together. So if we consider the first one, there are multiple possibilities to align your self-serve platform which are current data architecture. So we briefly talked about using just one single data landing zone, a single self-serve platform for using everything but we can also align it with the source data products and consumer data products so that there are two different self-serve platforms. Yeah, just one for both of them, just for each one, there is one. Yep. The hub generic and special data landing zones. This is the case when a lot of ingestion patterns are the same for the data product. So there can be a central self-serve platform which is the hub and this one is taken care of all the ingestion patterns and this ingests everything in the generic data landing zones. The generic data landing zone is more like the self-serve platform for all the data products and the hub is more something for the ingestion part of your data architecture and the special data landing zones are, these are relevant when there are some exceptions to the generic data landing zones. So if we only have two or three data products that are different, then we can have a separate self-serve platform for them. But most of the general practices will be on the general data landing zone. So this model is a bit more complicated but it can be useful if a lot of the virtualization, a lot of the ingestion patterns are the same for the data product. This is based on function and region. So I think that speaks for itself. The last one is when we have a really large enterprise and there are different governance patterns involved. So that we need to have different self-serve platforms for different governments teams, if you know what I mean. So for example, the sales domain requires different governance principles and sometimes it's more relevant to have a separate data governance team for only that one. But this is only the case in a really large company perhaps. So yeah, I'm curious if you recognize some of these patterns or that you just want to stick to the single self-serve platform which is a really good one as well, a good option. But yeah, I'm curious what you think about all the others. 

> Participant 5

> I'm trying to decide if we're single landing source or consumer aligned or hub generic special. Cause you got in our instance, Snowflake but you have multiple generic ingestion patterns. So, and then we're also thinking about Azure tooling as a secondary channel, which in my mind would be a secondary landing zone. Oh, okay. So you wind up with this ecosystem that has Snowflake with its specific capabilities, Azure with its Azure and Azure native for its specific capabilities. Ingestion patterns should be the same. Yeah. You might need some nuances to whether you're pushing to one or the other, but the structure is the same. So you've got generic patterns, two landing zones and then from a consumption standpoint, we're pushing it to multiple places, but it is, that's where it's, so it's not. 

**OPEN CODING TRACE:**

Snowflake but you have multiple generic ingestion patterns. So, and then we're also thinking about Azure tooling as a secondary channel, which in my mind would be a secondary landing zone. Oh, okay. So you wind up with this ecosystem that has Snowflake with its specific capabilities, Azure with its Azure and Azure native for its specific capabilities. Ingestion patterns should be the same.

**AXIAL CODING TRACE:**
added:
``` python

    hub_generic_special_data_landing_zones
       
``` 

> Interviewer

> Sounds a bit like the hub generic special data landing. 

> Participant 5

> I think so. Yeah, I think so. 

> Interviewer

> Cause it's Snowflake the, yeah, the place where you put your data products in, or do you have some form of containerization and you only use Snowflake as your warehouse or Azure storage and you have some connection between your containers and Snowflake and the containers is just the autonomous data quantum. 

> Participant 5

> No, they're also in Snowflake. So there are no other containers outside. So a product lives in a warehouse. Gotcha. Yep. 

> Interviewer

> So there are no containers actually. 

> Participant 5

> No, no container.

> Participant 5

> The data products are the Snowflake databases. 

> Interviewer

> Yep. Gotcha. Interesting. 

> Participant 5

> And then you deal with interoperability challenges across the two platforms. Yeah. Which we have to figure out, but I think, you know, the goal is that it lives in one of the two. Maybe you throw in a third of like a Mongo DB or something that's like document storage. Yeah. But you'd have to have different transformation capabilities for that. And it again would lead to different features and functions. But yeah, I think, I do think it's the middle. Gotcha. 

> Interviewer

> Yeah. Okay. Because I talked to another expert a few days ago and he was actually telling me about migrating from Snowflake more to a containerized architecture with the reason that the computing costs of Snowflake are so high. 

> Participant 5

> Yes. Without a doubt. And we're experiencing the same thing. However, I don't know. I have an interesting perspective on the cost. Yeah. The cost is high. If you use it for, you know, like it depends on what you're using it for. Like it's not meant to be a 24 seven always on warehouse. 

> Interviewer

> Gotcha.

> Participant 5

> If it's always on, it's like you're driving a sports car around the floor. Yeah. You're going to run out of gas soon. Yeah. You know, there's optimization there that if you know the bright pattern, you know, there's, you know, the Snowflake is optimized for storing a lot of data and churning through things in batches. Yeah. And flipping the switch, scaling for your processing and turning it off. Yeah. So you have to look at the landings that was in my mind as channels to say this channel is best for this use case. This channel is best for this set of use cases and make sure people, you know, in Snowflake's eyes, they don't care. You use it, you bring up a bill. Yeah. But it's not like it's also always reactive. You press the button, you're getting charged. You don't, you might not know until after. Yeah. Yeah, that's the first part. So it's, we've had to react to that costs, those costs as opposed to being able to predict. So when he, when the other, when you talk about containerized, what like, what's an example of the tech stack there? 

> Interviewer

> Docker. 

> Participant 5

> Docker, okay. No, that's okay. I had a feeling, but I'm just thinking, I'm thinking, how would you-

> Interviewer

> Or Kubernetes. 

> Participant 5

> Or Kubernetes. And you'd have, see, I view, I don't know, my, and this is my limited technical understanding, but you have, we use Docker for hosting and we use containers for hosting microservices or apps. How would you host micro, how would you host data products of microservices? Similar construct? 

> Interviewer

> Yeah, well, that you just have a container and the container is actually the boundary of all the technical aspects you provide to your data product developer. And there is an internal storage. There is a possibility to do transformations. It is accessible, et cetera. It's just a playground for your data. 

> Participant 5

> Okay, so you're viewing the container as the bounded context in which you have data and then obviously you share data across containers. Exactly. Yeah. And then theoretically- 

> Interviewer

> But yeah, Snowflake and for example, Databricks, those two platforms are also really suitable for a data mesh. So yeah, I understand the, yeah, the migration about Snowflake. 

> Participant 5

> But the difference, I guess the difference that I've seen and I'm experiencing now as we look at Azure, Snowflake has everything in a box. You've got storage, processing, transformation, delivery, all kind of natively. Yeah. Azure has a suite of tools, all with different names that can be combined and customized in any different way you want. More complicated sometimes. Way more complicated. And Docker seems to me like blank sheet of paper. You have your container, which is your playground, and you could bring in open source. Yes, it gives you storage, but you can bring an open source DFT for transformation. You could bring an open source orchestration tools. It is, yeah. So it gives you ultimate flexibility on what you put in that container. Thank you. Yeah, that's it. 

> Interviewer

> But we're moving away a bit from the actual research which is of course the self-serve platform. Yes. Yeah. We were briefly talking about the single data landing zones and have generic special data landing zones. Do you think the other three can be relevant as well so that we have something specialized on the source and something specialized on the consumer and perhaps on regions or that we have multiple governance landing zones? 

> Participant 5

> Absolutely. And I could see the need for source system and consumer aligned feels to me maybe more like a fabric. A data fabric. Yeah, it is. Okay. Because when I think source, I think products are organized and contained there and then somebody is given access to them. 

**OPEN CODING TRACE:**

Snowflake but you have multiple generic ingestion patterns. So, and then we're also thinking about Azure tooling as a secondary channel, which in my mind would be a secondary landing zone. Oh, okay. So you wind up with this ecosystem that has Snowflake with its specific capabilities, Azure with its Azure and Azure native for its specific capabilities. Ingestion patterns should be the same.

container_registry

**AXIAL CODING TRACE:**
added:
``` python

    multiple_data_landing_zones
    
    container_registry
    
``` 

> Interviewer

> Yeah. I actually believe that you can have a data mesh and a data fabric as one. 

> Participant 5

> Oh, without a doubt. Yeah, without a doubt. But that's another discussion. I know. And I think we might as well, we might have it actually but I totally agree. It all just depends on what those sources are doing. So interesting because there are some purists out there who say they're way different. The metadata piece of it is the key difference for another conversation. Yeah. But it's more governance aligned. But I think that's a good point. I do think regionally and large scale management is important, which I think gives you collectively exhaustive MECE for sure. 

> Interviewer

> Nice. All right. Yeah, I think we can perhaps go on to the next work. Well, these three. Yeah, you're really familiar, I guess, with these three over here. The infrastructure provisioning, the developer and the mesh decision. So I will in a little bit. These are more extensive compared to the previous one. So in my perspective, the infrastructure provisioning plane is actually the thing that provides everything outside the data product architecture. So it's more like, okay, how do we make sure that data products are able to communicate with networking, for example, where do we put data products? This can be in a VM environment, for example. But we can also have, yeah, Snowflake, for example, where we separate compute from compute and separate storage from compute. Snowflake is also a polyglot storage option. So there are multiple ways to store your data in JSON, in CSV, et cetera. It's also important to provide some machine learning capabilities such as a feature store, the data transformation orchestration. So how do we make sure that all the, yeah, all the data products are able to be orchestrated in the right way so that the sales data product is able to communicate with the subscriber data product. And that's, yeah, that also has to do with networking, for example, but I think networking and orchestration is a little bit different. This is more on the traffic and this is more about the direction of the traffic. Yep. A processing engine, of course, perhaps I can include that in the data product developer plane, but we can- 

> Participant 5

> No, I believe it's here. Okay. Because standing up the engine is the infrastructure, the usability of that is, so just so you know, this is exactly what I did to set up the mesh as a product. Oh, great. Each of these planes and put in these. So in my infrastructure plan, I'll tell you what I called it. Yep. Called it data storage capabilities, data processing capabilities, data orchestration layer, CSV, data landing capabilities, monitoring and alerting, data security, federated compute, and the data publishing layer. 

> Interviewer

> Gotcha. And that's all connected to the infrastructure provisioning or the 2D mesh- Infrastructure plan. Gotcha. 

> Participant 5

> Yep. And federated compute, you have many, much of it here. Yeah. So what I've seen is that I love that, I've obviously totally aligned with doing this cause we've got it. Yeah. You have to look at each of these individually and understand the context in which, because it's very easy to say, this one might go here or this one might go there. It's also difficult to get people to think this way. Yeah. Yeah. It requires, it's not necessarily intuitive. Like I'm struggling now with just explaining this to my manager, who's more on the governance side. And she has thoughts like, well, when you say, just like when you say processing, well, isn't that something that a product developer experience works with? And it could go both ways, but you kind of have to be stringent on the boundaries and say, I'm setting it here and here's why. Yeah. And from my perspective, I haven't, we haven't found anything outside of, like I have 20, 30, 40, 50, 50, like I have 28 different things across the three planes and I'm interested to see what they are. We haven't found anything that's outside of that. Like it's pretty much handled everything. Like if you say I need platform isolation, well, I believe that's like, that's data security. It helps protect the data and it helps give people the place to play. So you could look at platform isolation and maybe it's in networking, maybe it's in separating storage, like it could be managing compute infrastructure. Like there are ways, just different terminology, but it's to me, it's been tested. 

> Interviewer

> Great. Well, that sounds very much in line with the framework over here. Yeah. I'm glad that you mentioned it. And about the data source ingestion, you also see this as some kind of infrastructure component, for example. 

> Participant 5

> Yes. 

> Interviewer

> I think that covers all of these already. 

> Participant 5

> The only thing that we added here was work management and the ability to, and again, this is a team. This is a mesh platform product piece. 

> Interviewer

> Do you mean like Apache Airflow, for example? So the workflow automation engine, automate certain work processes? 

> Participant 5

> I, automation certainly baked in somewhere. So you have it called out. It just depends on the granularity. Like ours could certainly be more granular, but we wanted to limit it to 28, which is like nine per plane. But no, I mean, like we're using Azure DevOps and Azure DevOps Boards to manage our work, to manage the delivery of these things. So when I think about capabilities, I think we'll work management and how you organize the team. It's an infrastructure component because to deliver data products, you need some way to track them through the pipeline. Yeah. To deliver a new ingestion pattern, you're tracking it with stories in an agile development practice. So, take it or leave it. It might be out of the scope of what you're trying to do here, but like self-service is okay. How do you actually manage the demand through your data mesh pipeline? 

**OPEN CODING TRACE:**

So when I think about capabilities, I think we'll work management and how you organize the team. It's an infrastructure component because to deliver data products, you need some way to track them through the pipeline.

**AXIAL CODING TRACE:**
added:
``` python

    workflow_automation_engine
       
``` 

> Interviewer

> Got it. Yeah. Interesting though. Yeah. I always find it difficult to really distinguish between these three planes because I think there was a lot of overlap between them. There is. There is. There is. We consider the data product developer plane as more the functionalities you want to apply to the data product developer. So, in Zhamark Dehghani's book, there was just one functionality for the build, deploy, monitoring data product. That's just one functionality. There's another one for versioning. For security, the data product developer needs to be able to read the data products. So, for example, in Snowflake, we have the in-place consumption where we use VIEW instead of moving the data program, documenting your data product. So, there needs to be a way to add metadata to your data products. Declaratively create your data products. So, it should be really easy to create your data product and to register your data product. Everything needs to be understandable for the general list, let's say. So, someone who is not really involved in the data engineering practices, but has a bit of knowledge about everything. Testing your data products. So, there should be a testing interface. Yep. And of course, the CRUD operations. So, the create, update, delete, read operations. So, the four main operations related to APIs. So, yeah, these are all the functionalities I discovered. Curious about your... Do you think some of these are redundant or missing out on some features, some functionalities? 

> Participant 5

> I believe you have captured nearly everything that we have. Oh, great. Build, version, there's a life cycle. Well, sorry, data product management. So, I believe, first thing is build it. You need to have tools to build it. And then versioning is the life cycle. Yeah. And again, it just depends on how much you combine. The life cycle in my mind is versioning, monitoring, reading it, and potentially providing access to it. But access, you have to have the ability to provide access to it, which can be driven through the supervision plane. But the product management becomes once it's built, what are the things you need to do with it? Security is obviously there. You have read separate, which is totally fine. Document, that's where... Document is governance, right? You're getting the metadata about it. And what are you doing with that metadata? Yeah. I think that's the big piece here is that it's... Yes, it's being documented because that's key to the mesh, but how are you exposing that metadata? Again, in the supervision plane, like these two are kind of closely linked. Yeah. What else? We look at data retention, that we've called it out separately from data product management, but you could also group that together. We call out the DevSecOps pipelines, the ability to automate a lot of these things. That could be in CRUD operations, or it could be in build, just depends on where you want to put it. But outside of that, we treated transformation separately. Because you could build, but like, if you want... Let's say we're going to give you... We're going to give you a handful of common transformations that you can do with a standard set of tools. If you wanted to bring in a tool like DBT for your complex transformations, we want to give you the ability to plug into that. So that's the self-serve aspect of, we're going to give you some standards, but allow you to bring things that you want. 

**OPEN CODING TRACE:**

testing_dp

crud_operations

Document, that's where... Document is governance, right? You're getting the metadata about it. And what are you doing with that metadata? Yeah. I think that's the big piece here is that it's... Yes, it's being documented because that's key to the mesh, but how are you exposing that metadata?

We look at data retention, that we've called it out separately from data product management, but you could also group that together. We call out the DevSecOps pipelines, the ability to automate a lot of these things. That could be in CRUD operations, or it could be in build, just depends on where you want to put it.

But outside of that, we treated transformation separately. Because you could build, but like, if you want... Let's say we're going to give you... We're going to give you a handful of common transformations that you can do with a standard set of tools. If you wanted to bring in a tool like DBT for your complex transformations, we want to give you the ability to plug into that. So that's the self-serve aspect of, we're going to give you some standards, but allow you to bring things that you want. 

version_dp

**AXIAL CODING TRACE:**
added:
``` python
    
    testing_dp
    
    crud_operations
    
    version_dp
    
    document_dp
    
    build_deploy_monitor_dp
    
    data_transformation_orchestrator
       
``` 

> Interviewer

> I have a very good example of that actually, because I'm trying to make a data mesh MVP myself, just to convince the senior people about the value it can bring to your company. And now I'm actually trying to make as much possible in Terraform. Are you familiar with Terraform? So with infrastructure as code. For example, we are using Snowflake and AWS, and now I created this Terraform template, and you only need to run it. And it already connects your Snowflake account with your AWS Glue data catalog. And this week there will be a release of Amazon DataZone, which has almost all the functionalities of the third party catalog. So that's really nice. I'm curious to play a bit with that. Interesting. It's more about, yeah, that we provide a Terraform template that actually provisions the entire infrastructure for you. So yeah. 

> Participant 5

> Well, and that to me falls in the infrastructure plane. Yeah. Cause you're provisioning, like you have automated scripting that provisions the experience for the developer. Gotcha. And it gives them their container, you could Terraform wherever, whatever makes the most sense, but yes, absolutely. It fits in there. 

> Interviewer

> Yeah. Gotcha. Really interesting. By the way, this is just a personal question, but when you adjust the metadata in Snowflake, so the document DP, do you use your Colibra cataloging functionality for that? Or do you do that just in Snowflake itself? 

> Participant 5

> We're working on the feedback loop. A feedback loop. Yeah. Colibra houses metadata. And then the metadata about a product is housed there. And then ultimately there's a business glossary. Gotcha. Yeah. That has the standard terms that are used. And the link from glossary to product describes the context of use. Gotcha. And it's capturing that chain and that linkage that is the valuable piece. Yeah. You know what this term means in the context of a data product. And then, you know, the domain, it's tied to a domain context. So as you're searching it, you can understand and be aware of, it helps you understand what the ingredients are in your product. 

**OPEN CODING TRACE:**

And then ultimately there's a business glossary. Gotcha. Yeah. That has the standard terms that are used. And the link from glossary to product describes the context of use. Gotcha. And it's capturing that chain and that linkage that is the valuable piece. Yeah. You know what this term means in the context of a data product. And then, you know, the domain, it's tied to a domain context. So as you're searching it, you can understand and be aware of, it helps you understand what the ingredients are in your product. 

feedback_loop

**AXIAL CODING TRACE:**
added:
``` python

    data_catalog
    
    feedback_loop
       
``` 

> Interviewer

> Gotcha. Okay. So you're not running a particular query to adjust all the metadata because I was thinking about it and I only knew the normal metadata of the data set, but I didn't know how to adjust the metadata of the database. So yeah, this can be really valuable. 

> Participant 5

> Yeah. And it's logical and physical documentation too, because your table column schema, schema table column. 

> Interviewer

> Policy enforcement, right? Yes. Which we'll talk about in a bit for the... Okay. But yeah, it's actually enforcing the metadata into the Snowflake database and not any data sets. Okay. Got it. All right. Interesting. I think we covered all of these options already. So just let's go on to the... So almost the last one already. There are five decisions this time. So this is more about the data mesh supervision plan, the governance. There are certain functionalities we need to provide as a self-serve platform team to the governance team. So they need capabilities of course. For example, alerting. We briefly talked about that already, but that is included into the data mesh supervision plan, but we were talking about the infrastructure provision plan. Perhaps we can debate a little bit about that one. Schema registry, of course, really important to have interoperability mesh. API catalog. So what kind of API, what types of APIs can we look out for? Manage emergent graphs of data products. So this is more about the connections between data products, which are out there, which connections are out there. So the connection between the sales data product and the recommendation data product. We need to know that there is a connection. Manage security policies, of course. The enterprise data product catalog. That's something we need to provide to the governance team. A knowledge graph, which is really interesting because a lot of practitioners actually forget about this one. And the knowledge graph is actually the semantic layer. And I read the book of Piethein Stengholt called Data Management at Scale. A really, really good book. And he was actually talking about the meta lake and a meta lake consists out of a data product catalog. So your enterprise data product catalog and a knowledge graph. And a knowledge graph actually connects all the relationships between the different concepts in your metadata. So for example, we have a user and this user is connected to our address, is connected to a bank account. Yeah, everything related to the user. And this is done for all the data products in your data mesh. So we would get a really extensive knowledge graph but we keep track of all the different concepts and we make sure that everyone is using the same concepts, et cetera. So that's about the knowledge graph. Log management, so what operations are being executed. Yeah, a form of master data management so that if we are talking about a central entity like a customer, this customer means the same in each data product, the master data. Metadata management, I was thinking about this and perhaps I can use the data product catalog and knowledge graph as two side notes of this one. Yeah, the last data quality management we want to make sure that there is a certain level of quality in our data mesh. So yeah, that's actually all about the governance plan. Really curious what you think about these capabilities. And for example, that we placed alerting in the supervision plan and not in the infrastructure provision plan. 

> Participant 5

> So that's again, one that kind of can span both and the way the convention we've used is your infrastructure generates the alerts. So you need to have a capability to generate the alerts. Supervision is just the observation of those alerts. So the supervision plan is a consumer of alerts and it provides observability to users. So I think of it as the console, the operations console that says, here's what my platform's doing. So again, totally fine. It just depends on the convention you want. Do our alerts, a function of your supervision plan could go either way. But I guess we just set the convention that the infrastructure itself and the components need to be able to generate alerts for supervision to read. So that's where like alerting, log management, like I just kind of view that as like platform infrastructure monitoring and infrastructure. I think I agree with that one. Yeah. Then the thing that I wanted to talk about knowledge graphs quickly because metadata management, master data management, sorry, it's kind of like you need to know, I don't know if it helps the supervision plan and is the knowledge graph built on top of the products? 

**OPEN CODING TRACE:**

So the supervision plan is a consumer of alerts and it provides observability to users. So I think of it as the console, the operations console that says, here's what my platform's doing. So again, totally fine. It just depends on the convention you want. Do our alerts, a function of your supervision plan could go either way. But I guess we just set the convention that the infrastructure itself and the components need to be able to generate alerts for supervision to read. So that's where like alerting, log management, like I just kind of view that as like platform infrastructure monitoring and infrastructure.

**AXIAL CODING TRACE:**
added:
``` python
    
    schema_registry
    
    alerting
       
``` 

> Interviewer

> What do you mean? 

> Participant 5

> So do you glean the knowledge graph from the products themselves? So product A connects with product B in a specific way. 

> Interviewer

> Yeah, but then on a deeper level. So the metadata consisting inside the data products. So a sales data product has information about the customer, the order item ID, the inventory, the address of the customer, the payment received, all those metadata, yeah, all this metadata related to this particular data product that's also present in the knowledge graph. So there are certain links between all these metadata features variables. I can perhaps quickly show you a picture of what it would look like. 

> Participant 5

> Yeah, and I guess I asked because, knowledge graphs and master data, master data sets the standard of what we use and what we term knowledge graph would set what? Like, is it a layer on top of that? It's the application of the master data. I shot, I need to read the section in Piet's book about that because you end up with, like it's those pieces that I'm just trying to draw the connection to. 

> Interviewer

> Let me, it was from Cambridge Semantics. Ah, here I got it, yeah, okay. Oh, are you able to see it? 

> Participant 5

> Mm-hmm. A little bit, oh, 

> Interviewer

> I got it here. Zoom in for some reason. Yeah, yeah, so we have a central entity and I assume it gets bigger if there are more connections to it. Yeah, yeah. And this is actually how you keep track of all the semantics in your data. Okay, and it's your data map, it's your network diagram, if you will. It creates some more visibility. Yeah. Yeah, and it's important to automate this as well because yeah, I think people- Without a doubt. Without a doubt, yeah, people can be lazy sometimes. 

> Participant 5

> So we have something called data flows, which was probably the same as knowledge graph. Let me see how I described. I described data flows in a way, like it's interesting because we're constantly going back to the descriptions of how we define this thing in theory. Yeah. Data flows. Yes. Inputs, relationships and outputs. Yeah, that's exactly what I mean. That's some close graph. 

**OPEN CODING TRACE:**

So we have something called data flows, which was probably the same as knowledge graph. Let me see how I described. I described data flows in a way, like it's interesting because we're constantly going back to the descriptions of how we define this thing in theory. Yeah. Data flows. Yes. Inputs, relationships and outputs. Yeah, that's exactly what I mean. That's some close graph. 

**AXIAL CODING TRACE:**
added:
``` python

    knowledge_graph
       
``` 

> Interviewer

> Yeah. Oh, great. That's pretty good that you are keeping track of already because most companies forget about this part. 

> Participant 5

> Yeah, it's an important piece. I think it will come to fruition eventually. Yeah. But how far along it is is the debate. But I think the important thing is you wind up with, like you're touching on all the different pieces that are comprehensive, which is, again. 

> Interviewer

> Yeah. Great. Nice. And for example, about these two over here, so security policies. Yeah, that should be a functionality provided to the governance plane to actually, yeah. And for- Without a doubt. Yep. Yep. This feature as well, I guess. Emergent graphs. Tell me more about, tell me emergent graphs again. I took a note. Yeah, so this is actually what we first considered as the knowledge graph, but it's more on the high level. So we want to know that the sales data product is interacting with the governance, with the recommendation engine and with the customer data product and with the inventory data products. So it's about the connections between the data products and not between the data variables, but between the data products, who's interacting with who. Okay. And of course, an API catalog and schema registry. And then I think we discovered that, or well, we covered everything. 

> Participant 5

> Yeah, I view, you know, the API catalog, the data product catalog, that's telling somebody what's on the mesh. What's on the mesh, how can I get access to it? Yeah. The emergent graphs, the knowledge graph tells, is informative and descriptive about what's on the mesh. Yeah. Got the catalog, the actual inventory, the descriptors, and then the control about it. But all of those things are, you know, you could get into, you know, compliance reporting as it relates to quality, but I think you hit the key elements. Yeah. And that's probably the only thing that we've highlighted a little bit separately, but it's a specific need that we have to say, well, let's look at how we can use this observability to better manage or better automate or better make decisions on our data and where to focus efforts. Gotcha. Yeah. All right. Yeah, I think they were on the same page with this one. Really cool. 

**OPEN CODING TRACE:**

Yeah, I view, you know, the API catalog, the data product catalog, that's telling somebody what's on the mesh. What's on the mesh, how can I get access to it? Yeah. The emergent graphs, the knowledge graph tells, is informative and descriptive about what's on the mesh. 

**AXIAL CODING TRACE:**
added:
``` python

    api_catalog
    
    data_catalog
    
    knowledge_graph
    
    manage_emergent_graphs_of_dp
          
``` 

> Interviewer

> Really cool. Yeah, really cool indeed. It's also good to get a confirmation on certain options, et cetera. So thank you for that. Same. Yeah, same. That's how we help each other, right? Yes. Okay, this is the last framework and this is about the UI. So how do we make it accessible for the end user? I discovered several functionalities, but I think there can be more out there. So of course the cataloging function, they need to be able to search for the data product, discover the product, et cetera. Just know what's out there. The visualization function. So there can be a small visualization function which enables of course data lineage, some preliminary visualizations about the data sets, things like that. Data profiling. Data transformation function. This can be more like a no-code functionality that we just have a certain symbol that, for example, enables the user to extract all the missing data out of a data set, or just by clicking on this symbol, we can make sure that all the duplicates are removed from the data set. It's just like cleaning processing steps, data transformation steps with no code so that we really make it easy for the product developer. 

> Participant 5

> Wow. 

> Interviewer

> A data governance function. So yeah, the data product developer can make their own local policies, but also needs to adhere to global policy. So we can observe this one as actually provided to the mesh supervision plane, as well as to the product developer plane. There are two ways of implementing these. An application build function. This is an option, of course. It's not mandatory, but it can be really suitable for software as a service practices. Data integration function. So we can indicate what operational databases we wanna use, or we can perhaps export a data set function or add a data set function, import a data set. And the last one is about query recommendations so that we actually support the data product developer in their, yeah, in their query formulation, if you know what I mean. So in how they construct their queries and do certain practices. So yeah, this is about the UI, but I think there can be more out there to make the self-serve platform even more accessible for the generalist, let's say. 

> Participant 5

> That to me is key is who's the user of this. Yeah. Go ahead. 

> Interviewer

> So the user would be the generalist. So not the data engineer, for example. Everyone needs to be the generalist. Everyone needs to interact with this one. Well, needs to be able to interact with it. Yeah. 

> Participant 5

> Yeah, we are considering this as we look at, to me it's, well, it's part of supervision because it's the interaction with the mesh. And it is the discovery and exploration side of the supervision piece. There's a catalog piece, but there's also the interactions with. So if I look up a data product, you've done another good job of capturing things that are like, this is just about everything you would wanna see from a data product. Like it would be great. Wouldn't it be cool to see a knowledge graph of a data product? Yeah, definitely. Like in its record, like you get handed a card or you could show on a screen that says, here's my product, here's what it contains, here's its quality measures. Yeah. Here's what you can do with it. We're gonna recommend, it's got this number of null fields, this number of duplicates, whatever the quality would be. Yeah. And someone can choose. It's all about the generalist making a decision if they wanna consume that data. Yeah. And once they make a decision, okay, you need to enable them with the decision and then you need to enable them to actually take action on it. Because that's where the value comes in is again, creating all this stuff behind the scenes is not valuable until it's actually put in the hands of somebody to be put to use. So to put it to use, you've got cleansing that's needed. Yes. Again, takes so much time if you're gonna do analytics. Yeah. And I guess the debate that's kind of out there is, by the time you get to a data product, you have a pretty, you should have a pretty well-defined implementation or use case. So now that back to an analytics manager, they just wanna look at everything. So sometimes they don't want clean data. They need data to be in a more raw format. If they're looking at kind of, I'll use raw data poorly here, but if they're looking at raw data and they run some modeling or some views, I say, well, now I wanna dive into this part of data, which is actually gonna become the data product because you could productionalize an ML model based on top of it. Yeah. But that to me, when you provide that to an analyst or even a general, like you probably provide that data product to an analyst, you've already gone too far in cleansing the data. You already know too much about it. So it's not valuable to the analyst. It's valuable to the consumer because they may wanna consume that insight, but this is kind of the struggle a little bit with the right level of data providing access via a mesh to somebody who's gonna glean insights out of it. 

> Interviewer

> Yeah, indeed. Gotcha. Do you think there can be more functionalities compared to this framework of the year? For example, yeah, things like integration, governance, transformation, visualization and cataloging. So what does application build? Is that how you consume it? Yeah. This was more for the software engineer. So how you actually build your application. So how you actually make your products is more. Yeah, but now I'm thinking about it. I could phrase it differently. Yeah. And that we, yeah, it's more like how to register your data product, how to create it from scratch, that you provide some support in that. Yeah, that's perhaps even better. 

> Participant 5

> Yeah, if it's self-service then you need, you know, like, again, you gotta think about the users. When you said generalist upfront, they're not an engineer. No, no. So they need, yeah, a low code portals, ability to do things with clicks instead of lines of code. Exactly. Where you wanna go with this. And I think the piece that to me stands out is, you have, go scroll to the left for a second. What do you do with the data? What makes it valuable? The visualization, you create reports. Yeah. Like where's your analytics piece here? It's analytics. Like if you're gonna like... 

**OPEN CODING TRACE:**

Yeah, if it's self-service then you need, you know, like, again, you gotta think about the users. When you said generalist upfront, they're not an engineer. No, no. So they need, yeah, a low code portals, ability to do things with clicks instead of lines of code.

**AXIAL CODING TRACE:**
added:
``` python

    data_transformation_function
          
``` 

> Interviewer

> Yeah, you can easily use it for both. So you can use it for data lineage as well as building reports as exports. 

> Participant 5

> Profiling, okay. Yeah. Yeah. And then like the application side of it, do you build an app that operationalizes the data? Are you building an app that sends data to a UI for someone else to use? Are you building reports? Are you connecting it to Power BI or a Tableau or a visualization tool? It's more about the, like you look at in this UI making each of the planes part of it. 

> Interviewer

> I get it indeed. This option over here is too general still and it should be spied even further. Thank you for the feedback. That's really valuable. Of course. Just too general. Of course. Multiple, well, it has multiple interpretations and that makes it difficult for a practitioner to use this framework perhaps. 

> Participant 5

> Yeah. But yeah. If you can tie this back, this is your UI into the three planes. You're giving somebody their space, you're giving them the tools they need and you've captured the essence of that. To me, it just helps to maintain that plane architecture as you describe it to people. Yeah. And that plane architecture is general enough and it allows you to be flexible and have to capture all the different pieces that then someone can, maybe not intuitively, but you can easily logically think through it. Yeah. Which is why to me it's so powerful with a little bit of thought and explanation, it covers 99\% of the use cases. 

> Interviewer

> Yeah. Indeed. This is actually related to my last two questions because in terms of time, I don't wanna take too much time of your schedule. So now we come back to the inter-decision framework we were talking about in the beginning. Yeah. Well, as we can see over here, the first decision would be about, yeah, it's more high level. How do we want to, how many self-serve platforms, for example. Yeah. So these are all planes and this all comes back together in the UI and the UI provides an interface for all three of them actually. Yep. So my last two questions, the first one is about, we all think the framework is understandable for practitioners that it's easy to implement, et cetera. And the other question is more about how useful can this framework be for the practitioner? So it's more about usability and about interpretation, how easy is it to understand? 

> Participant 5

> So to me with all the context, it's very easy to understand. Yeah, with the context. But however, it's elegant because it has, you tie it to theory and if you get the theory and you understand what the description of those planes is, then you can absolutely tie that plane to a, because every time I explain the planes, I have to go into, this is what Zhamak says the planes are. I've actually changed product developer experience to data experience because of data in and data out. Again, different interpretation that helps simplify it for people. But yeah, if you compartmentalize it, then you help them understand it and use it. Yeah. And the value is absolutely there because you're giving people the tools and the things to consider when implementing this in their specific context. Yeah. And it's not simple. No, for sure. But that's the point. This simplifies it because there are so many things you need to consider. This is a playbook and a cookbook to help you do that, that to me is powerful from a repeatability standpoint. And it takes the theory and puts it into practice. You got it. Yeah. 

> Interviewer

> That sounds really good. There we go. Yeah. All right, > Participant 5, thank you for all this, all this new information we gathered to- 

> Participant 5

> You got it. You got it. Let's find time to keep in touch, whether it's once a month or whatever.I'll be back downtown probably twice a week. So let's get coffee in April or something. 

> Interviewer

> Yeah, sure. Yeah, because we have the governance principle left. That would be the last framework. Yeah, we can talk about it in a month, I guess. 

> Participant 5

> Yeah. Whenever you're ready. Whenever you're ready, let me know. 

> Interviewer

> Great. Thank you so much. You got it. Have a good day. 

> Participant 5

> See ya. Bye. Bye.
