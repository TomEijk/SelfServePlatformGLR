# p
2
## url
https://github.com/TomEijk/Data_as_a_Product_GLR/tree/master/python/self-serve-platform_codeablemodels/field_notes_open_coding_axial_coding
## tiny url
https://tinyurl.com/self-serve-platform-p2
## archive url
https://bit.ly/self-serve-platform-p2
## title
Interview Expert 2
## source code
no
## example
yes
## source type 
Interview
## author type
Practitioner
## references

**AXIAL CODING TRACE:**
``` python
p2 = CClass(source, "p2", values={
    "title": "Interview Expert 2",
    "url": "https://github.com/TomEijk/Data_as_a_Product_GLR/tree/master/python/self-serve-platform_codeablemodels/field_notes_open_coding_axial_coding",
    "archive url": "https://bit.ly/self-serve-platform-p2",
    "tiny url": "https://tinyurl.com/self-serve-platform-p2",
    "author type": "Practitioner",
    "type": "Interview",
    "example": True,
    "source code": False})
```

# coding

> Interviewer

> Record. Okay, recording started. Okay, I'm Tom. I'm Dutch 25 years old. I'm doing my thesis for Bain \& Company. I'm doing this in Chicago right now. So perhaps you've noticed that in the background. My thesis is about the architectural design decisions you need to make during data mesh migration. So what kind of architectural design options do we need? For example, if you talk about data products, there is a need for schema registries, a data catalog. Of course, you can choose for an event streaming backbone like Kafka, things like that. So it's really about the architecture itself and not not on the social aspect aspect per e. So it's yeah, it's the objective is to provide some guidance for the practitioner during their implementation. But this is more in an academic way because it's a thesis, of course. So, yeah, that's about me and I'm doing by the way, the master data science and entrepreneurship. It's it's about combining business decisions with more technical aspects. 

> Participant 2

> So that's where data really fits in, I guess. All right, cool. So I'm > Participant 2from Agile Lab, 39 years old, quite older. And I'm a staff data architect at Agile Lab. As a senior consultant, I'm driving data mesh initiatives for some customer of our work. Focusing on the data strategy itself since a data mesh implementation journey is a data strategy. Like it's the way you implement your data strategy. Right. So we help in the sense, of course, from being a data architect, both from an architecture point of view, but also from a operational model point of view, which includes the way federated governance, for example, should work. And that has a lot of adherence with the first thing that you've mentioned, which is, you know, eventually it's about architectural decisions. So even even federated governance decisions, those are to me architectural decisions. There can be different owners who should be accountable of making those decisions. Something real, computational, based on automatic stuff or not. But those are architectural decisions. So probably at the end of this meeting, we will end up, I think, having some common thoughts of the fact that there might be the need of a framework to standardize those decisions. 

> Interviewer 

> Oh, wow. That's exactly what I'm doing, actually. So you will notice that in a bit. I already did a lot of research for the data as a product principle. So those frameworks are ready. I've done a couple of interviews to validate it as well. Perhaps if you are interested, I can send it to you as well. Then, yeah, you can perhaps have a read. But this interview is more about the self-serve platform. And let me just show my screen and then we can have a chat about it. So I made everything in Python. And how I performed the research is I studied a lot of grey literature. And grey literature is actually everything that hasn't been officially published. So we can think about YouTube videos, blogs on Medium. 

> Participant 2

> Published, you mean as research papers. 

> Interviewer 

> So everything else that's grey literature. Over here, I've used like 43 sources. And after those sources, then nothing new came up. So these sources were selected. They had a lot of information. And after source 43, there was no new content anymore. So that's actually my research. And yeah, I looked for similarities in all the articles. And based on the similarities, I discovered some decisions and these decisions are created over here. So for the self-serve platform, I actually observed five different decisions. And the first one is how to align a self-serve platform and its components with data mesh. So this is more a high level perspective. And we will see that in a bit. On the second layer over here, we actually see the three well known planes, which were mentioned in the book of Zhamak Dehghani. So the infrastructure provisioning plane, the data product developer experience plane, and the data mesh supervision plane. And actually the last decision you need to consider is, yes, more about the user interface. So how do we create a self serve UI that is accessible for the general list? As Zhamak Dehghani calls it. It's like a T-shaped person who is really specialized in one single area, but he or she knows a little bit about everything. So it's more like the general. So if we talk about the first one. This is more about the self serve platform in a very high level way. So do we need a single data landing zone, a single self serve platform? Or is it more convenient to, for example, use two different kinds of self serve platforms? So a self serve platform that is being used by all these source aligned domains, and another self serve platform that is being used by consumer aligned domains. So there's a clear distinction between them. We can have a hub generic and special data landing zone way of implementing a self serve platform. I will quickly show you that it's actually based on the article of Piethein Strengholt. Yeah, we have a central hub over here. And yeah, that one has its own self serve platform, and it's actually distributing all the traffic in your data mesh architecture to the other data domains and they also have their own self serve platform. 

> Participant 2

> And his vision is interesting, although I think it's a bit biased by the fact that he works at Microsoft. Yeah. Which provides some kind of framework for these landing zone, data landing zone, data management landing zone. But yet the principles are super sound. 

> Interviewer 

> Yeah, indeed. And the last two are actually, when we really distinguish it on functional or regional region, or we have just a data management zone for every single business unit in our data architecture, I mean our company, let's say. Yeah, these are the decisions, well the decision options I discovered, and they were mentioned in some other articles as well. I'm really curious if you recognize some of these patterns in your practices, or I'm really curious about your opinion on this or perhaps you. 

> Participant 2

> Yeah, for example, since you're recording I cannot spell any names, but for our customer of *, I've chosen the multiple data landing zones in one to one mapping with domains, with one central data management landing zone. Yeah, that's a good question. I guess. For centralized services like data catalog, for example. Yeah. We had a lot of implementation on Azure, so it was fitting quite well like having a purview on this data management landing zone while having different, you know, decentralized pieces of infrastructure within each data landing zone, although we realized that our services are not super ready to embrace a fine granularity of data landing zone, because you're going to end up hitting some like cloud limits, you're going to end up in complex enterprises hitting problems with network for like, I don't know, a single Databricks workspace, for example, have strong requirements in terms of the number of IPs to reserve, if you have a private network, so there are several limitations. Yeah, you end up encountering if you start really splitting up your infrastructure. But there's no one size fits all architecture in this sense, I think, in this case, the diversity of the patterns that Strengholt has presented are really needed because there can't be just can be just one, two feet of scenarios. 

**OPEN CODING TRACE:**

I've chosen the multiple data landing zones in one to one mapping with domains, with one central data management landing zone.

**AXIAL CODING TRACE:**
added:
``` python
    multiple_data_landing_zones
``` 

> Interviewer 

> Yeah. But do you think there can be another option as well, another design option that you encountered in industry? 

> Participant 2

> I think these options basically cover all, I mean, besides the terminology, like, yeah, because the data landing zone is very Azure centric. Yeah. And if you remove that, you know, I've seen, you know, if you plot like X, Y, the fact that you have like domains identified more from a business perspective rather than an IT perspective. And I've seen, you know, shifting on this axis, you know, every scenario, like I've seen customers identifying domains as IT perimeter, you know, units like the mainframe domain, the website domain instead of, I don't know, the risk domain, sub domain in the finance domain. You know, so I've seen all kind of delineations of, you know, so but I think these options cover most of the things that I've seen. 

> Interviewer 

> Nice. And we can perhaps quickly go to the other one, because this is more about the infrastructure. This is a very extensive one, to be honest. So I will let's just start on the left. This is actually. Yeah, it's about the infrastructure. So what do we need to implement around our data products? So how do we now create the right infrastructure for them to operate? I discovered several architectural components such as the workflow automation engine. This can be Apache Airflow, for example. Of course, we need a processing service which can handle batch as well as stream processing and networking. For sure, because the VMs need to interact with each other, the data products need to interact with each other containers. Data source ingestion. So how do we ingest the data in our data product? Data transformation orchestration. So how do we orchestrate our architecture? The managed compute infrastructure. So what kind of compute engines do we need? For example, we can choose for a separation between compute and compute. So each data product can have its own computing engine like like is on Snowflake, for example. A polyglot storage option. So there can be multiple versions. Well, there can be multiple data types ingested into the storage. It's like a sort of centralized storage kept in our platform. And the last option is to create a machine learning platform so that we can provide a feature store and all the necessities to execute machine learning practices. So it's mainly about the options over here and on the bottom, we see more like extensions of these options. So that's less important, but it provides you with some elaboration on a certain design option on the top. So what's your opinion on this one? Because I think this is always a bit tricky because there are a lot of components. 

> Participant 2

> We have to list all the infrastructure pieces that might be required in a data mesh architecture. I think you're going to end up with a whole catalog of cloud services. Because it depends on the scenario that you have ahead of you. If you want to generalize, I can think of some missing pieces like from a semantic layer perspective, like data catalog and a semantic layer on top of it. What tracks down your lineage? 

> Interviewer 

> That's actually what is done over here. So this is really the distinction between the mesh layer. So it is more like a management layer. Here we have the data product developer layer. So what does our data product developer need and how can we provide it to him or her? And this is really about the infrastructure. So we should see those separately. But yeah, indeed, a semantic layer that one was mentioned over here. So the knowledge graph. So this is really about the infrastructure. So we shouldn't consider the data products itself and the data management layer. So it's really about everything around it. Everything else. 

> Participant 2

> Yeah, this is tricky. Because also depending on some patterns and best practices that you want to adopt, you can end up having other stuff like queue management systems for an enterprise bus for interprocess communication and distributed processing organization. But yeah, I'm curious how you ended up factoring out these things. 

> Interviewer 

> Oh, you want to know my research method? 

> Participant 2

> Yeah, just a little bit of it. I'm curious how you ended up with a map like that, which is a frame or starting from, you know, is that something that you have covered out like automatically or like this? So you have to go through one by one. Understanding what were the pieces and tagging them and associating them. 

> Interviewer 

> Yeah. Well, actually, this is an example of an article. I converted this article in a readme file. And when there was something interesting in just a paragraph, I made a note. So here we have we have some open coding. So I noticed that there is data lineage, API catalog. These are all architectural design options that I recognized from this single piece of text. I made a note here as well for the actual coding. So that's where we actually grew. We put the codes into our yeah, into our Python engine, let's say. And I can quickly show you more. Generate. No, it's not. It's over here. So this is where all the all the codes come together. So here I noted all the patterns. We just saw the API catalog. We have practices as well. Here I make the connections. So we have a mapping decision. We were just talking about the single lending, the different lending zones. This is I can add forces as well. So, for example, choosing for multiple data lending zones. Can as a negative influence on complexity. So these are forces. These are things that influence your architecture. So, yeah, for example, positive control over data if you choose a generic special data lending zone. You need to end up with a decision eventually. Right. And these. So these are all the articles. For example, this is source for this is the metadata. The article. And these are the codes that I discovered in the article. So they were talking about managed compute infrastructure. They were talking about configuring scheduling a scheduling pipeline, for example. So that's how I actually executed the entire research. Quite some work there. Yeah. About a month, I guess. Yeah. And then, yeah, less than I expected. Well, I did the same for the for the data as a product principle, and that one took three months. But it also helps you to recognize patterns quicker, faster. So I already performed this one and I learned how to read those articles. I learned how to recognize the patterns, the options, everything that has to do with architecture. So for the self-serve platform, everything went more smooth and I didn't have to start from scratch, if you know what I mean. 

> Participant 2

> Yeah. And how did you rank the article selected, the contributions? Because what I've seen, and this is kind of a pain point of the whole data mesh community that I've seen several different perspectives. Some of them I agree with, some of them I strongly disagree with. 

> Interviewer 

> My objective is not to provide the best way to implement your data mesh architecture. It's just to show you what is possible so we don't have to choose for VMs. We can also build our data architecture in Snowflake entirely. And then we don't use VMs, for example, or we don't have to choose for workflow automation engine. It's not necessary, perhaps, to implement Apache Airflow. So it's just to show you all the options, what is possible. And then you can make the decision yourself and choose for the right option according to yourself. So it's more to provide some guidance in what's possible and to help you in your journey. Instead of saying like, this is the best way to implement data mesh, you should adhere to every option I mentioned here. That's not the case. It's just to provide you with all the options I encountered during my research. 

> Participant 2

> It's a kind of a blueprint of what you might end up requiring, which can be then implemented and materialized. As many options that you want because the market addresses every kind of possibilities. 

> Interviewer 

> You can choose, for example, five options out of these eight, nine. So. Yeah, it's entirely up to you which one you prefer. And I can perhaps quickly show something else as well. So this is my thesis in Overleaf because LaTeX is so much better now. 

> Participant 2

> Yeah, I remember that could be my thesis in papers. So these are the resources. Oh, this is a bit. 

> Interviewer 

> Let me see the right part of my screen as well. Yes. Yeah. Great. So this is the overview table. This is one of the design decisions you can make during a data product implementation. So what strategy do we want to migrate? Do we want to start a greenfield development? Do we want to do a legacy modernization so we already have everything in the cloud? Or do we want to move our on premise architecture to a cloud architecture, but in a data mesh way? Over here, there are these are the evidences. So where this particular option was mentioned in which source. And over here, we see the forces. So if we check, for example, the immutable change audit log force. Twenty two positive influence on force. Twenty two or twenty two is like verifiability. So if you choose to implement an immutable change audit log, it will increase the verifiability of your data architecture. And verifiability was mentioned in source forty eight and source. Fifty four. So, yeah, that's actually. The objective of my research. You can see everything from a business perspective as well, if you really pay attention to the forces. Yeah, cool. Yeah. And this chapter, I would I want to send I want to send it to you after the interview. You can have perhaps have a look. So it's nice to get some information, I guess. Yeah, but if we come back to this framework. What's your opinion on it? Do you think we're missing something or some options are redundant over here that we don't need to consider them in an infrastructure provisioning? 

> Participant 2

> So from. An infrastructure provisioning. You know, I am a bit biased. Yeah, this because I don't know if you have seen it, but as a giant lab, we we do have a platform. Yeah. That we said as a product for both self provisioning and and au> Interviewerate governance. Yeah. And also has. A data product marketplace on it in the data mesh tier. So. I don't I'm trying to be not too much biased from what we have created. But but of course, you know, from. 

> Interviewer 

> From this, everything is anonymous, so I won't note agile. 

> Participant 2

> No, no, it's not it's not because of that. You can you can absolutely mention that it's a pleasure to be acknowledged, but I'm trying to give you a feedback not to be too much biased from what we have done. Which, of course, is what we believe. You know, it's the right thing to do. Of course, otherwise you have a big problem. But but but yet. You know, the theory you have read the book right and had as well. So. If I look at the problem of provisioning its products. That part is made of multiple steps that can lead to infrastructural pieces. So the first one is some kind of engine that somehow validates your request. OK. And that can range from anything that for you is a data product, which can include any of these infrastructure pieces. But that can go beyond that and go into the data contracts area, for example. So it can include, you know, what? What are you, you know, promising to your consumers of what you are going to provide on your output ports? So that can include also, you know, terms and conditions that can include, you know, billing models that can include service level objectives and data quality stuff. And also can include, you know, endpoints for your observability APIs like, OK, you can come and check how my data product is performing and is functioning from this endpoint. At the validation point, you can implement as many checks as you want on this request, which is a static deploy time kind of thing. So if this validation gets through, then you can have some kind of services that will actually perform provisioning and deployment. And then you can have something at runtime that keep, you know, that performs runtime validation, runtime checks. Like, for example, is there like a schema drift, like from what you have promised you were going to publish on your output ports with what is currently being produced by your jobs? That I can check on your metastore or on your data catalog. How is your quality performing compared to your SLOs and SLAs? And where are you plotting and how do you automate? How do you make quality actionable? Because it's just a metric on a dashboard. To me, that's kind of useless because you need some kind of actionability of those kind of things. Otherwise, you still need humans to go there and then think, OK, now what should I do? And, you know, who has been impacted by these decrease, sudden decrease of quality and stuff like that? Yeah. So this is what the platform should do from a provisioning perspective and governance automation perspective. 

**OPEN CODING TRACE:**

So the first one is some kind of engine that somehow validates your request. OK. And that can range from anything that for you is a data product, which can include any of these infrastructure pieces.

But that can go beyond that and go into the data contracts area, for example. So it can include, you know, what? What are you, you know, promising to your consumers of what you are going to provide on your output ports? So that can include also, you know, terms and conditions that can include, you know, billing models that can include service level objectives and data quality stuff. And also can include, you know, endpoints for your observability APIs like, OK, you can come and check how my data product is performing and is functioning from this endpoint. At the validation point, you can implement as many checks as you want on this request, which is a static deploy time kind of thing.

At the validation point, you can implement as many checks as you want on this request, which is a static deploy time kind of thing. So if this validation gets through, then you can have some kind of services that will actually perform provisioning and deployment. And then you can have something at runtime that keep, you know, that performs runtime validation, runtime checks. Like, for example, is there like a schema drift, like from what you have promised you were going to publish on your output ports with what is currently being produced by your jobs? 

That I can check on your metastore or on your data catalog. How is your quality performing compared to your SLOs and SLAs? And where are you plotting and how do you automate? How do you make quality actionable? Because it's just a metric on a dashboard. To me, that's kind of useless because you need some kind of actionability of those kind of things. 

version_dp

container_registry

**AXIAL CODING TRACE:**
added:
``` python
    workflow_automation_engine
    
    data_quality_management
    
    schema_registry
    
    data_catalog
    
    contracts
    
    container_registry
    
    version_dp
``` 

> Interviewer 

> Nice. Yeah, then we can perhaps quickly go to the data product developer decision. So this is more for the data product itself.

> Participant 2

> Developer experience plane, right? OK. 
 
> Interviewer 

> Yeah, exactly. The developer experience plane. So there should be one function that provisions the build, deploy, monitoring features needed for the data product. We need a version data product. We need to be able to version our data products to secure our data product. Of course, we want to read it. We can document. We have a document data product or document function so that we can make some notes during implementation, for example, versioning, stuff like that. Schemas. They collaboratively create data products. What I mean with declaratively is that it should be really easy and everyone is able to create a data product with the infrastructure we provide to them. It's important to test your data products. So we need a testing interface. We need to be able to test it. And the last one is that we provide some CRUD operations. So create, read, delete. And I don't know the U part of this, but you know what I mean, right? The four options we always use. 

> Participant 2

> Those are for the data product, right? Not the data itself. 

> Interviewer 

> Interesting. So you think this one should be in the previous framework, for example, the CRUD operations?

> Participant 2

> Because this is more. It's kind of a middle way. Like for I mean, if I look at the platform, then the platform should provide CRUD operations for data products. So a user should be able to create a data product, update a data product, delete a data product. So instead if we are, but a data product is a holistic, you know, architecture of quantum, right? It includes everything you should ask someone to own as much inclusive as possible, right? If instead you are talking about CRUD operations or data, like think about it, any table format that you like, then those are capabilities that are just provided by the table format that you pick. And those can be, those are just implementation quirk of how you generate data at your output ports, not some functionalities that your data product should support from a data mesh perspective, like because you don't want anything but select from if you talk about output ports from a data mesh perspective, like output ports are immutable and are read only. That's what I meant. 

**OPEN CODING TRACE:**

Like for I mean, if I look at the platform, then the platform should provide CRUD operations for data products. So a user should be able to create a data product, update a data product, delete a data product.

**AXIAL CODING TRACE:**
added:
``` python
    
    crud_operations
    
``` 

> Interviewer 

> Okay. Interesting. Because I think these options are really related to the data product developer and not really to platform team. Do you agree with that? Do you think these are really... 

> Participant 2

> Data product developers should be able to perform, like create a data product, update a data product, delete a data product, maybe not the developer, the owner, but then we switch to the roles of the data product team, right? Yeah. So those are actions that operations that a data product developer could carry. There can be like in the testing space, there can be, you know, what do you mean by testing in this case? Because from a platform perspective, if you go to the platform capability of provisioning, then testing could be like test the request that I'm going to perform for deployment and provisioning. But if you talk about software testing, then it's something else and it's something that in my opinion should be decoupled from the platform. Because in complex enterprises, you're going to end up having different domains and business units with different teams, with different practices, even for software development and stuff like that. So really the platform, a data mesh platform should be agnostic from this point of what the software development life cycle, you end up for your business logic, like for your Pyspark job, the platform should shouldn't care a lot of that. What the platform, if you're talking about deploying and provisioning, then the platform should maybe assume that you kind of have your own development life cycle and practice and you provide like the artifact that is the object of deployment operation or provisioning operation. Yeah. In this case, testing has a different flavor. 

**OPEN CODING TRACE:**

 testing could be like test the request that I'm going to perform for deployment and provisioning. But if you talk about software testing, then it's something else and it's something that in my opinion should be decoupled from the platform. Because in complex enterprises, you're going to end up having different domains and business units with different teams, with different practices, even for software development and stuff like that.

**AXIAL CODING TRACE:**
added:
``` python
    
    testing_dp
    
``` 

> Interviewer 

> Yeah. Oh, that's a really good consideration. I wasn't thinking about that way. 

> Participant 2

> And another part of the data product developer experience plane, which includes data product ownership experience plane. A very interesting thing is how, you know, when you don't have, when you decentralized, you don't have the central IT. Yeah. Then there is an issue to be solved in terms of change management and feature requests, which the platform could support. I've seen features like, you know, I've seen users asking, you know, OK, I've understood that we have decentralized ownership. So now I might need another output port for this data product that you are owning. Where should I ask you? Please implement these other output ports because I need it. Or where should I ask you, you know, could you also provide these other data quality metrics on these output ports? Because that's really important to us as consumers to understand that you're granting on your SLO, for example, that, you know, there is this kind of completeness. All right. Got it. So like change management topics from a developer experience plane are kind of a thing that we have seen to become somehow important. But this is something that probably also touches the supervision plane somehow. 

**OPEN CODING TRACE:**

A very interesting thing is how, you know, when you don't have, when you decentralized, you don't have the central IT. Yeah. Then there is an issue to be solved in terms of change management and feature requests, which the platform could support. I've seen features like, you know, I've seen users asking, you know, OK, I've understood that we have decentralized ownership. So now I might need another output port for this data product that you are owning. Where should I ask you? Please implement these other output ports because I need it. Or where should I ask you, you know, could you also provide these other data quality metrics on these output ports? Because that's really important to us as consumers to understand that you're granting on your SLO, for example, that, you know, there is this kind of completeness. All right. Got it. So like change management topics from a developer experience plane are kind of a thing that we have seen to become somehow important. But this is something that probably also touches the supervision plane somehow. 

**AXIAL CODING TRACE:**
added:
``` python
    
    change_management = CClass(practice, 'Change Management')
    
``` 

> Interviewer 

> Yeah. Probably. Yeah. So that's actually the next one. I think you have many comments on this one because, yeah, we have been talking about governance a lot. And you mentioned that governance in your perspective is really related to the self-serve platform, for example, in policy enforcement, I guess. 

> Participant 2

> Absolutely. Yeah. 200 percent. Like you, in my opinion. 

> Interviewer 

> You have extra minutes available, by the way, because we're... 

> Participant 2

> I do. Yeah. Okay. Great. We have two frameworks left and then we can perhaps check this one again and, yeah, provide feedback if this one is complete or that we need other decisions as well. But yeah, we can now focus on the provision. Yeah. Okay. In the last five minutes, I'm going to also show you something if you're curious, but you need to stop the recording. 

> Interviewer 

> Yeah. Oh, okay. Yeah. This is about the supervision, the governance layer, the government's team is responsible for this, things like that. Well, it's actually the self-serve platform team is providing this to the government team. That's a better definition. So the schema registry, the data product catalog, perhaps they can be merged together because they are often really related to each other. Of course, we need an API catalog as well. Manage emergent graphs of DPs. So how are they related to each other or the data products? Of course, security policies, really important policy automation. The knowledge graph, so the semantic layer you were talking about in the beginning. Metadata management, master data management. If we have a central entity, for example, a customer 360, this one needs to be the same in every data product. So there needs to be some master data to refer to. Log management, key blocks of all operations and last one is actually data quality, which I guess speaks for itself. Really important to take that into account as well. But I think you know some more options related. 

> Participant 2

> Many of them are quite, I'm quite confident they belong to the supervision plane. Some of them not so sure. Which are? Like, for example, if you go to the left. Yeah. Like schema registry, for example. You know, the supervision plane is some kind of a centralized layer. A bit. So the idea of a centralized schema registry. Maybe doesn't scale out so well if I think of the centralized and decoupled architecture when where we have like different domains with their own architecture. Which means that you might have several schema registries where consumers, you know, connect to to get the schema of center plane data that they want to consume from that domain. From a governance perspective, you know, the governance team, in my opinion, should define interoperability standards. Like, okay, any consumer might end up, you know, inquiring different schema registries. So you can have the conference schema registry, a custom schema registry, you can have a Darwin schema registry, you can have any schema registry that you want. As long as maybe you provide some standard API, some to get schema from. Yeah. And schema is under the ownership of data product owners. So centralizing them will create a dependency, a central dependency, which is cumbersome to maintain. Because what if you need to change the technology for the schema registry? Like every data product should stop. That's not scalable, right? No, it's not. So anything that should belong to the domains, thus to enable autonomy and scalability of the whole data management practice, then should be decentralized. What would really make sense to centralize on the supervision plane is, of course, the knowledge graph, because that's where really you put things together. It's really important. But still, from a change management and function perspective, like data products can still exist, even if you delete the knowledge graph, right? They can still serve their customers, they can still serve the application relying on their output ports, right? Yeah. So this is a driver that can let you understand, is there a strong coupling of data product lifecycle with what I'm putting at the supervision plane? Because supervision, like if you think of supervisor patterns or observers patterns, is something that is decoupled, so you don't interfere, but you provide added value on top of it to whom you might be addressing your features of your supervision plane. 

**OPEN CODING TRACE:**

Like schema registry, for example. You know, the supervision plane is some kind of a centralized layer. A bit. So the idea of a centralized schema registry. Maybe doesn't scale out so well if I think of the centralized and decoupled architecture when where we have like different domains with their own architecture. Which means that you might have several schema registries where consumers, you know, connect to to get the schema of center plane data that they want to consume from that domain. From a governance perspective, you know, the governance team, in my opinion, should define interoperability standards. Like, okay, any consumer might end up, you know, inquiring different schema registries. So you can have the conference schema registry, a custom schema registry, you can have a Darwin schema registry, you can have any schema registry that you want. As long as maybe you provide some standard API, some to get schema from. Yeah. And schema is under the ownership of data product owners. So centralizing them will create a dependency, a central dependency, which is cumbersome to maintain. Because what if you need to change the technology for the schema registry?

What would really make sense to centralize on the supervision plane is, of course, the knowledge graph, because that's where really you put things together. It's really important. But still, from a change management and function perspective, like data products can still exist, even if you delete the knowledge graph, right? They can still serve their customers, they can still serve the application relying on their output ports, right? Yeah. So this is a driver that can let you understand, is there a strong coupling of data product lifecycle with what I'm putting at the supervision plane?

**AXIAL CODING TRACE:**
added:
``` python
    
    schema_registry
    
    knowledge_graph
    
``` 

> Interviewer 

> Yeah, sure thing. And I think the right part of this framework, that's perhaps a bit more conceptual, like master data management. It's not a component we're talking about, but it's more... 

> Participant 2

> Yeah, that was the other one that I was looking at, like master data management being a practice. I was struggling in understanding how you might end up, you know, including as part of your supervision plane. Of course, there are several of these, many of these, like every of these have some governance decisions, which are architectural decisions that you need for your data mesh implementation. And like the example of the customer 360, which doesn't scale out if you think at a monolithic system, but from a data mesh perspective, but that's my personal opinion, it should be more of a distributed master management, master data management kind of a thing. So you, from a governance perspective, you define what are the global unique identifiers that every domain must include, but you don't strictly require all data related to the customers to be in one single place physically. Yeah, interesting. I agree with that. That we don't make it too monolithic-ish. I would like to write an article on this if you're interested. Oh, yeah. It's a medium, of course, but it's called Customer 360 and Data Mesh Friends or Enemies. So it helps and provides some patterns in how you end up defining and assigning these global unique identifiers, because the problem is then where and when you assign these global unique identifiers, which need to be super immutable. And, you know, in literature, they are called polysemies. And this is a super tricky part, but I don't see so much this as being part of the supervision plane. Yeah, okay. 

**OPEN CODING TRACE:**

Master data management being a practice. I was struggling in understanding how you might end up, you know, including as part of your supervision plane. Of course, there are several of these, many of these, like every of these have some governance decisions, which are architectural decisions that you need for your data mesh implementation. And like the example of the customer 360, which doesn't scale out if you think at a monolithic system, but from a data mesh perspective, but that's my personal opinion, it should be more of a distributed master management, master data management kind of a thing. So you, from a governance perspective, you define what are the global unique identifiers that every domain must include, but you don't strictly require all data related to the customers to be in one single place physically. Yeah, interesting. I agree with that. That we don't make it too monolithic-ish. I would like to write an article on this if you're interested. Oh, yeah. It's a medium, of course, but it's called Customer 360 and Data Mesh Friends or Enemies. So it helps and provides some patterns in how you end up defining and assigning these global unique identifiers, because the problem is then where and when you assign these global unique identifiers, which need to be super immutable. And, you know, in literature, they are called polysemies. And this is a super tricky part, but I don't see so much this as being part of the supervision plane.

**AXIAL CODING TRACE:**
added:
``` python
    
    mdm
    
``` 

> Interviewer 

> And I think you mean the same for the data quality management, perhaps, or the metadata? 

> Participant 2

> Yes and no. Yeah, so from an ownership perspective, like, you know, data product owners should own many of at least the development of the data quality jobs, so the compute part that performs data quality metrics calculation and check against this calculation. But also there can be some kind in the supervision plane, some kind of a broad view of, like, how well our data products are respecting the contracts that they have published. Yeah. And this goes into the distributed observability topic, probably. Yeah. So quality is part of what the observability includes, you know? Yeah. Yeah, okay. So that's more related to the data product, perhaps, and not really to the self-serve platform. But from a supervision plane, you can, I would have said more, you know, track the overall, you know, observability and how well it is going and maybe show, you know, like some kind of a, like, if you think of the data products dependency graph, like, you know, all the data products and domains connected to each other. Yeah. Maybe you can understand from the observability that there are some kind of, you know, problem issues because a data product has stopped functioning. You go into the lineage war here. So maybe on the supervision plane, you see this lineage. Yeah. And it is going, you know, green or red, depending on the observability and the observability can, you know, can include both quality, but also can include, you know, process monitoring. It can include several, several things. 

**OPEN CODING TRACE:**

from an ownership perspective, like, you know, data product owners should own many of at least the development of the data quality jobs, so the compute part that performs data quality metrics calculation and check against this calculation. But also there can be some kind in the supervision plane, some kind of a broad view of, like, how well our data products are respecting the contracts that they have published. Yeah. And this goes into the distributed observability topic, probably. Yeah. So quality is part of what the observability includes, you know? Yeah. Yeah, okay. So that's more related to the data product, perhaps, and not really to the self-serve platform. But from a supervision plane, you can, I would have said more, you know, track the overall, you know, observability and how well it is going and maybe show

if you think of the data products dependency graph, like, you know, all the data products and domains connected to each other. Yeah. Maybe you can understand from the observability that there are some kind of, you know, problem issues because a data product has stopped functioning. You go into the lineage war here. So maybe on the supervision plane, you see this lineage. Yeah. And it is going, you know, green or red, depending on the observability and the observability can, you know, can include both quality, but also can include, you know, process monitoring.

**AXIAL CODING TRACE:**
added:
``` python
    
    data_quality_management
    
    manage_emergent_graphs_of_dps
    
``` 

> Interviewer 

> Yeah. Yeah. Gotcha. So perhaps we can go to the last framework. So it isn't about the UI. Yeah. So this is more like, how do we make even ourselves a platform accessible to, to the general lists. So, of course we need a cataloging function. So the data catalog that needs to be a function for that needs to be easily accessible for the general list. We can implement a visualization function. So for example, a building pipeline function so that they only need to select certain icons. It's like no code development. It should be made really easily. The data governance function. So it enables data security. And for example, collaboration, an application built, but build function. That's of course an option. It's not mandatory. Yeah. If we need to build a software product, a SaaS product, then this can be really valuable. Data integration function. So the data product developer needs to be able to import data sets or export data sets through a function. And we can have a query recommendation function. This is more on the high level perspective. What, yeah, what generalists prefer to have in a self serve platform but perhaps there can be more options as well. Or some options are redundant. 

> Participant 2

> What I'm thinking of is... So these could be capabilities that the users of a data mesh, super broad thinking like consumers, data analysts, whoever is working with data in the analytical plane might need. I think you have done this kind of mapping, but from a data mesh enabling platform, I'm not sure that all these capabilities should be embedded into this platform. And the reason is that, as I mentioned earlier, anything that you're putting into these centralized platform is creating a centralized dependency for everyone. So what should be there? Thinking in a super generalistic and framework driven way, but from a framework perspective, anything that would impose constraints and somehow couple the change management. So the business of every single domain together, then probably should not be so tightly coupled. I don't know if you get what I mean. Like, for example, visualization function, if we are thinking about visualization of data. Thinking of a platform that includes, just to make a stupid example, that embeds both Power BI and Tableau functionalities, that will create a coupling really unnecessary. 

**OPEN CODING TRACE:**

for example, visualization function, if we are thinking about visualization of data. Thinking of a platform that includes, just to make a stupid example, that embeds both Power BI and Tableau functionalities, that will create a coupling really unnecessary. 

**AXIAL CODING TRACE:**
added:
``` python
    
    visualization_function
        
``` 

> Interviewer 

> Yeah, you're tightly coupled to that specific vendor. 

> Participant 2

> Yeah, I don't get the why in this sense. Like if you then some other business analyst or your CEO decides that she wants to analyze and perform a query from some kind of a client or UI that she likes. What you need is that that client is interoperable with the standards that you have set up among your data products. That's really the ultimate goal, in my opinion, you should achieve. Because otherwise, you are too much relying on one single technology and one single vendor, which is very risky from a management perspective. It's not agnostic as well, which is really good. Yeah, we struggle with the fact that the platform should be technology agnostic as much as possible. Yeah. 

> Interviewer 

> All right. Yeah, these were actually all the frameworks. I have only two questions left in terms of time as well. Yeah, so the objective of this framework is to provide you with some guidance during your self-serve platform development in this particular case. Do you think this framework can help, well can support users in their journey? Do you think it can be useful? Or would you use it yourself, for example? 

> Participant 2

> Yeah, I was thinking about this. So what I see happening when you have to start designing a system, depending on the context you're starting from, you either start with use cases. So you start simple, you take a couple of use cases, you map them, and then you extract from them the requirements of your design. Or if you're going to have some more greenfield kind of a project ahead of you, then you can start with the framework that brings you to what the best practice says that you might need. So in that case, it could help. There are not so many greenfield scenarios out there. Maybe for companies that are starting out their journey or their data journey, like data driven companies that start with a data driven mindset, not a digital mindset. Yeah, you take the business aspect into account as well. Maybe those can benefit. Of course, there are several items here that are requirements. Eventually, you're going to need to define what are your retention policies for your data, and how do you implement them, and how you verify them. What is your practice and technology for security and access management, and how do you assign ownership for access control, and how do you audit access control. So, there are a lot of decisions that needs to be taken when designing a data architecture, of course. So these mapping kind of help out in sorting them out. 

**OPEN CODING TRACE:**

Of course, there are several items here that are requirements. Eventually, you're going to need to define what are your retention policies for your data, and how do you implement them, and how you verify them. What is your practice and technology for security and access management, and how do you assign ownership for access control, and how do you audit access control. So, there are a lot of decisions that needs to be taken when designing a data architecture, of course. So these mapping kind of help out in sorting them out.

> Interviewer 

> Do you think it's complex to use this architecture or is it really understandable for as many practitioners as possible. So if you, well, I still have to provide some context, of course, in my thesis around these decisions. But do you think it's understandable for everyone? Easy to use? 

> Participant 2

> So the difference between, so the gap that stands between a framework and its execution is some kind of a roadmap, like execution roadmap. Because here you have the whole bunch of things that you need to put in place. But if then you need to start, the first thing is, okay, where do we start from and why? And what goals do we set? And how do we measure our advancement, our progress in this journey? 

**OPEN CODING TRACE:**

So the difference between, so the gap that stands between a framework and its execution is some kind of a roadmap, like execution roadmap. Because here you have the whole bunch of things that you need to put in place. But if then you need to start, the first thing is, okay, where do we start from and why? And what goals do we set? And how do we measure our advancement, our progress in this journey? 


> Interviewer 

> Yeah, I agree. All right, thank you. This was actually all I wanted to discuss for the self-serve platform. Thank you so much. This is really valuable and I will implement the feedback as well. We'll keep you up to date as well about all the changes. Yeah, so I will stop sharing. Okay. You wanted to show me something?

