# s 
12
## url
https://www2.deloitte.com/content/dam/Deloitte/be/Documents/technology/consulting_untangle_your_mess_and_knit_your_mesh_deloitte_be_report_en.pdf
## tiny url
https://tinyurl.com/self-serve-platform-s12
## archive url
https://bit.ly/self-serve-platform-s12
## title
Untangle your mess and knit your mesh: A cross-company point of view
## source code
no
## example
yes
## source type 
Practitioner Audience Article
## author type
Practitioner
## references

**AXIAL CODING TRACE:**
``` python
s12 = CClass(source, "s12", values={
    "title": "Untangle your mess and knit your mesh: A cross-company point of view",
    "url": "https://www2.deloitte.com/content/dam/Deloitte/be/Documents/technology/consulting_untangle_your_mess_and_knit_your_mesh_deloitte_be_report_en.pdf",
    "archive url": "https://bit.ly/self-serve-platform-s12",
    "tiny url": "https://tinyurl.com/self-serve-platform-s12",
    "author type": "Practitioner",
    "type": "Practitioner Audience Article",
    "example": True,
    "source code": False})
```

# coding

> Your organisation needs to consider a self-service infrastructure as a platform

> “A self-serve data platform must create tooling that supports a
domain data product developer’s workflow of creating,
maintaining and running data products with less specialised
knowledge …” 1

> One of the key principles of data mesh is the self-serve infrastructure as a platform. The
capabilities described by Zhamak and needed for a self-service infrastructure as a platform
are based on ease of use and a common toolset for the domain teams. In a data mesh set-up,
niche technologies will not be sustainable due to the specific skill set as well as the additional
work needed to follow the DATSIS principles (Figure 2). Snowflake’s Data Cloud provides an
extensible foundation for domain teams to support a governed and decentralised data mesh
architecture. It enables domains to go beyond creating and sharing data as products, but also
processing logic as products.

> A decentralisation accelerator

> It is critical that domain teams can access the resources and tools they need on demand to
support them at every stage of the data product lifecycle—from accessing the right data, to
processing and preparing it, to analysing, creating models and serving data products to other
domains. 

> The platform needs to provide an elastic performance engine, so domain teams can power
large-scale pipelines, ad hoc exploration, BI reporting, feature engineering, interactive
applications, and much more, all with a single engine. This allows organisations to simplify
their architectures without sacrificing speed or usability. Regardless of whether the teams
work in SQL, code (such as Java, Scala, or Python), or a mix, the performance engine supports
them all the same. Furthermore, with elastic scalability and isolated multi-cluster compute,
each domain team gets access to the dedicated resources they need without impacting
performance or concurrency for other teams. This can accelerate the transition toward a
decentralised but governed architecture.

**OPEN CODING TRACE:**

Elastic Performance Engine

Empowers Large scale pipelines

Ad hoc exploration

BI reporting

application_build_function

Feature Engineering

Interactive Applications

Allows to simplify architecture without impacting performance of concurrency for other teams

Data Source Ingestion

**AXIAL CODING TRACE:**
added:
``` python

    elastic_performance_engine
    scalability
    ad_hoc_exploration = CClass(force, 'Ad hoc exploration')
    bi_reporting = CClass(force, 'BI reporting')
    feature_engineering = CClass(force, 'Feature Engineering')
    interactive_applications = CClass(force, 'Interactive Applications')
    understandability = CClass(force, 'Understandability')
    data_source_ingestion
    unified_batch_stream_processing_service = CClass(pattern, 'Unified Batch Stream Processing Service')
    application_build_function
    
```

> With the explosion of variety, volume and velocity of data, domain teams will need a platform
capable of ingesting vast amounts of data in very different formats. Data may come from
different sources and can be consumed from other domains as products. The platform should
be open in order to consume data from other domains at the same time as providing data to
other domains, avoiding any vendor-locking. Being open means being able to interact with
the ecosystem and ingest and expose standard formats and should not be confused with
open-source technologies.

> Data products as architectural quantums

> Domains can easily design new products with the given platform capabilities to create a
product pipeline for ingesting, serving and processing data products. The following diagram
represents how new data products can be created from existing data domains. In the
example, a new ‘design to cost’ data product can be created using the data provided by Parts,
Finance, Supplier, and Bill of Materials domains. Using Snowflake Data Exchange or a third-party data catalog such as Collibra, those data domains can advertise and expose their data
products that will be immediately available to be consumed and processed to provide new
data products.

> Connecting organisations and data teams to the most relevant data when they need it, without silos
or complexity, is what the Snowflake Data Cloud is designed to do.
To deliver on that, the Snowflake Data Cloud is backed by Snowflake’s platform, which is
uniquely built for performance at scale, ease of use, and governed data sharing and
collaboration; and it’s well suited to support both the centralised standards and the
decentralised ownership necessary for a successful data mesh deployment.
Snowflake’s platform gives these teams a one-stop shop, while supporting a wide range of
skills. 

> High level implementation

> Implementation of data mesh in Snowflake can be based on different topologies:

> Domains can be based on accounts and leverage Snowflake’s Secure Data Sharing
capabilities to break down silos, cross region and cross cloud and work on a single copy
of the data.

> Domains could also be based on database or schema and leverage a third-party catalog
such as Collibra to make products discoverable and accessible.

**OPEN CODING TRACE:**

Data Catalog

schema_registry

Discoverable

Accessible

costs

complexity

**AXIAL CODING TRACE:**
added:
``` python
    schema_registry
    data_catalog
    discoverability
    accessibility
    costs
    complexity
```

> In any case, Snowflake can allocate independent resources for the domains to load, process
and prepare their data products leveraging the Snowflake virtual warehouses and then list
the data products on a third-party catalog. Consumer oriented domains can then use these
products through data sharing or intra account cross database access depending on the
selected topology.

> Implementation considerations

> Not choosing the right tools and infrastructure can limit the benefits of a data mesh, adding
extra complexity and increasing the time to value and cost. Zhamak proposes a data
infrastructure provisioning plane that only advanced data product developers use directly as
it is low level infrastructure lifecycle management. SaaS platforms like Snowflake remove that
complexity and dependency of low-level expertise. Snowflake resources deployment and
management can be completely automated with infrastructure as code in a self-serve
manner with highest level of security and governance and across any public cloud.
The next level is being able to abstract the complexities of data workflows. By using all
Snowflake capabilities, data workflows can be automated using ‘data as a product’
approaches and integrating with different tools.
A data mesh architecture should also be complemented with other tools such as ingestion
and automation, machine learning, and other adjacent technologies.

**OPEN CODING TRACE:**

data_governance_function
infrastructure as code
declaratively_create_dp
automation
time_to_value
other_platforms
data_transformation_function

**AXIAL CODING TRACE:**
added:
``` python
    
    data_governance_function
    infrastructure_as_code = CClass(practice, 'Infrastructure as Code')
    automation = CClass(force, 'Automation')
    declaratively_create_dp
    time_to_value = CClass(force'Time-to-Value')
    other_platforms
    data_transformation_function
    data_source_ingestion
    
```











