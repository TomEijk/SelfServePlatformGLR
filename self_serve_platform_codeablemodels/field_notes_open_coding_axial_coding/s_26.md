# s 
26
## url
https://www.mo4tech.com/how-does-the-banking-industry-build-a-self-service-data-platform.html
## tiny url
https://tinyurl.com/self-serve-platform-s26
## archive url
https://bit.ly/self-serve-platform-s26
## title
How we scale our data platform efficiently and reliably
## source code
no
## example
yes
## source type 
Practitioner Audience Article
## author type
Practitioner
## references

**AXIAL CODING TRACE:**
``` python
s26 = CClass(source, "s26", values={
    "title": "How does the banking industry build a self-service data platform?",
    "url": "https://www.mo4tech.com/how-does-the-banking-industry-build-a-self-service-data-platform.html",
    "archive url": "https://bit.ly/self-serve-platform-s26",
    "tiny url": "https://tinyurl.com/self-serve-platform-s26",
    "author type": "Practitioner",
    "type": "Practitioner Audience Article",
    "example": True,
    "source code": False})
```

# coding

> Meet Data Hub: Delivery Heroâ€™s Data Mesh Platform

> Where it all began
Delivery Hero is a listed company with over 45,000 employees in more than 70+ countries, with several thousand employees in IT, and hundreds in data.
The data teams at Delivery Hero are scattered across different regions, countries and companies. At the beginning of 2020, teams used different technologies and several cloud providers and did not share a common infrastructure. The high-quality standards were missing, and as a result, the data quality was often poor. The question of who owns specific data was not easy to answer. The communication between the teams, as well as the exchange of data, was hampered. The proper access management process was neglected, security was solved differently from team to team, data modeling was not standardized and varied from team to team.
The architecture was very chaotic and resembled the image below:

![web](https://techdhstaging.wpengine.com/wp-content/uploads/2022/09/Architecture-before-Data-Hub-800x619.jpg)

> To combine data, perform analysis, and create reports, engineers moved files between different teams, different infrastructures, and cloud providers.
The main issues were data availability, data ownership, access management, data quality, security, GDPR, etc.

> Taking action

> To begin with, we had to decide what our first priority would be. After brainstorming with multiple data team leads, three primary key points were identified.

![Actions](https://techdhstaging.wpengine.com/wp-content/uploads/2022/09/Data-top-3-challenges-800x202.jpg)

> The following three priorities were then chosen as top priorities for the first year:

> Scalability of data infrastructure.
Data protection from unauthorized access.
Sharing of data access, and a way to combine data across functions.

> Scalability of data infrastructure.
Data protection from unauthorized access.
Sharing of data access, and a way to combine data across functions.

> Data Mesh helps!

> Data mesh is a concept that is particularly aimed at large organizations such as Delivery Hero with multiple domain data units. Data Mesh is not a technology and does not describe how to solve a specific problem.
In a nutshell, Data Mesh describes how to organize a team and accountability, in order to build a data-driven organization and create a paradigm shift from centralized ownership to decentralized ownership, from a monolithic approach to a distributed approach, from data as a by-product to data as a product, from the pipeline as a first-class concern to the domain as a first-class concern.
As the basis for this transition, Data Mesh recommends having a data infrastructure as a platform.

![4principles](https://techdhstaging.wpengine.com/wp-content/uploads/2022/09/architecture-1.jpg)

> This platform should provide support for multiple Domain Data teams within an organization. Now letâ€™s look at how this platform was built at Delivery Hero.

> Data Hub is a unified analytics platform at Delivery Hero

**OPEN CODING TRACE:**

unified_batch_stream_processing_service

**AXIAL CODING TRACE:**
added:
``` python
    
    unified_batch_stream_processing_service

```

> Finding scalable and future-proof solutions with the ability to easily exchange data without having to physically move data between different cloud providers or teams was not easy. The decision-making board goes through several interactions and comparisons about existing internal solutions. Some of the teams used solutions based on Redshift+Airflow/Jenkins, others on BigQuery+Airflow.

> BigQuery

> After months of comparing the advantages and disadvantages of both big data engines, the decision was made in favor of BigQuery. At this point, half of Delivery Heroâ€™s data teams were already using BigQuery. The biggest benefit of using BigQuery was the ability to use a SQL JOIN between different projects, without having to move data physically. Considering the enormous amount of data that Delivery Hero has already in different projects, this argument was disproportionately strong.

> Status Quo

> Next, we considered which team already using BigQuery has more technically oriented solutions. The decision finally fell on the solution developed by Logistics BI, one of our tribes. This project was the starting point. Massive thanks to this team â€“ they inspired this whole project!
None of the solutions fully ran on GCP. Logistics also used a heterogeneous setup of  BigQuery on GCP, and a Kubernetes cluster on AWS. As the main idea was to unify all data teams as soon as possible, all teams started moving their data to BigQuery in the central project. As this infrastructure was originally developed for one team, we soon encountered limitations and started with the platform review.

> Challenges

> The biggest challenge was making this platform scalable for multiple teams, while also considering whether Kubernetes clusters on AWS still make sense when the data engine for this project is going to be BigQuery. After the proof of concept, the decision was made to rebuild the entire infrastructure on GCP, with a focus on scalability. 
The idea was to provide a powerful environment, with the possibility to manage and auto-scale Kubernetes clusters, to deploy multiple dedicated Airflow environments (prod, staging, development), with the possibility to use airflow plugins and extensions. It also needed to be protected with the highest security standards â€“ and last but not least, beat GCP Composer! ðŸ™‚
The GCP project structure made it possible. Each data team, which we call Domain Data Unit corresponding to Data Mesh terminology, is provided with a dedicated GCP project with all the necessary components such as BigQuery, VPC, Kubernetes Cluster, CloudSQL, Load Balancer, Secret Manager, Monitoring, etcâ€¦ 

> Architecture

> After two months of initial development with the newly established Data Hub team, along with the help of a Google partner consulting firm, we built initial infrastructure on GCP and began migrating multiple teams from AWS to GCP. From the beginning, this project was seen as an internal open source project, to which all data entities can contribute. Many decisions were made in a very open atmosphere, and after long, sometimes highly debated discussions. The focus was on building a platform that covers all of our use cases. As a result, the following platform was created:

![Architecture](https://techdhstaging.wpengine.com/wp-content/uploads/2022/09/GCP-Datahub-Architecture-Infra-800x571.jpg)

> The entire management of the projects comes from infrastructure projects like control and storage, which includes GKE, VPC, Container Registry, Secret Management, etc., and projects for development and staging. On the domain data units side, there are dedicated projects, one per team, where Airflow environments get provisioned by the Kubernetes cluster. The domain data unit team (DDU) gets fully installed and configured in production, staging, and multiple development environments.
This allows teams to work independently, and also allows team members to use a remote development environment without having to install it locally. All Airflow URLs are protected with SSO. Deploying a new DDU currently takes less than two hours. To manage access requests, we use a Jira-based service desk with two permission levels for regular and PII data access. 
The PII columns are protected in BigQuery with the column policy tag. The data is accessible via Google Groups and is managed via an ACL file. As this manual process still takes a lot of time, the Data Hub team is already working on a fully automated solution to reduce the time it takes team members to access data. One of the possible solutions currently being discussed is Sailpoint, in combination with Google Roles/Groups.

**OPEN CODING TRACE:**

access_control

container_registry

distributed_file_storage

separating_storage_and_compute

VMs

data_transformation_orchestration

workflow_automation_engine

application_build_function

accessibility

automation

**AXIAL CODING TRACE:**
added:
``` python
    
    access_control
    container_registry = CClass(force, 'Container Registry')
    distributed_file_storage
    separating_storage_and_compute
    VMs
    data_transformation_orchestration
    workflow_automation_engine
    application_build_function
    accessibility
    automation
    
```










