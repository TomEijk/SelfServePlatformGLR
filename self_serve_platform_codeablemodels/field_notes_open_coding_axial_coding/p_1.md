# p
1
## url
https://github.com/TomEijk/Data_as_a_Product_GLR/tree/master/python/self-serve-platform_codeablemodels/field_notes_open_coding_axial_coding
## tiny url
https://tinyurl.com/self-serve-platform-p1
## archive url
https://bit.ly/self-serve-platform-p1
## title
Interview Expert 1
## source code
no
## example
yes
## source type 
Interview
## author type
Practitioner
## references

**AXIAL CODING TRACE:**
``` python
p1 = CClass(source, "p1", values={
    "title": "Interview Expert 1",
    "url": "https://github.com/TomEijk/Data_as_a_Product_GLR/tree/master/python/self-serve-platform_codeablemodels/field_notes_open_coding_axial_coding",
    "archive url": "https://bit.ly/self-serve-platform-p1",
    "tiny url": "https://tinyurl.com/self-serve-platform-p1",
    "author type": "Practitioner",
    "type": "Interview",
    "example": True,
    "source code": False})
```

# coding

> Interviewer

> Great. Yeah, now it's recording. Okay, let me share my screen right away. I've found a lot about the... You cannot minimize zoom when you are recording the meeting. Okay. Can you see my screen? 

> Participant 1

> No, I can't. 

> Interviewer

> You can only see the zoom screen or what do you mean? 

> Participant 1

> Yeah, I can only see that. 

> Interviewer

> Oh, wait a sec. And now? Yes. Great. It's super big. Yeah, I have a white screen over you, so I guess that's the problem. I can zoom in a little bit. 

> Participant 1

> Oh, that's perfect. Yeah. Yeah, this is better. Okay. No, the one previous was just better, but that's great. Yeah. Okay, great. If it's a little bit slow, then it's perfect. 

> Interviewer

> Yeah. It's like the same as the previous time. I discovered some decisions we need to take into account while implementing a self-serve platform, while actually realizing a self-serve platform. The first one is more high level, so how to align a self-serve platform and its components with data mesh. And I will show you in a bit what that means. Then we have the three famous planes according to Zhamak Dehghani, so the infrastructure provisioning plane, the experience plane, and the supervision plane. And on the bot> Interviewer, we have a last decision, and that's more about the self-serve UI. So what essential elements do we need to make the self-serve UI as convenient as possible? So if we check the first one, this is actually based on an article of Piethein from Microsoft, and he discovered an extinction, a distinction between the way you implement a self-serve platform. So there can be a single data landing zone, a single self-serve platform. We can align it with the source system and consumer aligned system. So for example, we have a self-serve platform specialized on all the incoming data from the operational databases, but we have a separate one for the consumer aligned data products where they get their infrastructure provisioning from. So that we actually have two different types of self-serve platforms. There can be a hub generic and special data landing zones. I will quickly grab the article over here. So that's actually stated over here. So there can be a distribution hub. We can align it with main sub-cities diaries, things like that, a special data landing zone for a special data product perhaps. So it can be specified on region things. Yeah, there are many ways we can start thinking about how to implement it on a high level, if you know what I mean. Do you recognize some of these patterns or would you say that we just need one single self-serve platform? What's your opinion on that? 

> Participant 1

> When it comes to the user experience, okay, basically which is driving your platform, I am not looking at the data anymore. I am looking at the personas. Okay, so who are you talking to in your organization? And I think instead of having this data to tool approach or data to platform approach, I would have rather a persona to tool approach. I'm a business analyst, okay. What's my mission? What do I need to do with the data? I'm a data scientist. I'm a data mesh platform developer. I'm a data engineer that wants to build data products. Okay, so basically I would identify more towards the personas you're going to serve and based on those personas, then I would try to identify what tools they would need and what their operation would be on the platform itself. Then of course, when you're looking at the segmentation of data the way you did it here, this talks to a different persona. It talks to the data engineer. It talks to the data product owner and maybe the consumer, but not even that sure because you're trying to abstract that from them. Okay, so I think that this is going to be parameters of the or fields or tabs or screens of your UX. Okay, like of fields, you know, like hey, where do you want your data to land or where is your data coming from and it will be able to, you will identify this kind of lineage eventually or things like that. But I would not take the approach of focusing on the data for the platform rather focusing on the personas. That would be my good process there. 

**OPEN CODING TRACE:**

And I think instead of having this data to tool approach or data to platform approach, I would have rather a persona to tool approach. I'm a business analyst, okay. What's my mission? What do I need to do with the data? I'm a data scientist. I'm a data mesh platform developer. I'm a data engineer that wants to build data products. Okay, so basically I would identify more towards the personas you're going to serve and based on those personas, then I would try to identify what tools they would need and what their operation would be on the platform itself. Then of course, when you're looking at the segmentation of data the way you did it here, this talks to a different persona. It talks to the data engineer. It talks to the data product owner and maybe the consumer, but not even that sure because you're trying to abstract that from them.

**AXIAL CODING TRACE:**
added:
``` python
    persona_to_tool_approach = CClass(practice, 'Persona to Tool approach')    
``` 

> Interviewer

> Yeah, I got it. So it's more about thinking of the end user instead of thinking about the system itself. So here for example, they are thinking about the region where the elsewhere platform is. 

> Participant 1

> Yeah, but that's not where you take the end user into account, I guess. No, I mean you can guide the user on that. Okay, so for example, I'm not too much involved with platforms. Okay, so because I mean not platform, but regions, I'm in three regions. So this is something which is really as almost consumers or data engineering level. Okay, so where's my data? Where am I moving it? Am I even allowed to move data from Europe to the US for example, or from some countries to the US? Okay, so all that is really valid culture, I would say. But once more, it's going to be based on the personas, right? No. I'm a data scientist. I don't care where my data is. Okay, I just want to be able to, let's say, find roadsters. Okay, so the algorithm I'm going to run, I'm going to run them wherever I'm going to run them. Okay, so it could be on European data, it could be on US data. What I'm interested in is roadsters. And therefore, in my job requirement, whether I'm in Europe or not, I don't really care because the guy could be pretending he's in Europe, but he's actually in Russia or in China. So once more, it's focusing on the users, the users, the users, the users. Once more, it's focusing on the users of your data. But your segmentation is good. Okay, don't get me wrong. It's more like, I think it's not applicable, at least to my perspective, okay, I don't need to be wrong, okay, but when I'm seeing that, it's not applicable to the self service part purely. Where is my data? How is my data going to be consumed? So it's almost more, yeah, when you're thinking about the different paradigm in data mesh, you have, you know, you've got these four principles. They are, of course, they are mesh together as well. But I would say this is almost between self service and governance. Because you will have, when you're thinking about where all your data is, this is really governance, okay, but all you consume is self service, or you consume the mesh is self service. Yeah, so you're in between there. 

**OPEN CODING TRACE:**

I'm in three regions. So this is something which is really as almost consumers or data engineering level. Okay, so where's my data? Where am I moving it? Am I even allowed to move data from Europe to the US for example, or from some countries to the US? Okay, so all that is really valid culture, I would say. But once more, it's going to be based on the personas, right? No. I'm a data scientist. I don't care where my data is. Okay, I just want to be able to, let's say, find roadsters. Okay, so the algorithm I'm going to run, I'm going to run them wherever I'm going to run them. Okay, so it could be on European data, it could be on US data. What I'm interested in is roadsters. And therefore, in my job requirement, whether I'm in Europe or not, I don't really care because the guy could be pretending he's in Europe, but he's actually in Russia or in China. So once more, it's focusing on the users, the users, the users, the users. Once more, it's focusing on the users of your data.

version_dp

**AXIAL CODING TRACE:**
added:
``` python
    functional_and_regionally_aligned_data_landing_zones  
    
    version_dp
``` 

> Interviewer

> Yeah, this is really the gray area kind of stuff. 

> Participant 1

> Yeah, so I think you did a good job at defining the different patterns, okay. Yeah, that's definitely true. Like you've got landing zone, you've got regionally and functional zones. If you're thinking about, usually the industry tries to focus on, at least logically speaking, three zones, okay, so which is landing zone. Or if you read anything from Databricks, they call that bronze, okay. And then you've got silver for them, which is kind of a refined zone. And then you've got the gold zone. But I would say these are kind of logical zones, then you can have specific areas where your data is physically located, okay. So you could have a European gold zone, for example. Or you could have an APAC silver zone, okay. And what could be also interesting, what is after that? Probably not in just a group of your studies there, but what could be interesting is how do you aggregate data across the zone as well, okay. So now data mesh can simplify eventually that kind of stuff, based once more on federated governance. 

> Interviewer

> Yeah, so perhaps we can quickly go to the other framework, because this is more in terms of infrastructure. How do I make it? That's good. Probably too big, but we can go from left to right. 

> Participant 1

> So this is about the infrastructure provisioning aspect. So what you need to provide to the data, product developers, et cetera. So that's great. That's kind of the services you want to be able to provide. Typically when you're thinking persona again, it's going to be the data product builder engineer, okay. So that is going to build these data products and it needs to know all these things. 

> Interviewer

> Yeah. So for example, on the left, we have the workflow au> Intervieweration engine that can be Apache Airflow, for example. Processing service, of course. We have networking. So how do VMs communicate? We have VMs themselves. Data source ingestion. So it's more like what's happening up front. Data transformation orchestration, managed compute infrastructure. And what I mean with that is your EC2 instance, or for example, we have Snowflake and they separate storage and compute. 

> Participant 1

> Before that, you may need also all this data provisioning as well. When you've got your VMs, for example, on networking, but you've got the data transformation, you've got data source ingestion, but you've got to provision this thing. So provisioned data structure creates a space for that. You may also have a set of tools for, okay, you're ingesting the data, and then you're orchestrating. But how do you, in what stage here do you create your model? How you deploy your... Not an ML model, not an AI model, but your data model. Where at some point I think you need to create- 

> Interviewer

> I think that's more the second plane over here. So the data product experience, data product development plane. So this is how Zhamak made a distinction between three different planes and they all build up to the platform. And I think this is more in terms of infrastructure. So what do we need around our data products? 

> Participant 1

> Correct. You're correct. You still need the data provisionings, however, okay. Like creating the databases and things like that. Because that's infrastructure. But you're correct. The modeling would go at the data product level. The data transformation orchestration, if we're thinking like that, could also be at the data product level. Because your data transformation is specific to a data product. It's not infrastructure. The tooling, let's say you use Glue, for example. Okay. So the tooling is here at the infrastructure level. The operationalization of it, or the orchestration of it, would be at the data product. Okay. So you would place this option in the other framework over here. Yeah. But I would still have something like the tooling itself. Okay. So I don't define the tooling. Okay. So, yeah. Yeah. 

**OPEN CODING TRACE:**

The data transformation orchestration, if we're thinking like that, could also be at the data product level. Because your data transformation is specific to a data product. It's not infrastructure.

**AXIAL CODING TRACE:**
added:
``` python
    data_transformation_orchestration  
``` 

> Interviewer

> Because if we quickly have a look at this one, then we can compare the both options, well, both frameworks. This is more about, yeah, there needs to be some build, deploy, monitor data product provision function. We need to version, read, of course. So security. Testing. Yeah. 

> Participant 1

> So security is, yeah. Okay. I would have something about the data contract here. The data contract, the SLAs and the SLOs. Yeah. So it's basically, I would, the way you have your documents is a data product. Okay. So for me, the documentation of the data product is part of the data construct. So if I would do something like you did, I would replace document by build data contract or establish data contract and document would be part of this steps there, as well as establishing SLAs and also stuff you put in the data. 

**OPEN CODING TRACE:**

testing_dp

I would have something about the data contract here. The data contract, the SLAs and the SLOs. Yeah. So it's basically, I would, the way you have your documents is a data product. Okay. So for me, the documentation of the data product is part of the data construct. So if I would do something like you did, I would replace document by build data contract or establish data contract and document would be part of this steps there, as well as establishing SLAs and also stuff you put in the data.

**AXIAL CODING TRACE:**
added:
``` python
    
    testing_dp
    
    document_dp  
    
    contracts = CClass(practice, 'Service Level Agreements')
``` 

> Interviewer

> Yeah. It would be like a child note of document DP. Oh, that's a good one. I can extend it. Yeah. And yeah, declarative, declaratively create DP is like, yeah, making it as easy as possible to develop your data product. So develop an easy sales, so a PY, this is a bit connected to it. Other framework. 

> Participant 1

> Yeah. Okay. That makes sense. Okay. Yeah.

> Interviewer

> This is almost the last one because we're already almost out of time. Yeah. This is like the data management plane, the revision plane. So yeah, of course we need an alerting function, a schema, three important catalog, manage emergent graphs, manage security policies, of course, policy management, central data product catalog, knowledge graph, which can take care of the semantics, of course, log management, master data management, metadata management, and data quality management. So that's actually all the options we need to think about when implementing a self-serve platform. Yeah. Do we miss something here or? 

> Participant 1

> I don't think you miss, but I think you can group some things together. For example, I would, I would try to combine the emergent graph plus catalog plus knowledge graph maybe together, and maybe even the metadata management, because they all kind of link together into something there. Yes. And make child notes of data product catalog and knowledge graph under the meta data. Yeah. Yeah. Then maybe you can have something more about the governance part, okay, where you have the policies together and maybe in the end. Okay. So once more, if you start thinking about personas, okay, in the enterprise, and that will probably vary from, I would say, user company to user company. But if you're thinking about, okay, at this level, what do you have? You've got users of the data mesh. They want to access the data. You've got ops people that are in charge of making sure that this doesn't go wacko, okay. And then you've got data governance people. You may have also people. So if you fake persona display as well, you can maybe, maybe they're still separate silos or simple branches on your tree there, but you can group them at least logically by function. So here it is more about grouping and not like that we missed something or that we... Yeah, because the thing was maybe, maybe, maybe, yeah, it depends. As you're going to drill a little bit in things like what's the difference between a schema registry and a data product catalog. 

**OPEN CODING TRACE:**

For example, I would, I would try to combine the emergent graph plus catalog plus knowledge graph maybe together, and maybe even the metadata management, because they all kind of link together into something there.

You've got users of the data mesh. They want to access the data. You've got ops people that are in charge of making sure that this doesn't go wacko, okay. And then you've got data governance people. You may have also people. So if you fake persona display as well, you can maybe, maybe they're still separate silos or simple branches on your tree there, but you can group them at least logically by function.

Yeah, because the thing was maybe, maybe, maybe, yeah, it depends. As you're going to drill a little bit in things like what's the difference between a schema registry and a data product catalog. 

schema_registry

**AXIAL CODING TRACE:**
added:
``` python
    manage_emergent_graphs_of_dps
    
    persona_to_tool_approach
    
    data_catalog
    
    schema_registry
      
``` 

> Interviewer

> A schema registry and a data product catalog. Yeah. Yeah, it can be the same indeed, but I think you can put the API catalog in there as well, perhaps. 

> Participant 1

> Yeah, but who's consuming the APIs? Is your data scientist going to consume the APIs? Probably not. 

> Interviewer

> Yeah, so you would include the, well, you would merge the schema registry with the data product catalog over here. 

> Participant 1

> Yeah. Yeah. And maybe it's interesting in the other thing. Okay. So yes, ops is interested in it. The users are interested in it. Governance, I don't really care if it's a thing that's coming in the waffle. So maybe you can, I'm thinking out now with you, okay. Maybe it's also kind of almost like a matrix. You've got different capabilities and you've got different users and you push cross where people want that. 

> Interviewer

> Yeah. Hmm. Got it. Okay. Yeah. I can definitely do something with that. Then we can quickly go to the last one where we have to. 

> Participant 1

> Okay. If I'm one minute late, it's fine. 

> Interviewer

> Yeah. So this is the self-serve UI. So how do we make our self-serve platform accessible for the generalist, more like the T-shaped person in Zhamak’s book. Yeah. We're not really experts in something, but they know a bit about everything. So we need a cataloging function. This is, yeah. In line with the catalog, a visualization function. So then they are able to visualize the data, data transformation function, data governance function, actual application build function, integration. So how do we add data sets to our architecture and a query recommendation function? I think we can extend this a bit. You probably have some other options as well. 

> Participant 1

> No, I think I don't even have all of those yet. For example, the two last one I don't know, I'm going to do integration and query. But this is going back to your persona thinking. Probably a little bit blindsided today, but it's really like if you're thinking, okay, what am I talking to? Am I talking to the data engineer, which is actually building the internal data pipeline? So yeah, it's going to talk to the data transformation. If I'm talking to my business analyst, what does he want? He wants to talk to the visual vision function and maybe the application build. So once more, maybe a matrix or just this, but adding a tag to it, saying, hey, this is for this guy, this guy, this guy. 

> Interviewer

> Yeah, because these are all options. You don't have to implement them all. 

> Participant 1

> It depends on your maturity model. I was just talking with someone before; do you have a maturity model for your data mesh? So maybe the last level of your data mesh in the maturity model has all of that, because all that makes sense. But in the priorities, you may say, well, okay, I'm just talking to my data engineers. So, their information is just enough. Yeah, because maybe they don't need to buy a new application over here and then this would be redundant, this function. Yeah. Or then they don't have to buy it, but they can integrate and then you only have the integration layer. So for example, for cataloging, that's a very common use case. Do you need integration or do you need to rebuild the data kind of all? Okay. Yeah. So I got to run, sorry about that. Thank you so much. You're very welcome. It's always a pleasure discussing with you. So if you feel, don't hesitate. The problem is my agenda is a little bit messy these days, as you know, but it's even worse off in those days. So just feel, if you feel like half an hour, half an hour is usually zero as well for me, but just pick the time you want. Okay. And I'm happy to have this discussion with you any day. Okay. 

**OPEN CODING TRACE:**

 Yeah, because maybe they don't need to buy a new application over here and then this would be redundant, this function. Yeah. Or then they don't have to buy it, but they can integrate and then you only have the integration layer. So for example, for cataloging, that's a very common use case. Do you need integration or do you need to rebuild the data kind of all?

**AXIAL CODING TRACE:**
added:
``` python
    role_based_access_control
``` 

> Interviewer

> Yeah. Thank you so much. We will keep in touch. 

> Participant 1

> Yeah. Bye. Bye. Bye.

