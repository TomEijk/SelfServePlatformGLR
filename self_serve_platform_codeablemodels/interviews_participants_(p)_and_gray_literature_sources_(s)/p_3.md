# p
3
## url
https://github.com/TomEijk/Data_as_a_Product_GLR/tree/master/python/self-serve-platform_codeablemodels/field_notes_open_coding_axial_coding
## tiny url
https://tinyurl.com/self-serve-platform-p3
## archive url
https://bit.ly/self-serve-platform-p3
## title
Interview Expert 3
## source code
no
## example
yes
## source type 
Interview
## author type
Practitioner
## references

**AXIAL CODING TRACE:**
``` python
p3 = CClass(source, "p3", values={
    "title": "Interview Expert 3",
    "url": "https://github.com/TomEijk/Data_as_a_Product_GLR/tree/master/python/self-serve-platform_codeablemodels/field_notes_open_coding_axial_coding",
    "archive url": "https://bit.ly/self-serve-platform-p3",
    "tiny url": "https://tinyurl.com/self-serve-platform-p3",
    "author type": "Practitioner",
    "type": "Interview",
    "example": True,
    "source code": False})
```

# coding

> Interviewer

> Are you able to see my screen? 

> Participant 3

> Yes, perfectly. 

> Interviewer

> Thanks. Great. Okay. Okay. This interview is about the self-serve platform and I actually encountered five different decisions we can make during our data, well, our self-serve data platform implementation. The first one is about, yeah, it's very high level and it's actually how you, it's actually about aligning yourself sort of platform. So do we want to create two separate data platforms for the source data products and the consumer data products, for example, or do we want to have just one single self-serve platform that is able to support these source data products, as well as the consumer data products? Do we want to align the self-serve platform with different regions so that each region has a, has its own self-serve platform, so there are different possibilities in how you look at a self-serve platform from a high level perspective. On the second layer, there are actually three core decisions, and these are based on the book of Zhamak Dehghani. So the infrastructure provisioning plane, the data product developer experience plane, and the mesh supervision plane. And in my perspective, the infrastructure provisioning plane is more concerned about the infrastructure around the data product. The data product developer experience plane is about what the data product itself needs and to function well, et cetera. And the last one is more like the governance plane in my perspective. So that's how we can support three different categories, yeah, in our self-serve platform. And the last decision is about the self-serve UI. So how do we make our self-serve platform accessible for the general list? So the people who are not really experts in a certain area, but they know all the basics and they are able to use the tools, et cetera. They are not, it's like a T-shaped person. So they know, yeah, they know a bit about everything. All right. So let's start with the first one. So how to align your self-serve platform. Are you able to see the whole graph or. 

> Participant 3

> Yeah, I see the whole graph, but I can't read it when it's out of. 

> Interviewer

> And like this? 

> Participant 3

> Yeah, perfect. 

> Interviewer

> Okay, great. So how to align a self-serve platform and its components with data mesh. Well, let's go from left to right. So the first option is of course, to have one single data landing zone, one single self-serve platform that supports everything in our mesh. The second one is to align it with the source system and the consumer aligned system, then we have a hub generic and special data landing zones. And this is based on an article of Peter Haines Trangle from Microsoft. And I can perhaps quickly show you a visual to get a clear understanding of what it means. So the hub generic and special data landing zones is actually that we have a central hub, a central, yeah, a central domain where everything is being ingested in, and this has its own data landing zone. So everything comes in into the central hub and then it's transported to all the different data domains, which are aligned with just one single self-serve platform, and then it's transported to the other domains, which are connected to each other. And of course we can create special data landing zones if there are some special cases as well for different data domains, et cetera. So it's actually, everything's being ingested in one central hub. Then we have a data landing zone with certain domains, for example, everything's related to sales, customers, et cetera, and a special data landing zone can be something totally different compared to sales. For example, inventory, things like that. So that we have that one for that one. Yeah, function and regional, regionally aligned data domain, data landing zone, sorry. I think that's pretty clear. And the last one is, yeah, that we actually have different governance planes as well. So in a large enterprise, of course, there are very different kinds of business units and each business unit can have its own governance plane as well. So there are so many different types of self-serve platform structuring or mapping capabilities we can think of. What's your opinion on these? Are you familiar with all of them? Do you think some of these approaches apply to your own architecture? What's your... 

> Participant 3

> Well, from what I can tell you have, there's to be really frank, the patterns or the frameworks as you specify them, from a theoretical point of view. It's not like I am aware of them. I do have some questions like for the hub, generic and special data landing zones, isn't it against the whole ideation of a data mesh approach, where you divide all of the data and the processing of the data into the domains, into the business domains, because they're for the hub, for instance, you say, okay, we have a central data domain where everything lands, but that isn't like a data mesh should work that it seems a bit like more like a data lake approach. 

**OPEN CODING TRACE:**

I do have some questions like for the hub, generic and special data landing zones, isn't it against the whole ideation of a data mesh approach, where you divide all of the data and the processing of the data into the domains, into the business domains, because they're for the hub, for instance, you say, okay, we have a central data domain where everything lands, but that isn't like a data mesh should work that it seems a bit like more like a data lake approach. 

**AXIAL CODING TRACE:**
added:
``` python
    hub_generic_special_data_landing_zones
``` 

> Interviewer

> Yeah. It's more like a master data management approach so that we have a central entity that takes care of all the integration practices. 

> Participant 3

> So now you're talking specifically about the governance metadata or the governance data, so it's related to, 

> Interviewer

> yeah, to the governance metadata. 

> Participant 3

> Okay. Okay. For the platform. Yes, yes, yes. Well, in my perspective, I look at it a bit like we should structure our, our every other domain we have in our data mesh, so how we are handling it right now, we have all of our layers and segregation of layers, but in the end we have per domain, a zone where we output whatever we want to output. And then, I think that should be the same with regards to the governance. So there, we also have a specific team, a platform team, which will have it internal checks and all the internal stuff, nicely, wrapped in a private layer, but also there you should have an output zone or a landing zone, as you, as you would like to call it, where you can provide the information for other teams to come and fetch it. I would definitely keep that centrally because it's also from a government or a platforming point of view, it's exactly the same as you do with other data products. So you provide some insights on all of the data or the governance rulings. You are in control of the logic with regards to the governance. So it should be, it should reside in your domain. 

**OPEN CODING TRACE:**

so how we are handling it right now, we have all of our layers and segregation of layers, but in the end we have per domain, a zone where we output whatever we want to output. And then, I think that should be the same with regards to the governance. So there, we also have a specific team, a platform team, which will have it internal checks and all the internal stuff, nicely, wrapped in a private layer, but also there you should have an output zone or a landing zone, as you, as you would like to call it, where you can provide the information for other teams to come and fetch it. I would definitely keep that centrally because it's also from a government or a platforming point of view, it's exactly the same as you do with other data products.

**AXIAL CODING TRACE:**
added:
``` python
    multiple_data_landing_zones
``` 

> Interviewer

> Okay. Yeah. So there is central entity. 

> Participant 3

> Yeah. Does it make sense? Yeah. 

> Interviewer

> Yeah, sure. Okay. So in terms of self-serve platform, do you think you will, you only need a one single self-serve platform or would you specify different types of self-serve platforms in your, your data mesh? So do you think there will be more instead of just one? 

> Participant 3

> Well, that's the first time that I think about this and it was, and that's why I'm happy to accept your, your invites every time you come, because it's also makes me think a lot on our approach. Because I see some use case or I see there there's value in creating different sort of experiences. Yeah. Because even though you're setting up a self-serve platform for the data generalist, you will have different degrees of, of, of, of, of, of skills and then even ambitions. So I think it would be nice if you offer, some kind of separation where you select the profiles of people who are working with, and I think you have mainly those are two main profiles I think about it's the data, generalization, and then generalist who prefer to work with the data and translate the data or transform the data into something else and create logic out of them. Yeah. That's one aspect. And then you have a big aspect of the people who want to go about the visualization and there you would also, or you could also provide a platform, but. I think for the way we are going right now, yeah. We're setting up one big platform where we add components and we tweak components and perhaps it's, it is interesting when the platform has matured enough to go for a split, so current at the, at this moment, it might already be interesting to already think about the option of creating some kind of permission or access to the components as well. Yeah. But I'm not really firm in any of both options. I see value in both of them. 

**OPEN CODING TRACE:**

Yeah. Because even though you're setting up a self-serve platform for the data generalist, you will have different degrees of, of, of, of, of, of skills and then even ambitions. So I think it would be nice if you offer, some kind of separation where you select the profiles of people who are working with, and I think you have mainly those are two main profiles I think about it's the data, generalization, and then generalist who prefer to work with the data and translate the data or transform the data into something else and create logic out of them. Yeah. That's one aspect. And then you have a big aspect of the people who want to go about the visualization and there you would also, or you could also provide a platform, but. I think for the way we are going right now, yeah.

Perhaps it's, it is interesting when the platform has matured enough to go for a split, so current at the, at this moment, it might already be interesting to already think about the option of creating some kind of permission or access to the components as well. 

**AXIAL CODING TRACE:**
added:
``` python

    functional_and_regionally_aligned_data_landing_zones
    
    role_based_access_control
       
``` 

> Interviewer

> Yeah, sure thing. Okay. Interesting. So then we can perhaps quickly go on to the three, yeah, more well-known planes in, inside the self-serve platform. I will quickly zoom in because it's not really visible, I guess. There we go, always left to right. So in terms of infrastructure provisioning, I encountered some options. The first one is the workflow automation engine. So this can be Apache Airflow, for example, that actually activates the workflow of your data processes. So if you think about the bronze, silver, gold layer, et cetera, it actually activates all of it. That's an important unit we need. Of course the processing service. So how do we transform the data and stuff? Networking and networking is about how the data products can interact with each other. So a data product can be, can be inside a VM and these VMs need to interact somehow. So if we have the source aligned data product, we can have a data product and data product, this one needs to ingest its data into the consumer aligned data product through network practices. The data source ingestion. So we need an ingestion application that ingest all the data into the data products. So for example, virtualization, data transformation orchestration. So we need an orchestrator that makes sure that everything is orchestrated well. So that the sales data in the source aligned data product will be ingested into the right recommendations data product, for example. So everything is orchestrated in the right way. We need to think about a compute infrastructure. So of course the query engine separating compute from compute and storage from compute, like it's done in Snowflake, for example. An elastic performance engine so that everything is skilled in line with the computing power. So if we don't use a lot of computing power, the performance engine makes sure that everything is yeah, actually, minimalized. A polyglot storage option. So there can be multiple types of data ingested into the general storage, for example. So we can have CSV files, but also queries and other storage options. And the last one is about the machine learning platform. So we can provide a feature store and other machine learning related practices. These are actually all the options I encountered to serve in the infrastructure provisioning plane. So this is really about the infrastructure. What do we not need inside the data product, but what do we need around the data product? Do you think some of these patterns slash practices are redundant or? 

> Participant 3

> Well, I was wondering about the workflow automation engine as you specified the airflow. Whilst in my opinion, it's more of an orchestrator. So I was wondering how you could, if you could specify this, the difference for both. So the difference between the workflow automation engine and the orchestration, the data transformation orchestration. 

> Interviewer

> Yeah, in my perspective, airflow was really something that activates, that triggers a workflow, really orchestrating it. But now I'm thinking it more thoroughly. It's perhaps the same thing. So yeah, it's a good suggestion that we can merge these two options, perhaps. 

> Participant 3

> Yeah. Okay. But yeah. Okay. Nice. And then I was also wondering about the unified batch stream processing service, because if you go a bit further, you basically, whenever you do something with data. Either it's compute or it's storage, basically either you're writing it, you're reading it, or you're processing it in one way or the other. So I was thinking about the, the fact that it might be a bit redundant. It might be a specific type of workflow, type of compute. But yeah, either it's batch or it's stream, but anyway, you're using compute. So I'm not sure what you, what you specifically mean with this, or is it something you mean like more infra components or something being set up to, to facilitate streaming or. 

**OPEN CODING TRACE:**

And then I was also wondering about the unified batch stream processing service, because if you go a bit further, you basically, whenever you do something with data. Either it's compute or it's storage, basically either you're writing it, you're reading it, or you're processing it in one way or the other. So I was thinking about the, the fact that it might be a bit redundant.

**AXIAL CODING TRACE:**
added:
``` python

    unified_batch_stream_processing_service
       
``` 

> Interviewer

> Yeah, it was more related to the computing engine indeed.Yeah. As well. So that we need something that makes it possible to transform the data. So for example, yeah. Extracting all the missing values out of the data sets. 

> Participant 3

> Yeah. Yeah. Voila. Yeah. Okay. Yeah. Okay. As you wish, of course. 

> Interviewer

> Yeah. And it's really good feedback to perhaps have a second look on this one and yeah. Yeah, perhaps merge this as well with the compute and the storage layer. Could be, could be. Make it more clear. 

> Participant 3

> Yeah. And then the VMs that's, I think really interesting because that that I'm not sure how to phrase this, but we know, or we also thought about how. How could we organize certain aspects of the data product infrastructure? And of course, VMs are one of them, but you quickly stumble against the Docker and, and, but, yeah, the imaging and the orchestration of those images to, to facilitate computing or storage. Uh, so I'm not sure how to call it, but I think VMs or, or, or Docker orchestration or whatever, it seems interesting to grasp, which, which I should, I would not call VM. Okay. It specifically points a bit to the virtual machines while. 

> Interviewer

> Yeah, there are different options of going to it. You could also use microservices, et cetera, running or deploying microservices. Yeah, sure. So perhaps it's a bit more hosting, uh, hosting patterns or hosting best practices. And then the last, um, that's a good one. Yeah. The last thing is the data source ingestion as well. 

> Participant 3

> Well, if you, if you go to that, uh, aspect, I think it's interesting, but then you also, I think you would also need some sort of, uh, exposure, uh, practice because then you're really separating your, uh, data pipe in the extraction process. So basically the, no, no, sorry, scratch it. Now I'm explaining myself. I see. Um, I see my missing part or am I missing, uh, perhaps, perhaps still, if you separate like that, it's really interesting because then you have the ingestion flows, which basically prepare all your data to be, uh, transformed to be used in a pipe. But in the end, you could also stay that whatever your transformation is producing, it's, it's a beta product, right? Yeah. And data products have different sorts of output ports. And perhaps you also want your platform to enable the, uh, specification of output ports. So from a data generalist, I build something in simple language and in the end I say, okay, I want this on Snowflake and I want this on S3. And where you simply specify those two and then, and sort of practice or pattern kicks in where an automatic, uh, deployment of your data of your output goes to the specified output ports. Yeah. And that's something, if you're specifying ingestion or using ingestion as a specific component inside your platform, you could also look at it that you also want to exposure of your data. Also have a specific practice. Yeah. That's a good one. That can be a child node, for example, like we did over here to make our clear, what kind of possibilities are out there. Depending on how broad you want to go in a theoretical exercise. Yeah. It's worth mentioning the option. 

**OPEN CODING TRACE:**

So from a data generalist, I build something in simple language and in the end I say, okay, I want this on Snowflake and I want this on S3. And where you simply specify those two and then, and sort of practice or pattern kicks in where an automatic, uh, deployment of your data of your output goes to the specified output ports. Yeah. And that's something, if you're specifying ingestion or using ingestion as a specific component inside your platform, you could also look at it that you also want to exposure of your data.

**AXIAL CODING TRACE:**
added:
``` python

    polygot_storage_option
       
``` 

> Interviewer

> Yeah. All right. Thank you. Yeah. That's really helpful. So that's actually all about the infrastructure provisioning plane, I guess. We can perhaps go to the data product developer plane. What, so what kind of functionalities does a self-serve platform need to provide to a data product developer to function well? Well, for example, there needs to be a built deploy, monitor monitoring function. Data products. The data products developer needs to be able to version the data products. So a version function data product. We need to be able to secure our data product with access controls. For example, of course there needs to be some form of accessibility. So we need to be able to read the data product with SQL endpoint, for example, or we can use in-place consumption like it's done in Snowflake. There needs to be a document data, a document function. So when we want to create some discoverability and searchability through a data catalog, for example, the data developer needs to be able to write something in the business catalog and to make it to write metadata to just make it data product more visible to the other participants in the data mesh. There needs to be a function to declaratively create data products. And that means that we, yeah, that it should be really easy to register your data product to, to make your data product, et cetera. Everything should be really clear. There needs to be some testing functionality provided to the data product developer. So a testing interface to make sure that the data product adheres to the policies that it satisfies, satisfies certain requirements, et cetera. Cd  the CRUD operations. So the create, update, delete, create, delete, delete, delete. 

> Participant 3

> So that's what I encountered for the data product developer experience plane, but perhaps there are more options possible. Well, I think I, first of all, I think it's really nice to have this on an overview. I've been struggling with this a bit. Yeah. We are all the aspects and I think you meet quite a lot of them. The only thing from, from in our from experience, it's a very interesting thing to go for declaratively creation of the data products. Yeah. But I'm not sure how that in in practice would work because in the end there needs to be some sort of control over the data products. Data product needs to be unique, needs to be valuable, feasible, blah, blah, blah. Value they will be able to design on their own uniqueness will be more difficult. Like how are you going to specify or are you going to check when you're setting up a data product, whether it's whether it's unique. I, perhaps there are really smart algorithms for that as well. I'm not sure. But for that reason, we currently are not focusing on the, on, on that aspect. So that's something we leave out of scope for the time being, because we see a huge risk in losing control of our data mesh itself and the value that it promises to bring. 

**OPEN CODING TRACE:**

version_dp

crud_operations

testing_dp

The only thing from, from in our from experience, it's a very interesting thing to go for declaratively creation of the data products. Yeah. But I'm not sure how that in in practice would work because in the end there needs to be some sort of control over the data products. Data product needs to be unique

**AXIAL CODING TRACE:**
added:
``` python
    
    testing_dp
    
    crud_operations
    
    
    version_dp
    
    declaratively_create_dp
       
``` 

> Interviewer

> Yeah. You mean in terms of interoperability that the Yeah, more specifically, more specifically. Yeah, sorry. That the data product is becoming too autonomous and that it actually defines his own policies and yeah, it's actually doing separate operations compared to the other data mesh participants and it's getting out of scope and things like that. 

> Participant 3

> Yeah, out of scope might be indeed, but that's something that's I think difficult to steer on, but it's specifically for setting up. Like if you say, okay, I want to set up a customer data product, which encapsulates all our customers. And then there's another team who thinks, okay, we're we can also do that. We also have our customers. Um, how is, how is this going to be kept in check that the mesh doesn't explode with a double or, or non unique data products, stuff like that. Yeah. I mean, so I think from a theoretical point of view, what you're doing is exactly how it should be, but then I'm not sure whether it needs to be in the data product developer experience plane or you bring it to the, yeah, the data product developer experience. 

**OPEN CODING TRACE:**

I want to set up a customer data product, which encapsulates all our customers. And then there's another team who thinks, okay, we're we can also do that. We also have our customers. Um, how is, how is this going to be kept in check that the mesh doesn't explode with a double or, or non unique data products, stuff like that. Yeah. I mean, so I think from a theoretical point of view, what you're doing is exactly how it should be, but then I'm not sure whether it needs to be in the data product developer experience plane

**AXIAL CODING TRACE:**
added:
``` python

    declaratively_create_dp
       
``` 

> Interviewer

> Yeah. The data governance plane, but there needs to be a strong relationship in making sure that, uh, the mesh doesn't bloat itself with, uh, unuseful products. Yeah. Or double, uh, non unique products. Yeah, I agree with that. I think that's more related to the mesh decision plane that I have, but it's good to mention that in the data product developer experience plane as well, that there needs to be some kind of governance. Template through infrastructure as code, for example, and that we have a functionality in that someone can, uh, suggest a certain policy to the central entity. That's taken care of all the processes within the data mesh and this entity is able to approve or decline this, uh, this suggestion. 

> Participant 3

> Exactly. Yeah. That's interesting. So yeah, for example, a governance functionality, a governance, um, option functionality. I don't, yeah, I'm not sure. I think what you did here is perfect. It's just something I'm thinking out loud. And then, uh, uh, I hope to see something like that as well in the governance plan where, where it is checked, where it is kept in check. 

> Interviewer

> Yeah. Yeah. Okay, great. Now we can perhaps, perhaps go to the governance plane. Then we have a more, um, clear idea of all the different planes because it's always, it interacts a lot with each other and there is some overlap between the three of them. So some, yeah, it's sometimes hard to distinguish, uh, the functionalities between these three. Yeah. Yeah. Okay. So what capabilities are accessible more conveniently on a data mesh supervision plane? So more on a governance level. Um, of course, alerting is a very important practice. So if something is going wrong in the, uh, data mesh, a person needs to be alerted and needs to be warned about this, uh, this error. Uh, we need a schema registry. So, uh, there needs to be some schema standards and every data products needs, needs to adhere to these standards. Uh, we need an API catalog. So what kind of APIs are available inside our mesh, um, manage emergent graphs of data products. So this is actually, um, that we have a visibility of all the, of all the connections so that we know that the, uh, sales data product is connected. Communicating with the recommendation data products that we know that the, um, 

> Participant 3

> This is something you would, you would call lineage basically. 

> Interviewer

> Yeah, exactly. Okay. Yeah. So that we have clear visibility of all the connections between the different data products. Yep. Um, yeah, of course the security policies. These needs to be defined in the, uh, governance plane, uh, and needs to be enforced through policy automation. Um, a data product catalog. This is of course the enterprise data product catalog, where all the metadata of the data products is being stored in. Um, the knowledge graph, a knowledge graph is, um, is about the semantics so that we have, um, Um, clear relationships between the different concepts concerning the metadata. So do you know what I mean with that? But it's like the semantics layer that we know what a certain value in our metadata repository actually means. 

> Participant 3

> I was thinking to the, some sort of entity mapping, but I don't think that's what you mean. 

> Interviewer

> Yes. Yeah. For example, we have, um, Cambridge, uh, semantics. I think it was called as on. They have a very famous one Cambridge knowledge graph. That's easier. And I can give you more, some kind of visibility about what I mean. Yeah, it's like this. So we have certain values. This is even better. Can you see it or no, but that's, I think what is it? Yeah. The zoom in function doesn't really cooperate. 

> Participant 3

> Yeah, but I think I know what you mean. It's really to, to, to map the relations between entities and not specifically the data products, but the possible entities within the data. Exactly. So the metadata actually. Interesting.

> Interviewer

> Yeah. So, yeah. So for example, we have the, we have a customer and this of course can be a customer 360 and this can be defined. Well, this, uh, value of course is included in the, in the payment data product, in the sales data product, the recommendations data product. And the connections that the data well that the customer over here has with all the other data features that's described in the knowledge graph. So we connect all the values with the customer. And for example, uh, the inventory, something in inventory, like, um, the number of, um, iPhones, we have still left in inventory is connected to different. Metadata values. 

> Participant 3

> Yeah. Yeah. I understand. Nice. Nice. 

> Interviewer

> So this is something we can include in the so-called meta data lake, meta lake, where we have the data catalog. And on top of the data catalog, we have a semantics layer, which is the knowledge graph. So that we don't, that we do not only have the meta, the data catalog, but also we need to keep track of the, uh, yeah. Of the semantics within our mesh that everything means the same and that we do not have a customer payment value and a customer, um, sales value. A customer that customer needs to be, needs to mean the same in every data product, instead of. Uh, yeah, that each data product is coming up with their own, uh, interpretation of the customer. 

> Participant 3

> Yeah. I'm with you. Um, very interesting aspect. Yeah. Not something we're doing just yet. I think we're starting by, uh, defining the UBIQ with us. I'm not sure if I pronounced it well, but it's some kind of a definition, uh, wording definition or semantics definition where we need to make sure that we all mean the same thing with the words we were using inside our, uh, documentation. I think that could be used as a fundament, uh, which regards to, uh, uh, the graphing actually, or the knowledge graph. Very, very interesting aspect. 

**OPEN CODING TRACE:**

wording definition or semantics definition where we need to make sure that we all mean the same thing with the words we were using inside our, uh, documentation. I think that could be used as a fundament, uh, which regards to, uh, uh, the graphing actually, or the knowledge graph. Very, very interesting aspect. 

**AXIAL CODING TRACE:**
added:
``` python

    knowledge_graph
       
``` 

> Interviewer

> Yeah. And besides the knowledge graph, we can, um, take care of the log management, for example. So what operations, uh, are being realized and what transformations are happening. So it's also a bit indie data lineage area. Of course, master data management is really important. And, uh, while I was explaining the knowledge graph and the data product catalog, I think these are, these can be child notes of the metadata management. So metadata management is of course the more high level perspective on knowledge graph and, uh, data product catalog. So that's some mistake I made here, but I can, uh, yeah, I can of course adjust this in the near future. And of, yeah, the last one is data quality management, which is really important. Yeah. What kind of standards do we want? What kind of quality standards do we want to, uh, yeah. And forcing the data products. 

> Participant 3

> Definitely. Definitely. The only thing I'm not sure whether it's somewhere in there or perhaps the, the, the, the, the, the, the data management part of the data management. Perhaps the different patterns or practices can facilitate it, but I was looking for how to enable all of the data product owners to, uh, invest increase or even detect extra wishes. Do you want to, is it something you think you should, um, enable by the data mesh supervision plane, or is that something you leave out of scope? 

> Interviewer

> Yeah. Sorry. You mean like a feedback loop? 

> Participant 3

> Yeah, exactly. Because, uh, the data mesh approach, I, at least as far as I've read it and the approach we're taking is where we're going to really a federated model. So you have the data platform and the supervision plane, which will enable, uh, the, the continuous feedback culture for people to, yeah. As, as a Federation decide on how the data products and the data mesh itself moves. Mm-hmm. Like data quality itself. It's also something important in you start with a couple of rules and then you decide, is it good enough quality or not? Yeah. And at some point you could say that the users of the mesh start thinking about the data quality and they say, we believe it's not strict enough. We should change the data quality aspects or the data quality checks to something else. Yeah. Um, and that's the kind of Federation we're hoping to achieve within DPG media. Um, and I was wondering, I, to be really honest at this moment, we're taking your approach of out of scope. Let's just have them handle it ourselves. But for the future, we, we were aiming to some practice where the experience or the supervision plane really shows these kinds of stuff. Like people having questions or suggestions about certain aspects, whether it is a data product and it's quality or whether it's the quality rules itself, or perhaps even the knowledge graphing or the log management, whatever. Yeah. Would you consider this something as a specific pattern or practice you want to, uh, put into your, your theoretical exercise or do you think that's something best to keep out of scope?

**OPEN CODING TRACE:**

Yeah. And at some point you could say that the users of the mesh start thinking about the data quality and they say, we believe it's not strict enough. We should change the data quality aspects or the data quality checks to something else.

But for the future, we, we were aiming to some practice where the experience or the supervision plane really shows these kinds of stuff. Like people having questions or suggestions about certain aspects, whether it is a data product and it's quality or whether it's the quality rules itself, or perhaps even the knowledge graphing or the log management, whatever.

**AXIAL CODING TRACE:**
added:
``` python

    feedback_loop
       
``` 

> Interviewer

> Um, no, I think I would definitely include this in the supervision plane and in the theoretical exercise, because yeah, it's a, it's a very good suggestion to include that in your practice. It's like the, I was thinking about it and it's in line with the feedback loop that Zhamak Dehghani describes in her book. That's one of the aspects I miss in this graph, I guess. 

> Participant 3

> So, yeah, yeah, I'm not sure it all depends on how do you want to handle it, but if you, I think it's from, from a theoretical exercise and if you have all the money in the world and all that time, you can do it. I would like to have something like that really thought through within my, uh, platform. 

> Interviewer

> Yeah. Thank you. This, this is a good suggestion. Yeah, definitely. Cool. All right. And besides that, do you think we need anything more in this, um, governance plane? And it's being provided by the, by the self-serve platform, of course. So it's not yet about the, uh, federated governance principle in the data mesh, but. 

> Participant 3

> No, no, no. I understand. Um, not really. I think you have, uh, mostly it's the only thing I was wondering a bit about because, okay, you have, um, I had a suggestion you made to put it under the metadata management, the data catalog, the analytics data management. Uh, the data catalog, the API catalog, uh, and the knowledge graph. Yeah. Could be, I understand what you're going at. The thing I don't really understand is the master data management practice. So how do you see that with relation to the other metadata management? 

> Interviewer

> Yeah, it's more that it's more that we keep a central, uh, yeah, I think you can store. So, um, well, how do I call it so that we have a central store, all the information of, for example, the customer is being, is being kept. So we have customer information from the, from the payment data products, and we have different customer information from the shipping data products. And also from the, uh, sales data products. So these are three different internal storages that all have extra information about the customer. And the master data management is actually a customer 360, poly got storage, for example, where we store all the tree. Uh, yeah, three different internal storages where the, uh, where the, where there is information, where there's metadata about this specific customer. So that's, it's like master data. Everything is, um, yeah, it's being linked together. After data within the mesh, and that's why I connected this one to the, um, yeah, to the governance and the governance team is being responsible of connecting these three different customer entities. Uh, yeah, but that last thing you mean based on, for instance, the knowledge graph, I'm yeah. Yeah. Yeah. The global identifier connected to all the. 

> Participant 3

> Yeah. Okay. Very interesting. I it's the first I, now that you explained it, I think it's really a really, really interesting to do this. Um, but is, is this something you came up with or is this something that, uh, was also specified in the book? Because my, from my point of view, it's something like the master data management stuff we had in, in the past, it is a bit superseded by, uh, or to the data mess supersedes the, the old data master data management practices. You put it like, uh, putting it in a 360 vision on certain entities, all the relationships are mapped for a specific thing. I think it's really valuable. 

**OPEN CODING TRACE:**

From my point of view, it's something like the master data management stuff we had in, in the past, it is a bit superseded by, uh, or to the data mess supersedes the, the old data master data management practices. You put it like, uh, putting it in a 360 vision on certain entities, all the relationships are mapped for a specific thing. I think it's really valuable. 

**AXIAL CODING TRACE:**
added:
``` python

    mdm
       
``` 

> Interviewer

> Yeah. Um, yeah. So actually you, um, you let the data products still, uh, do, do their practices and you don't interfere with their practices. You don't interfere with their autonomy. Uh, you keep a central storage with all the master data where everything is being structured and linked together. 

> Participant 3

> Yeah, but it does, uh, because then also you're touching the lines of the security, because, uh, let's say I want to know, I I'm from department X and I want to know everything about customer Y. Uh, if I go to the master data management and you need to make sure that whatever information I can check that, uh, that I'm allowed to access it as well. So I think it was at quite a lot of, uh, a load on a supervision plane, but I do understand what you're saying that if you have it, uh, it's, it would be really valuable for the company. Yeah, sure thing. 

> Interviewer

> All right. Yeah. I think then never we're in line. Yeah. Well, I think we did discuss every aspect of the governance plane. Now. So then we can go to the last one already, the self-serve UI. So how do all the features of the self-serve platform visible and accessible to the general list? Um, yeah, so there need to be different versions. Of course the cataloging function. So this is the data catalog, the knowledge graph, et cetera, so that we can easily. Search and discover everything available in the data mesh. So the cataloging function functionality, um, a visualization function. So in our UI, we need a visualization function to create easy visualizations of the data sets inside the data product, a data transformation function. And this can be, for example, uh, no code, uh, transformation so that we can easily select certain features and that this feature automatically makes sure that all the duplicates are removed from the data set are the, all the missing values are removed from the data set. Um, things like that. So it takes care of the transformation function and it's, uh, it can be a visible. A visible item, if you know what I mean. So that, yeah, that's, we actually specify the workflow, the data workflow. Yep. Um, a data governance function. So the data product developer itself needs to be able to, uh, yeah, to check. The data governance in its own data product. So what's, you know, what's being done, what's being executed in my data product and how can I make sure that I also adhere to the global policies and that I can like local policies. Of course, uh, this one is an option. Of course, we, it's not mandatory to have an application about function, but if we are a software as a service company, it can be really available to have an interface with the, with an application about function. Uh, data integration function. So, uh, yeah, of course we need to add data sets and we need to export data sets. We need to import and export data sets. Um, we can specify which data sources we want to use. How, yeah, and everything is being integrated through this data integration function. Um, the last one is a query recommendation function to actually help, um, yeah, the general list in, uh, specifying the queries and, uh, yeah, supporting them in, uh, yeah, in this. Yeah, in defining the queries and to make sure that they, uh, they know how to reach out to the data and things like that. But I think it's not, it's not complete yet. Do you think there can be different kinds of functionalities in the self-serve UI to make your data mesh accessible? 

> Participant 3

> Oh man, very difficult question. 

> Interviewer

> Your self-serve platform accessible. So it's not a consumer, but it's for the, for the employees in your company. 

> Participant 3

> Yeah. Yeah, I think you, you already put a lot of effort in the possible options and values that it could bring because like the application build function never thought about, uh, also the data integration function. That's the only thing I see that I'm not sure about. Okay. Um, because I'm not sure how you would like to have a different option of adding the data sets from the developer point of view. In the end, you have in your transformation, you have an automatic check where it gets added to the data catalog. And I'm not sure whether you want to allow any user to start adding a data sets or a catalog. 

**OPEN CODING TRACE:**

Also the data integration function. That's the only thing I see that I'm not sure about. Okay. Um, because I'm not sure how you would like to have a different option of adding the data sets from the developer point of view. In the end, you have in your transformation, you have an automatic check where it gets added to the data catalog. And I'm not sure whether you want to allow any user to start adding a data sets or a catalog. 

**AXIAL CODING TRACE:**
added:
``` python

    data_integration_function
       
``` 

> Interviewer

> Um, yeah. Okay. So if I can elaborate on this one a little bit, um, yeah, it's of course the self-serve UI for the self-serve platform. And if I want to start making a data product, I can indicate, indicate with the data integration function, which operational database I need to develop this data product. So for example, uh, I want some web traffic or I want to use this database with, uh, it's CSV files, or I want to, to get this event streaming back bone, et cetera. So it's actually indicating what you want to develop. Well, what you want to integrate in your data. 

> Participant 3

> So it overlaps really hard with the developer experience playing then. 

> Interviewer

> Yeah. Yeah. And that's why it's a follow-up decision. So it's related to all three actually. Uh, okay. Okay. So, wow, man, that's a difficult exercise. Yeah. I think it's not complete yet and we can include some different functionalities as well. 

> Participant 3

> Yeah. After what I can, I honestly, I don't have it in my, um, I don't have the overview, so I can't really help you in, um, in adding more, but I think on the other things that you should, um, be able to provide something that, um, that brings it all together. But the only thing, is there a possibility or how I'm not sure how do you visual, how you should visualize this in a, in the schema. But I think that it should be a huge difference in the user interface that is provided to also the type of development you would do. I like in many that kind of, is it a possibility to separate stuff? I think that also is a direct result in how, what user interface you want to provide here. Right. Yeah. Yeah. That doesn't matter for this, this theoretical schema or. Yeah, I can definitely think about it. I have to, I'm not sure. I, uh, perhaps, perhaps it's not necessary to do something like that. But about this slide, I think you cover a lot. Yeah. Thank you. And for my point of view, from what we at DPG media want to do and what we would focus on, everything is there. You have the data governance function, which basically facilitates also the, the Federation in the end transformation, the building reports and the finding or the discoverability of the different data products. So I think a lot of good things are already there. 

**OPEN CODING TRACE:**

 But I think that it should be a huge difference in the user interface that is provided to also the type of development you would do.

schema_registry

**AXIAL CODING TRACE:**
added:
``` python
    
    schema_registry
    
    role_based_access_control
       
``` 

> Interviewer

> Great. All right. And we can, for, yeah, then we can perhaps go to the last one and I actually have only two questions left in terms of time as well. I want to be conscious of your time and don't want to, uh, to take too much of it. So, um, yeah, the last two questions are more about usability and inter, uh, interpretability. So, uh, not inter operability, but more how you interpret this. Uh, so this is more on the data practitioner. Do you think, um, all these decisions are clear to the data? 

> Participant 3

> Well, to a practitioner when they want to, uh, implement the self-serve platform, or do you think there can be some adjustments? Well, to be honest, I really need your walk over. I'm not really sure. 

> Interviewer

> Like, yeah, so these are the decisions we just discussed. So on top, we had the alignment, then we had three different planes and the last one was the self-serve UI. But that can be perhaps a different decision we need to make during our self-serve platform implementation. 

> Participant 3 

> So basically, but it's, then it's not really, uh, a decision, right? Because you started how to align a self-serve platform and its components with data mesh. You start, uh, you start perhaps already listing some stuff and then you come into the other, uh, boxes where you say, okay, architectural design for infrastructure provisioning. Then you need to fill in what you want. Um, isn't it more something like that where, ah, okay. Yeah. Yeah. You decide on what you want and that's the decision for you. Yeah. Okay. Yeah. I like that. I think it's, it's, it's okay. Clear, but I really needed your walk over because I don't understand completely how the, the framework like this with the decisions that need to be made work. But if it's like that. It's more like, um, okay. 

> Interviewer

> We want to decide, uh, on what's needed for self-serve platform. So first we think about, do we want to, uh, make more, well, do we want to make multiple data platforms? So the first decision is more on a high level is more on a strategy level. Do we want to create different kinds of self-serve platform or just one? Uh, then if we have made this decision, we can think about these three on the second level. So what types of infrastructure do we need? What does our data product developer need, needs and, uh, what does our governance team actually needs? Okay. If this is, if these three are also clear, then we can go on to the last decision and then, and that's the UI. So what, how do we make it more accessible? Yeah, I understand. But from what you see right now and how you explain it now. 

> Participant 3

> I think the first question is a bit difficult then because you say, okay, how to align a self-serve platform and its components with the data mesh. That's a really broad question. Yeah. Um, then I think it's more interesting to start with indeed. Do you want to create separate environments or separate self-serve platforms defending on the profile or something, or for your, uh, end user? Do you want to create groups of, of end users to offer them a self-serve platform? 

> Interviewer

> That's a good one. So to rephrase the question and yeah, because this question is perhaps too broad. 

> Participant 3

> Yeah. For me, I thought for me. 

> Interviewer

> All right. Great. And then I actually have one last question for this interview and that's more about usability. Do you think practitioners can use this framework in real life during their implementation or do you still think it's too theoretical? Um, I think it's really, uh, I think it's useful. Yes, absolutely. It's more like, um, if you want to start now implementing a data, well, a self-serve platform, would you use this framework for example, to help with your, yeah. To provide some guidance in your journey. 

> Participant 3

> Yes, I would. 

**OPEN CODING TRACE:**

Usability

> Interviewer

> Okay. Great. Yes. The whole interview. So I will stop the recording right now.
