# s 
19
## url
https://danielrcarletti.medium.com/turning-airflow-into-a-full-self-service-data-platform-b67eccdd3445
## tiny url
https://tinyurl.com/self-serve-platform-s19
## archive url
https://bit.ly/self-serve-platform-s19
## title
Turning Airflow into a full self service Data Platform
## source code
no
## example
yes
## source type 
Practitioner Audience Article
## author type
Practitioner
## references

**AXIAL CODING TRACE:**
``` python
s19 = CClass(source, "s19", values={
    "title": "Turning Airflow into a full self service Data Platform",
    "url": "https://danielrcarletti.medium.com/turning-airflow-into-a-full-self-service-data-platform-b67eccdd3445",
    "archive url": "https://bit.ly/self-serve-platform-s19",
    "tiny url": "https://tinyurl.com/self-serve-platform-s19",
    "author type": "Practitioner",
    "type": "Practitioner Audience Article",
    "example": True,
    "source code": False})
```

# coding

> Turning Airflow into a full self service Data Platform

![Airflow](https://miro.medium.com/v2/resize:fit:720/format:webp/1*FJsMPN5kPMI7JuqhsaP7rA.png)

> Apache Airflow has become one of the most widely used data engineering tool on the market. That’s no coincidence, as it is an awesome tool. Easy to install, scalable, compatible with pretty much everything in the market and very easy to use.
Even though Airflow is easy to get the hang of, it is yet another tool to learn. For teams that will not work with it on a daily basis, it is not so trivial. That could lead to an overloaded data engineering team, working overtime to automate jobs that could easily be handled automatically.
For example: a Data Scientist has a Spark code that retrains a model, and needs it to be executed weekly. So, on top of building that, you are going to make this person create an Airflow DAG to do that? Come on, that’s not cool. Let’s automate that process!
That’s where your Data Platform engineer comes in. And here’s the cool thing about Airflow: DAG definitions are done in code, not a fancy UI interface. That means you can define many DAGs in a loop, using a config file for their intricacies. Something like:

``` python
job1:
    schedule: 0 0 * * 1
    type: spark
    files:
        - retrain_model.py
job2:
    schedule: 0 * * *
    type: python
    files:
        - pandas_etl.process
job3:
    schedule: 0 * * *
    type: bash
    files:
        - bash_script.sh
```

> Then it is all a matter of reading the config and creating the DAGs accordingly. From Airflow’s tutorial basic pipeline definition, we’ll add:
reads config yaml file
instantiates DAGs in a loop
adds the required operator to each DAG
adds DAG to globals

```python
from datetime import timedelta
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.operators.bash_operator import BashOperator
from airflow.utils.dates import days_ago
import yaml
import importlib
import os

with open(os.path.join(os.path.dirname(__file__), "config.yaml"), 'r') as yaml_stream:
    try:
        config = yaml.safe_load(yaml_stream)
    except yaml.YAMLError as exc:
        print(exc)

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email': ['airflow@example.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}
for job, configs in config.items():
    dag = DAG(
        job,
        default_args=default_args,
        description='A standard description',
        schedule_interval=configs["schedule"],
        start_date=days_ago(100)
    )
    
    operators = []
    if configs["type"] == "spark": # add spark job
        for file in configs["files"]:
            operator = BashOperator(
                task_id='my_bash_operator',
                bash_command=f"spark-submit {file}",
                dag=dag,
            )
            operators.append(operator)
            
    elif configs["type"] == "python": # add python job
        for file in configs["files"]:
            module = file.split(".")[0]
            function = file.split(".")[1]
            def python_function():
                imported_module = importlib.import_module(module)
                function_object = getattr(imported_module, function)
                function_object()
            
            operator = PythonOperator(
                task_id='my_python_operator',
                python_callable=python_function,
                dag=dag,
            )
            operators.append(operator)
    elif configs["type"] == "bash": # add bash job
        for file in configs["files"]:
            operator = BashOperator(
                task_id='my_bash_operator',
                bash_command=f"{file} ",
                dag=dag,
            )
            operators.append(operator)
    else:
        raise Exception("Need to configure this new operator")
    
    # important: add your DAG to globals
    globals()[job] = dag

```

> Voilá. Now, your Data Scientist only needs to provide his own code and a cron expression. Doesn’t even need to know you’re using Airflow or anything else. Also, it makes it easy to move to another data pipeline tool: all you need to do is write a little code to create your jobs on the new tool.
This is a basic template. You can add many more options if you need, like allowing your config to change the default_args , add params to your operators, etc. Heck, you can have it so every aspect of your DAG is configurable, like descriptions and dependencies.
This Python file is available in my GitHub.

**OPEN CODING TRACE:**

infrastructure as code

workflow_automation_engine

easily_replicable

automation

**AXIAL CODING TRACE:**
added:
``` python
    workflow_automation_engine
    infrastructure_as_code
    automation
    easily_replicable = CClass(force, 'Easily Replicable')
     
```






















